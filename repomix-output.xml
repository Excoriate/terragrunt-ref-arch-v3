This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.config/
  direnv/
    direnvrc
.github/
  ISSUE_TEMPLATE/
    bug_report.md
    bug_report.yml
    feature_request.yml
  workflows/
    dagger-ci.yml
    issue-comment-created.yml
    labels-assigner.yml
    lock-threads.yml
    semantic-pr.yml
  auto-comment.yml
  CODEOWNERS
  config.yml
  dependabot.yml
  labeler.yml
  no-response.yml
  PULL_REQUEST_TEMPLATE.md
  settings.yml
  stale.yml
.gitlab/
  issue_templates/
    bug_report.md
    feature_request.md
  merge_request_templates/
    default.md
ci/
  ci-terragrunt/
    .gitattributes
    .gitignore
    dagger.json
    err.go
    go.mod
    job_ci_tf.go
    job_ci_tg.go
    job.go
    main.go
    utils.go
  README.md
docs/
  dagger.io/
    arguments.md
    cache-volumes.md
    chaining.md
    circleci.md
    clients-cli.md
    clients-http.md
    clients-sdk.md
    constructors.md
    custom-functions.md
    custom-types.md
    daggerverse.md
    default-paths.md
    documentation.md
    engine.md
    enumerations.md
    error-handling.md
    faq.md
    fs-filters.md
    github-actions.md
    github.md
    gitlab.md
    ide-integration.md
    index.md
    interfaces.md
    internals.md
    llm.md
    module-dependencies.md
    module-structure.md
    module-tests.md
    packages.md
    playground.md
    remote-modules.md
    return-values.md
    secrets.md
    services.md
    state.md
    terminal.md
    troubleshooting.md
    types.md
  aws-remote-backend-setup.md
  environment-variables.md
infra/
  terraform/
    modules/
      age-generator/
        locals.tf
        main.tf
        outputs.tf
        variables.tf
        versions.tf
      dni-generator/
        locals.tf
        main.tf
        outputs.tf
        variables.tf
        versions.tf
      lastname-generator/
        locals.tf
        main.tf
        outputs.tf
        variables.tf
        versions.tf
      name-generator/
        locals.tf
        main.tf
        outputs.tf
        variables.tf
        versions.tf
      README.md
  terragrunt/
    _shared/
      _config/
        app.hcl
        git.hcl
        README.md
        remote_state.hcl
        tags.hcl
      _units/
        age_generator.hcl
        dni_generator.hcl
        lastname_generator.hcl
        name_generator.hcl
        README.md
      README.md
    _templates/
      .terraform-version.tpl
      README.md
    global/
      dni/
        age_generator/
          .terraform-version
          README.md
          terragrunt.hcl
          unit_cfg_providers.hcl
          unit_cfg_versions.hcl
        dni_generator/
          .terraform-version
          README.md
          terragrunt.hcl
          unit_cfg_providers.hcl
          unit_cfg_versions.hcl
        lastname_generator/
          .terraform-version
          README.md
          terragrunt.hcl
          unit_cfg_providers.hcl
          unit_cfg_versions.hcl
        name_generator/
          .terraform-version
          README.md
          terragrunt.hcl
          unit_cfg_providers.hcl
          unit_cfg_versions.hcl
        README.md
        stack.hcl
      default.tfvars
      env.hcl
      README.md
    config.hcl
    default.tfvars
    root.hcl
scripts/
  hooks/
    pre-commit-init.sh
  envrc-utils.sh
  justfile-utils.sh
.editorconfig
.envrc
.gitattributes
.gitignore
.pre-commit-config.yaml
.release-please-manifest.json
.shellcheckrc
.yamlfmt.yml
.yamllint.yml
CHANGELOG.md
flake.lock
flake.nix
justfile
LICENSE
README.md
release-please-config.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="infra/terragrunt/_shared/_config/git.hcl">
locals {
  # ---------------------------------------------------------------------------------------------------------------------
  # üåê GIT BASE URLS
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Provide a centralised configuration for Git base URLs
  #
  # Key Responsibilities:
  # - Provide a centralised configuration for Git base URLs
  # - Enable consistent Git base URL usage across all modules
  # - Support cross-module Git base URL configuration sharing

  # This file is sourced by the root config.hcl file, and from there, all the units
  # will inherit the base URLs.
  # ---------------------------------------------------------------------------------------------------------------------
  git_base_urls = {
    github = "git::git@github.com:"
    gitlab = "git::gitlab.com:"
    local  = "${get_repo_root()}/infra/terraform/modules"
  }
}
</file>

<file path=".config/direnv/direnvrc">
# Direnv configuration for optimized performance
# This file contains settings to improve direnv performance in development environments

# Reduce direnv logging verbosity for faster startup
export DIRENV_LOG_FORMAT=""

# Optimize garbage collection
export DIRENV_WARN_TIMEOUT=100

# Exclude certain directories from watching to improve performance
export DIRENV_WATCH_EXCLUDES=".*/_build/.*:.*/.git/.*:.*/.direnv/.*"
</file>

<file path=".github/ISSUE_TEMPLATE/bug_report.md">
---
name: Bug Report
about: Report an issue in the Terragrunt Reference Architecture
title: "[Component]: Describe the problem"
labels: bug
assignees:
---

## Problem Description

Provide a clear and concise explanation of the unexpected behavior.

## Environment Details

- Terragrunt Ref Arch Version:
- Terragrunt Version:
- Terraform Version:
- Operating System:
- Affected Component:
  - [ ] Root Configuration
  - [ ] Shared Configuration
  - [ ] Environment Configuration
  - [ ] Stack Configuration
  - [ ] Unit Configuration
  - [ ] CLI/Tooling
  - [ ] Other (specify)

## Reproduction Steps

Detailed steps to reproduce the issue:

1.
2.
3.

## Expected vs Actual Behavior

**Expected:**
Describe the expected behavior based on the architecture's design.

**Actual:**
Describe the observed unexpected behavior or error.

## Diagnostic Information

- Environment Variables:
- Configuration Files:
  - [ ] root.hcl
  - [ ] config.hcl
  - [ ] terragrunt.hcl
  - [ ] unit_cfg_providers.hcl
  - [ ] unit_cfg_versions.hcl
  - [ ] Other (specify)

Log Output:

```
[Paste relevant log output]
```

Error Message:

```
[Paste complete error message]
```

## Impact Assessment

Severity:

- [ ] Critical (workflow blocking)
- [ ] High (significant functionality impaired)
- [ ] Medium (partial functionality affected)
- [ ] Low (minor issue)

Impacted Workflows/Components:

## Additional Context

- Custom architecture modifications
- Relevant configuration details
- Supporting evidence (screenshots, logs)

## Proposed Solution

Suggested approaches or potential fixes (optional):
</file>

<file path=".github/workflows/dagger-ci.yml">
---
name: Dagger CI

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]
  workflow_dispatch:

jobs:
  tf_modules_static_check:
    name: Terraform Modules Static Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run Terraform Modules Static Check
        uses: dagger/dagger-for-github@8.0.0
        with:
          version: "latest"
          verb: "call"
          module: ci/ci-terragrunt # Specifies the directory of the Dagger module
          args: job-terraform-modules-static-check # Calls the specific Dagger function
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

  tg_units_static_check:
    name: Terragrunt Units Static Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run Terragrunt Units Static Check
        uses: dagger/dagger-for-github@8.0.0
        with:
          version: "latest"
          verb: "call"
          module: ci/ci-terragrunt # Specifies the directory of the Dagger module
          args: job-terragrunt-units-static-check --no-cache --aws-access-key-id=env://AWS_ACCESS_KEY_ID --aws-secret-access-key=env://AWS_SECRET_ACCESS_KEY
            --env-vars="TG_NON_INTERACTIVE=$TG_NON_INTERACTIVE,TG_LOG_LEVEL=$TG_LOG_LEVEL,TG_STACK_REMOTE_STATE_BUCKET_NAME=$TG_STACK_REMOTE_STATE_BUCKET_NAME,TG_STACK_REMOTE_STATE_LOCK_TABLE=$TG_STACK_REMOTE_STATE_LOCK_TABLE,DEFAULT_REGION=$DEFAULT_REGION"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          DEFAULT_REGION: "us-east-1"
          TG_NON_INTERACTIVE: "true"
          TG_LOG_LEVEL: "info"
          TG_STACK_REMOTE_STATE_BUCKET_NAME: ${{ secrets.TF_STATE_BUCKET }}
          TG_STACK_REMOTE_STATE_LOCK_TABLE: ${{ secrets.TF_STATE_LOCK_TABLE }}

  tg_units_plan:
    name: Terragrunt Units Plan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run Terragrunt Units Plan
        uses: dagger/dagger-for-github@8.0.0
        with:
          version: "latest"
          verb: "call"
          module: ci/ci-terragrunt # Specifies the directory of the Dagger module
          args: job-terragrunt-units-plan --no-cache --aws-access-key-id=env://AWS_ACCESS_KEY_ID --aws-secret-access-key=env://AWS_SECRET_ACCESS_KEY
            --env-vars="TG_NON_INTERACTIVE=$TG_NON_INTERACTIVE,TG_LOG_LEVEL=$TG_LOG_LEVEL,TG_STACK_REMOTE_STATE_BUCKET_NAME=$TG_STACK_REMOTE_STATE_BUCKET_NAME,TG_STACK_REMOTE_STATE_LOCK_TABLE=$TG_STACK_REMOTE_STATE_LOCK_TABLE,DEFAULT_REGION=$DEFAULT_REGION"
        env:
          # Map GitHub secrets to environment variables for the Dagger function call
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          # Add variables from .env needed by the Dagger function or Terragrunt itself
          DEFAULT_REGION: "us-east-1" # Or use secrets.AWS_REGION if available/preferred
          TG_NON_INTERACTIVE: "true" # From .env
          TG_LOG_LEVEL: "info" # From .env
          TG_STACK_REMOTE_STATE_BUCKET_NAME: ${{ secrets.TF_STATE_BUCKET }} # Assumed Secret Name
          TG_STACK_REMOTE_STATE_LOCK_TABLE: ${{ secrets.TF_STATE_LOCK_TABLE }} # Assumed Secret Name
          # AWS_REGION: ${{ secrets.AWS_REGION }} # Example if region is needed
</file>

<file path=".github/workflows/issue-comment-created.yml">
---
name: Issue Comment Management

on:
  issue_comment:
    types: [created]

permissions:
  issues: write

jobs:
  comment-actions:
    runs-on: ubuntu-latest
    steps:
      - name: Handle Comment Commands
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const comment = context.payload.comment;
            const issue = context.payload.issue;
            const commentBody = comment.body.trim().toLowerCase();

            // Reopen command
            if (commentBody === '/reopen') {
              if (issue.state === 'closed') {
                await github.rest.issues.update({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: issue.number,
                  state: 'open'
                });
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: issue.number,
                  body: 'Issue reopened by repository maintainer.'
                });
              }
            }

            // Additional commands can be added here
</file>

<file path=".github/auto-comment.yml">
---
pullRequestOpened: |-
  :wave: Thanks for contributing to the Terragrunt Reference Architecture!

  Before we can merge this PR, please ensure:
  - [ ] Added/updated tests for new functionality
  - [ ] Followed Terragrunt and Terraform best practices
  - [ ] Updated documentation for new features
  - [ ] Verified configuration modularity
  - [ ] Checked performance impact on infrastructure configurations
  - [ ] Reviewed changes for security implications
  - [ ] Linked to relevant issue (if applicable)

  Terragrunt Specific Checklist:
  - [ ] Validated HCL configuration syntax
  - [ ] Tested with multiple environment configurations
  - [ ] Considered multi-stack and cross-environment scenarios
  - [ ] Ensured DRY (Don't Repeat Yourself) principles

  Thank you for improving the Terragrunt Reference Architecture! üöÄ
</file>

<file path=".github/CODEOWNERS">
# CODEOWNERS file for Terragrunt Reference Architecture

# Default owners for everything in the repo
* @Excoriate

# Terragrunt Infrastructure
infra/terragrunt/ @Excoriate @terragrunt-maintainers

# Terraform Modules
infra/terraform/ @Excoriate @terraform-maintainers

# Documentation
*.md @Excoriate @docs-maintainers

# CI/CD Configuration
.github/ @Excoriate @ci-maintainers
</file>

<file path=".github/dependabot.yml">
---
version: 2
updates:
  # Terraform and Terragrunt dependencies
  - package-ecosystem: "terraform"
    directory: "/infra/terragrunt"
    schedule:
      interval: "weekly"
    groups:
      terragrunt-dependencies:
        patterns:
          - "*"
    labels:
      - "dependencies"
      - "infra/terragrunt"
      - "terraform"

  # Terraform module dependencies
  - package-ecosystem: "terraform"
    directory: "/infra/terraform"
    schedule:
      interval: "weekly"
    groups:
      terraform-module-dependencies:
        patterns:
          - "*"
    labels:
      - "dependencies"
      - "infra/terraform"

  # GitHub Actions
  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"
    groups:
      actions-dependencies:
        patterns:
          - "*"
    labels:
      - "dependencies"
      - "ci"

  # Documentation dependencies
  - package-ecosystem: "npm"
    directory: "/"
    schedule:
      interval: "monthly"
    groups:
      docs-dependencies:
        patterns:
          - "remark"
          - "markdown"
    labels:
      - "dependencies"
      - "docs"
</file>

<file path=".github/PULL_REQUEST_TEMPLATE.md">
## üèóÔ∏è Terragrunt Reference Architecture PR

### What Changes

- Brief description of changes
- Key modifications or enhancements to infrastructure configuration

### Change Type (select all that apply)

- [ ] Terragrunt Configuration
- [ ] Shared Configuration
- [ ] Environment Configuration
- [ ] Terraform Module
- [ ] Documentation
- [ ] Dependency Update

### Checklist

- [ ] Followed Terragrunt best practices
- [ ] Maintained configuration modularity
- [ ] Added/updated tests
- [ ] Updated documentation
- [ ] Verified cross-environment compatibility
- [ ] Ensured DRY (Don't Repeat Yourself) principles

### Configuration Impact

- Affected stacks/environments
- Potential breaking changes
- Performance considerations

### Additional Context

- Related issues
- Configuration diffs
- Screenshots or logs (if applicable)
</file>

<file path=".github/settings.yml">
---
repository:
  name: terragrunt-ref-arch-v3
  description: Modular Infrastructure as Code Reference Architecture with Terragrunt and Terraform
  topics: terragrunt, terraform, infrastructure-as-code, devops, cloud, iac
  default_branch: main
  allow_squash_merge: true
  allow_merge_commit: false
  allow_rebase_merge: true
  delete_branch_on_merge: true
  has_projects: false
  has_wiki: true

labels:
  - name: infra/terragrunt
    color: '#1D76DB'
    description: Terragrunt configuration and infrastructure changes
  - name: infra/terraform
    color: '#0075CA'
    description: Terraform module modifications
  - name: tool/cli
    color: '#7B42BC'
    description: CLI tool enhancements
  - name: performance
    color: '#FFC300'
    description: Performance optimization for infrastructure code
  - name: config/root
    color: '#5319E7'
    description: Root configuration changes
  - name: config/shared
    color: '#006B75'
    description: Shared configuration modifications
  - name: config/environment
    color: '#207DE5'
    description: Environment-specific configuration updates

branches:
  - name: main
    protection:
      required_pull_request_reviews:
        required_approving_review_count: 1
        dismiss_stale_reviews: true
        require_code_owner_reviews: true
      required_status_checks:
        strict: true
        contexts:
          - "lint"
          - "test"
          - "codecov/project"
      enforce_admins: false
      restrictions:
        apps: []
        teams: []
</file>

<file path="ci/ci-terragrunt/.gitattributes">
/dagger.gen.go linguist-generated
/internal/dagger/** linguist-generated
/internal/querybuilder/** linguist-generated
/internal/telemetry/** linguist-generated
</file>

<file path="ci/ci-terragrunt/.gitignore">
/dagger.gen.go
/internal/dagger
/internal/querybuilder
/internal/telemetry
</file>

<file path="ci/ci-terragrunt/dagger.json">
{
  "name": "terragrunt",
  "engineVersion": "v0.18.2",
  "sdk": {
    "source": "go"
  },
  "include": [
    "!../../.direnv",
    "!../../.vscode",
    "!../../.idea",
    "!../../.trunk",
    "!../../go.work",
    "!../../go.work.sum"
  ]
}
</file>

<file path="ci/ci-terragrunt/err.go">
package main

import (
	"errors"
	"fmt"
	"strings"
)

// Constants for module name and emoji representations for different types of messages.
const (
	ModuleName = "CI Terragrunt"
	ErrorEmoji = "‚ùå"
)

// ModuleError represents a custom error for the GoToolbox module.
type ModuleError struct {
	message string
	err     error
}

// Error returns the error message with a prefixed module name and emoji.
func (e *ModuleError) Error() string {
	prefix := fmt.Sprintf("%s [%s]", ErrorEmoji, ModuleName)
	if e.err != nil {
		return fmt.Sprintf("%s %s: %v", prefix, e.message, e.err)
	}

	return fmt.Sprintf("%s %s", prefix, e.message)
}

// Unwrap returns the underlying error.
func (e *ModuleError) Unwrap() error {
	return e.err
}

// NewError creates a new ModuleError with a custom message.
//
// Parameters:
//   - message: Custom error message.
//
// Returns:
//   - *ModuleError: A new ModuleError instance.
func NewError(message string) *ModuleError {
	return &ModuleError{message: message}
}

// WrapError wraps an existing error with a custom message.
//
// Parameters:
//   - err: The original error to be wrapped.
//   - message: Custom error message.
//
// Returns:
//   - *ModuleError: A wrapped error with a custom message.
func WrapError(err error, message string) *ModuleError {
	return &ModuleError{
		message: message,
		err:     err,
	}
}

// Errorf creates a new ModuleError with a formatted message.
//
// Parameters:
//   - format: The format string for the error message.
//   - args: The arguments to be substituted in the format string.
//
// Returns:
//   - *ModuleError: A new ModuleError instance with a formatted message.
func Errorf(format string, args ...interface{}) *ModuleError {
	return &ModuleError{
		message: fmt.Sprintf(format, args...),
	}
}

// WrapErrorf wraps an existing error with a formatted message.
//
// Parameters:
//   - err: The original error to be wrapped.
//   - format: The format string for the error message.
//   - args: The arguments to be substituted in the format string.
//
// Returns:
//   - *ModuleError: A wrapped error with a formatted message.
func WrapErrorf(err error, format string, args ...interface{}) *ModuleError {
	return &ModuleError{
		message: fmt.Sprintf(format, args...),
		err:     err,
	}
}

// JoinErrors joins multiple errors into a single ModuleError.
//
// Parameters:
//   - errs: A variadic list of errors to be joined.
//
// Returns:
//   - *ModuleError: A new ModuleError containing the joined error messages,
//     or nil if no errors were provided.
func JoinErrors(errs ...error) *ModuleError {
	if len(errs) == 0 {
		return nil
	}

	messages := make([]string, 0, len(errs))

	for _, err := range errs {
		if err != nil {
			var me *ModuleError
			if errors.As(err, &me) {
				// If it's already a ModuleError, strip the prefix
				messages = append(messages, strings.TrimPrefix(me.Error(), fmt.Sprintf("%s [%s] ", ErrorEmoji, ModuleName)))
			}
		}
	}

	return &ModuleError{
		message: strings.Join(messages, "\n"),
	}
}
</file>

<file path="ci/ci-terragrunt/go.mod">
module dagger/terragrunt

go 1.23.6

require (
	github.com/99designs/gqlgen v0.17.70
	github.com/Khan/genqlient v0.8.0
	github.com/vektah/gqlparser/v2 v2.5.23
	go.opentelemetry.io/otel v1.34.0
	go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc v0.8.0
	go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploghttp v0.8.0
	go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc v1.32.0
	go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetrichttp v1.32.0
	go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.32.0
	go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp v1.32.0
	go.opentelemetry.io/otel/log v0.8.0
	go.opentelemetry.io/otel/metric v1.34.0
	go.opentelemetry.io/otel/sdk v1.34.0
	go.opentelemetry.io/otel/sdk/log v0.8.0
	go.opentelemetry.io/otel/sdk/metric v1.34.0
	go.opentelemetry.io/otel/trace v1.34.0
	go.opentelemetry.io/proto/otlp v1.3.1
	golang.org/x/sync v0.12.0
	google.golang.org/grpc v1.71.0
)

require (
	github.com/cenkalti/backoff/v4 v4.3.0 // indirect
	github.com/go-logr/logr v1.4.2 // indirect
	github.com/go-logr/stdr v1.2.2 // indirect
	github.com/google/uuid v1.6.0
	github.com/grpc-ecosystem/grpc-gateway/v2 v2.23.0 // indirect
	github.com/sosodev/duration v1.3.1 // indirect
	go.opentelemetry.io/auto/sdk v1.1.0 // indirect
	go.opentelemetry.io/otel/exporters/otlp/otlptrace v1.32.0 // indirect
	golang.org/x/net v0.38.0 // indirect
	golang.org/x/sys v0.31.0 // indirect
	golang.org/x/text v0.23.0 // indirect
	google.golang.org/genproto/googleapis/api v0.0.0-20250106144421-5f5ef82da422 // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20250115164207-1a7da9e5054f // indirect
	google.golang.org/protobuf v1.36.6 // indirect
)

replace go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc => go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploggrpc v0.8.0

replace go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploghttp => go.opentelemetry.io/otel/exporters/otlp/otlplog/otlploghttp v0.8.0

replace go.opentelemetry.io/otel/log => go.opentelemetry.io/otel/log v0.8.0

replace go.opentelemetry.io/otel/sdk/log => go.opentelemetry.io/otel/sdk/log v0.8.0
</file>

<file path="ci/ci-terragrunt/job_ci_tf.go">
package main

import (
	"context"
	"fmt"
)

func (m *Terragrunt) JobTerraformModulesStaticCheck(ctx context.Context) (string, error) {
	modules := []string{
		"dni-generator",
		"lastname-generator",
		"name-generator",
		"age-generator",
	}

	results := []JobResult{}

	for _, module := range modules {
		tfModuleMntPath := fmt.Sprintf("%s/%s", defaultMntPath, getTerraformModulesExecutionPath(module))
		m, err := m.WithSRC(ctx, tfModuleMntPath, m.Src)

		if err != nil {
			return "", WrapErrorf(err, "failed to set source code for module %s mounted in path %s", module, tfModuleMntPath)
		}

		execCtr := m.Ctr.
			WithExec([]string{"terraform", "init", "-backend=false"}).
			WithExec([]string{"terraform", "validate"}).
			WithExec([]string{"terraform", "fmt", "-recursive"})

		stdout, err := execCtr.Stdout(ctx)

		if err != nil {
			return "", WrapErrorf(err, "failed to validate module %s", module)
		}

		results = append(results, JobResult{
			WorkDir: tfModuleMntPath,
			Output:  stdout,
			Err:     nil,
		})
	}

	return ProcessActionSyncResults(results)
}
</file>

<file path="ci/ci-terragrunt/job_ci_tg.go">
package main

import (
	"context"
	"dagger/terragrunt/internal/dagger"
	"fmt"
	"sync"
)

// JobTerragruntUnitStaticCheck performs a series of static checks on Terragrunt
// configurations for a specific unit. It utilizes the provided AWS credentials
// and other parameters to execute the checks in a Dagger container context.
//
// Parameters:
//
//	ctx: A context.Context for managing the lifecycle of the Dagger container.
//	     This is used to control cancellation and timeouts for the operation.
//	awsAccessKeyID: A pointer to a dagger.Secret containing the AWS access key ID
//	                 required for authentication with AWS services.
//	awsSecretAccessKey: A pointer to a dagger.Secret containing the AWS secret access
//	                    key required for authentication with AWS services.
//	awsRoleArn: A string representing the AWS role ARN to assume for the command.
//	             This parameter is optional and can be omitted if not needed.
//	awsOidcToken: A pointer to a dagger.Secret containing the OIDC token (JWT)
//	               obtained from CI (e.g., from CI_JOB_JWT_V2). This is used for
//	               authentication when using OIDC. This parameter is optional.
//	awsRegion: A string specifying the AWS region to use for the command. This
//	           parameter is optional and can be omitted if not needed.
//	awsRoleSessionName: A string representing an optional name for the assumed
//	                    role session. This parameter is optional.
//	noCache: A boolean flag indicating whether to disable caching for the command.
//	         If set to true, the command will not use any cached results.
//	tgArgs: A slice of strings containing arbitrary arguments to pass to the
//	         Terragrunt command. This parameter is optional.
//	loadDotEnvFile: A boolean indicating whether to load a .env file for
//	                 environment variable configuration. This parameter is optional.
//	envVars: A slice of strings including all environment variables to pass to
//	          the command. This parameter is optional.
//
// Returns:
//
//	A string containing the output of the static checks, and an error if any
//	occurred during the execution of the checks.
func (m *Terragrunt) JobTerragruntUnitsStaticCheck(
	// ctx is the context for the Dagger container.
	// +optional
	ctx context.Context,
	// awsAccessKeyID is the AWS access key ID to use for the command.
	// +optional
	awsAccessKeyID *dagger.Secret,
	// awsSecretAccessKey is the AWS secret access key to use for the command.
	// +optional
	awsSecretAccessKey *dagger.Secret,
	// awsRoleArn is the AWS role ARN to use for the command.
	// +optional
	awsRoleArn string,
	// awsOidcToken is the OIDC token (JWT) obtained from  CI (e.g., from CI_JOB_JWT_V2). Pass as a secret.
	// +optional
	awsOidcToken *dagger.Secret,
	// awsRegion is the AWS region to use for the command.
	// +optional
	awsRegion string,
	// awsRoleSessionName is an optional name for the assumed role session.
	// +optional
	awsRoleSessionName string,
	// noCache set the cache buster
	// +optional
	noCache bool,
	// tgArgs are the arbitrary arguments to pass to the terrawgrunt command
	// +optional
	tgArgs []string,
	// loadDotEnvFile is a boolean indicating whether to load .env file
	// +optional
	loadDotEnvFile bool,
	// envVars is a slice of strings including all environment variables to pass to the command.
	// +optional
	envVars []string,
) (string, error) {
	units := []string{
		"dni_generator",
		"lastname_generator",
		"name_generator",
		"age_generator",
	}

	var wg sync.WaitGroup
	resultChan := make(chan JobResult, len(units))

	if loadDotEnvFile {
		mWithDotEnvLoaded, err := m.WithDotEnvFile(ctx, m.Src)

		if err != nil {
			return "", WrapErrorf(err, "failed to load .env file")
		}

		m = mWithDotEnvLoaded
	}

	// Connect to AWS, with credentials or OIDC.
	m = m.WithAWSKeys(ctx, awsAccessKeyID, awsSecretAccessKey, awsRegion)

	if noCache {
		m = m.WithCacheBuster()
	}

	if len(envVars) > 0 {
		mWithEnvVars, err := m.WithEnvVars(envVars)

		if err != nil {
			return "", WrapErrorf(err, "failed to set environment variables")
		}

		m = mWithEnvVars
	}

	// Run each unit check concurrently
	for _, unit := range units {
		currentUnit := unit

		// mount path dynamically calculated per unit.
		tgUnitPath := getTerragruntExecutionPath(defaultRefArchEnv, defaulttRefArchLayer, currentUnit)
		tgUnitMntPath := fmt.Sprintf("%s/%s", defaultMntPath, tgUnitPath)

		// Global configuration, let's start with the source code and the .env file.
		mWithSrc, err := m.WithSRC(ctx, tgUnitMntPath, m.Src)

		if err != nil {
			return "", WrapErrorf(err, "failed to set source code for unit %s", currentUnit)
		}

		m = mWithSrc

		wg.Add(1)

		go func() {
			defer wg.Done()
			var finalErr error
			var finalOutput string

			// Execution runtime, just a fancy nawe for the container's state per execution.
			execCtr := m.Ctr.
				WithExec([]string{"terragrunt", "init", "--working-dir", tgUnitMntPath}).
				WithExec([]string{"terragrunt", "terragrunt-info", "--working-dir", tgUnitMntPath}).
				WithExec([]string{"terragrunt", "hclfmt", "--check", "--diff", "--working-dir", tgUnitMntPath}).
				WithExec([]string{"terragrunt", "validate-inputs", "--working-dir", tgUnitMntPath}).
				WithExec([]string{"terragrunt", "hclvalidate", "--show-config-path", "--working-dir", tgUnitMntPath})

			stdout, err := execCtr.Stdout(ctx)
			if err != nil {
				finalErr = WrapErrorf(err, "failed to get the stdout of the terragrunt action for the following environment: %s, layer: %s, unit: %s", currentUnit, defaulttRefArchLayer, currentUnit)
			} else {
				finalOutput = stdout
			}

			// Collect results
			resultChan <- JobResult{
				WorkDir: tgUnitPath,
				Output:  finalOutput,
				Err:     finalErr,
			}
		}()
	}

	wg.Wait()
	close(resultChan)

	finalOutput, err := processActionAsyncResults(resultChan)
	if err != nil {
		return "", err
	}

	return finalOutput, nil
}

// JobTerragruntUnitsPlan executes a Terragrunt plan command for multiple units concurrently.
// It takes a context for managing the lifecycle of the Dagger container and various AWS credentials
// and configuration options necessary for the command execution.
//
// Parameters:
//   - ctx: The context for the Dagger container. This is used to control the execution lifecycle.
//     // +optional
//   - awsAccessKeyID: The AWS access key ID to use for the command. This is a secret value.
//     // +optional
//   - awsSecretAccessKey: The AWS secret access key to use for the command. This is a secret value.
//     // +optional
//   - awsRoleArn: The AWS role ARN to use for the command. This is used for assuming a role in AWS.
//     // +optional
//   - awsOidcToken: The OIDC token (JWT) obtained from CI (e.g., from CI_JOB_JWT_V2). Pass as a secret.
//     // +optional
//   - awsRegion: The AWS region to use for the command. This specifies the geographical region for AWS services.
//     // +optional
//   - awsRoleSessionName: An optional name for the assumed role session. This can help identify the session in AWS.
//     // +optional
//   - noCache: A boolean flag that indicates whether to disable caching for the command execution.
//     // +optional
//   - tgArgs: A slice of arbitrary arguments to pass to the Terragrunt command. This allows customization of the command.
//     // +optional
//   - loadDotEnvFile: A boolean indicating whether to load a .env file. If true, environment variables from the file will be loaded.
//     // +optional
//   - envVars: A slice of strings including all environment variables to pass to the command. This allows for dynamic configuration.
//     // +optional
//
// Returns:
// - A string containing the output of the command execution, or an error if the execution fails.
func (m *Terragrunt) JobTerragruntUnitsPlan(
	// ctx is the context for the Dagger container.
	// +optional
	ctx context.Context,
	// awsAccessKeyID is the AWS access key ID to use for the command.
	// +optional
	awsAccessKeyID *dagger.Secret,
	// awsSecretAccessKey is the AWS secret access key to use for the command.
	// +optional
	awsSecretAccessKey *dagger.Secret,
	// awsRoleArn is the AWS role ARN to use for the command.
	// +optional
	awsRoleArn string,
	// awsOidcToken is the OIDC token (JWT) obtained from  CI (e.g., from CI_JOB_JWT_V2). Pass as a secret.
	// +optional
	awsOidcToken *dagger.Secret,
	// awsRegion is the AWS region to use for the command.
	// +optional
	awsRegion string,
	// awsRoleSessionName is an optional name for the assumed role session.
	// +optional
	awsRoleSessionName string,
	// noCache set the cache buster
	// +optional
	noCache bool,
	// tgArgs are the arbitrary arguments to pass to the terrawgrunt command
	// +optional
	tgArgs []string,
	// loadDotEnvFile is a boolean indicating whether to load .env file
	// +optional
	loadDotEnvFile bool,
	// envVars is a slice of strings including all environment variables to pass to the command.
	// +optional
	envVars []string,
) (string, error) {
	units := []string{
		"dni_generator",
		"lastname_generator",
		"name_generator",
		"age_generator",
	}

	var wg sync.WaitGroup
	resultChan := make(chan JobResult, len(units))

	if loadDotEnvFile {
		mWithDotEnvLoaded, err := m.WithDotEnvFile(ctx, m.Src)

		if err != nil {
			return "", WrapErrorf(err, "failed to load .env file")
		}

		m = mWithDotEnvLoaded
	}

	// Connect to AWS, with credentials or OIDC.
	m = m.WithAWSKeys(ctx, awsAccessKeyID, awsSecretAccessKey, awsRegion)

	if noCache {
		m = m.WithCacheBuster()
	}

	if len(envVars) > 0 {
		mWithEnvVars, err := m.WithEnvVars(envVars)

		if err != nil {
			return "", WrapErrorf(err, "failed to set environment variables")
		}

		m = mWithEnvVars
	}

	// Run each unit check concurrently
	for _, unit := range units {
		currentUnit := unit

		// mount path dynamically calculated per unit.
		tgUnitPath := getTerragruntExecutionPath(defaultRefArchEnv, defaulttRefArchLayer, currentUnit)
		tgUnitMntPath := fmt.Sprintf("%s/%s", defaultMntPath, tgUnitPath)

		// Global configuration, let's start with the source code and the .env file.
		mWithSrc, err := m.WithSRC(ctx, tgUnitMntPath, m.Src)

		if err != nil {
			return "", WrapErrorf(err, "failed to set source code for unit %s", currentUnit)
		}

		m = mWithSrc

		wg.Add(1)

		go func() {
			defer wg.Done()
			var finalErr error
			var finalOutput string

			// Execution runtime, just a fancy nawe for the container's state per execution.
			execCtr := m.Ctr.
				WithExec([]string{"terragrunt", "init", "--working-dir", tgUnitMntPath}).
				WithExec([]string{"terragrunt", "plan", "--working-dir", tgUnitMntPath})

			stdout, err := execCtr.Stdout(ctx)
			if err != nil {
				finalErr = WrapErrorf(err, "failed to get the stdout of the terragrunt action for the following environment: %s, layer: %s, unit: %s", currentUnit, defaulttRefArchLayer, currentUnit)
			} else {
				finalOutput = stdout
			}

			// Collect results
			resultChan <- JobResult{
				WorkDir: tgUnitPath,
				Output:  finalOutput,
				Err:     finalErr,
			}
		}()
	}

	wg.Wait()
	close(resultChan)

	finalOutput, err := processActionAsyncResults(resultChan)
	if err != nil {
		return "", err
	}

	return finalOutput, nil
}
</file>

<file path="ci/ci-terragrunt/job.go">
package main

import (
	"fmt"
	"strings"
)

// JobResult represents the result of a Continuous Integration (CI) action.
// It contains information about the unit that was executed, the output of the action,
// and any error that may have occurred during execution.
type JobResult struct {
	WorkDir string // WorkDir indicates the specific unit of work that was executed.
	Output  string // Output contains the result or output generated by the action.
	Err     error  // Err holds any error encountered during the execution of the action.
}

// processActionResults collects results from concurrent actions executed in a separate goroutine.
// It aggregates any errors encountered during the execution and formats a success report.
// The function takes a channel of ActionResult, which contains the results of the actions,
// and returns a formatted string report of successful actions or an error if any occurred.
//
// Parameters:
//   - resultChan: A channel of ActionResult that receives results from concurrent actions.
//
// Returns:
//   - A string containing a summary of all successful actions and their outputs, or an empty string
//     if there were errors.
//   - An error that aggregates all encountered errors, or nil if no errors occurred.
func processActionAsyncResults(resultChan chan JobResult) (string, error) {
	// collectors for errors and successful results
	var collectedActionErrors []error
	var successfulResults []JobResult

	// Iterate over results received from the channel
	for result := range resultChan {
		if result.Err != nil {
			// Collect any errors encountered
			collectedActionErrors = append(collectedActionErrors, result.Err)
		} else {
			// Store successful results
			successfulResults = append(successfulResults, result)
		}
	}

	return formatResultsReport(collectedActionErrors, successfulResults)
}

// ProcessActionSyncResults collects results from a slice of synchronously executed actions.
// It aggregates any errors encountered during the execution and formats a success report
// by calling the internal formatResultsReport helper function.
// It takes a slice of ActionResult and returns a formatted string report or a joined error.
func ProcessActionSyncResults(results []JobResult) (string, error) {
	// collectors
	var collectedActionErrors []error
	var successfulResults []JobResult // Collect successful results in order

	// Separate errors and successes from the input slice
	for _, result := range results {
		if result.Err != nil {
			collectedActionErrors = append(collectedActionErrors, result.Err)
		} else {
			successfulResults = append(successfulResults, result)
		}
	}

	// Delegate processing and formatting to the helper function
	return formatResultsReport(collectedActionErrors, successfulResults)
}

// formatResultsReport takes collected errors and successful results, aggregates errors,
// and formats a success report. This is the core reusable logic.
func formatResultsReport(collectedActionErrors []error, successfulResults []JobResult) (string, error) {
	// Handling errors first.
	if len(collectedActionErrors) > 0 {
		// Use JoinErrors from err.go (assuming it's in the same package or imported)
		return "", JoinErrors(collectedActionErrors...)
	}

	// Handling, and showing outputs
	var outputBuilder strings.Builder
	outputBuilder.WriteString("All actions passed successfully.\n\nOutput:\n") // Simplified success message
	outputBuilder.WriteString("=====================\n")

	// Display successful outputs in the order they were collected
	if len(successfulResults) == 0 {
		outputBuilder.WriteString("(No successful actions with output)\n")
	} else {
		for _, result := range successfulResults {
			// Use the existing String() method or format directly
			outputBuilder.WriteString(result.String()) // Assumes ActionResult has a useful String() method
			// Ensure a newline after each result's output if not already present by String()
			if !strings.HasSuffix(result.String(), "\n") {
				outputBuilder.WriteString("\n")
			}
			outputBuilder.WriteString("--------------------\n") // Separator
		}
	}

	return outputBuilder.String(), nil // Return combined stdout and nil error
}

// String returns a formatted string representation of the ActionResult.
// It includes the working directory, the command executed, and any output or error messages.
// If there is an error associated with the ActionResult, it will be included in the output.
// If there is no output, a default message indicating that is returned.
//
// The format of the returned string is as follows:
//   - If there is an error: "WorkDir [<WorkDir>]: Error - <ErrorMessage>"
//   - If there is no error and output is present:
//     "--- WorkDir: <ExecutionPath> ---\nCommand: <CommandName>\n<Output>"
//   - If there is no error and no output:
//     "--- WorkDir: <ExecutionPath> ---\nCommand: <CommandName>\n(No standard output)"
func (ar JobResult) String() string {
	if ar.Err != nil {
		return fmt.Sprintf("WorkDir [%s]: Error - %v", ar.WorkDir, ar.Err)
	}

	output := "(No standard output)"

	if ar.Output != "" {
		output = ar.Output
	}

	// Format clearly indicating the unit and command (extracted from Unit key)
	parts := strings.Split(ar.WorkDir, ".")
	tgExecutionPath := ar.WorkDir
	commandName := ""

	if len(parts) == 2 {
		tgExecutionPath = parts[0]
		commandName = parts[1]
	}

	return fmt.Sprintf("--- WorkDir: %s ---\nCommand: %s\n%s", tgExecutionPath, commandName, output)
}
</file>

<file path="ci/ci-terragrunt/main.go">
package main

import (
	"context"
	"dagger/terragrunt/internal/dagger"
	"fmt"
	"path/filepath"
	"strings"
	"time"

	"github.com/google/uuid"
)

const (
	// Default version for binaries
	defaultTerraformVersion  = "1.10.1"
	defaultTerragruntVersion = "0.77.7"
	defaultImage             = "alpine"
	defaultImageTag          = "3.21.3"
	defaultMntPath           = "/mnt"
	defaultBinary            = "terragrunt"
	// Default Ref-Arch configuration details
	// TODO: change this later, according to your specific architecture.
	defaultRefArchEnv    = "global"
	defaulttRefArchLayer = "dni"
	defaulttRefArchUnit  = "dni_generator"
	// Default for AWS
	defaultAWSRegion              = "us-east-1"
	defaultAWSOidcTokenSecretName = "AWS_OIDC_TOKEN"
	// Configuration
	configRefArchRootPath                  = "infra/terragrunt"
	configRefArchATerraformModulesRootPath = "infra/terraform/modules"
	configterraformPluginCachePath         = "/root/.terraform.d/plugin-cache"
	configterragruntCachePath              = "/root/.terragrunt-cache"
	configNetrcRootPath                    = "/root/.netrc"
)

// Terragrunt represents a structure that encapsulates operations related to Terragrunt,
// a tool for managing Terraform configurations. This struct can be extended with methods
// that perform various tasks such as executing commands in containers, managing directories,
// and other functionalities that facilitate the use of Terragrunt in a Dagger pipeline.
type Terragrunt struct {
	// Ctr is a Dagger container that can be used to run Terragrunt commands
	Ctr *dagger.Container

	// Src is the source code for the Terragrunt project.
	Src *dagger.Directory
}

func New(
	// ctx is the context for the Dagger container.
	ctx context.Context,

	// imageURL is the URL of the image to use as the base container.
	// It should includes tags. E.g. "ghcr.io/devops-infra/docker-terragrunt:tf-1.9.5-ot-1.8.2-tg-0.67.4"
	// +optional
	imageURL string,

	// tgVersion (image tag) to use from the official Terragrunt image.
	//
	// +optional
	tgVersion string,

	// tfVersion is the Terraform version to use.
	//
	// +optional
	tfVersion string,

	// Ctr is the custom container to use for Terragrunt operations.
	//
	// +optional
	ctr *dagger.Container,

	// srcDir is the directory to mount as the source code.
	// +optional
	// +defaultPath="/"
	// +ignore=["*", "!**/*.hcl", "!**/*.tfvars", "!**/.git/**", "!**/*.tfvars.json", "!**/*.tf", "!*.env", "!*.envrc", "!*.envrc"]
	srcDir *dagger.Directory,

	// EnvVars are the environment variables that will be used to run the Terragrunt commands.
	//
	// +optional
	envVars []string,
) (*Terragrunt, error) {
	if ctr != nil {
		mod := &Terragrunt{Ctr: ctr}
		mod, enVarError := mod.WithEnvVars(envVars)
		if enVarError != nil {
			return nil, WrapErrorf(enVarError, "failed to initialise dagger module with environment variables")
		}

		modWithSRC, modWithSRCError := mod.WithSRC(ctx, defaultMntPath, srcDir)
		if modWithSRCError != nil {
			return nil, WrapErrorf(modWithSRCError, "failed to initialise dagger module with source directory")
		}

		mod = modWithSRC
		mod = mod.CommonSetup(tfVersion, tgVersion)

		return mod, nil
	}

	if imageURL != "" {
		mod := &Terragrunt{}
		mod.Ctr = dag.Container().From(imageURL)
		modWithSRC, modWithSRCError := mod.WithSRC(ctx, defaultMntPath, srcDir)
		if modWithSRCError != nil {
			return nil, WrapErrorf(modWithSRCError, "failed to initialise dagger module with source directory")
		}

		mod = modWithSRC
		mod, enVarError := mod.WithEnvVars(envVars)

		if enVarError != nil {
			return nil, WrapErrorf(enVarError, "failed to initialise dagger module with environment variables")
		}

		mod = mod.CommonSetup(tfVersion, tgVersion)
		return mod, nil
	}

	// We'll use the binary that should be downloaded from its source, or github repository.
	mod := &Terragrunt{}
	if tfVersion == "" {
		tfVersion = defaultTerraformVersion
	}

	if tgVersion == "" {
		tgVersion = defaultTerragruntVersion
	}

	defaultImageWithTag := fmt.Sprintf("%s:%s", defaultImage, defaultImageTag)

	mod.Ctr = dag.
		Container().
		From(defaultImageWithTag)

	modWithSRC, modWithSRCError := mod.WithSRC(ctx, defaultMntPath, srcDir)
	if modWithSRCError != nil {
		return nil, WrapErrorf(modWithSRCError, "failed to initialise dagger module with source directory")
	}

	mod = modWithSRC
	mod, enVarError := mod.WithEnvVars(envVars)

	if enVarError != nil {
		return nil, enVarError
	}

	mod = mod.CommonSetup(tfVersion, tgVersion)

	return mod, nil
}

// CommonSetup configures the Terragrunt container with common dependencies and settings.
// It installs Git, sets up specified Terraform and Terragrunt versions, and configures
// cache volumes for Terraform plugins and Terragrunt operations. It also enables the
// Terragrunt provider cache server.
//
// Parameters:
//   - tfVersion: The version of Terraform to install.
//   - tgVersion: The version of Terragrunt to install.
//
// Returns:
//   - The updated Terragrunt instance with common setup applied.
func (m *Terragrunt) CommonSetup(tfVersion, tgVersion string) *Terragrunt {
	m = m.
		WithGitPkgInstalled().
		WithTerraform(tfVersion).
		WithTerragrunt(tgVersion).
		WithTerraformPluginCache().
		WithTerragruntCache().
		WithTerragruntProvidersCacheServerEnabled()

	return m
}

// OpenTerminal returns a terminal
//
// It returns a terminal for the container.
// Arguments:
// - ctx: The context for the operation.
// - srcDir: The source directory to be mounted in the container. If nil, the default source directory is used.
// - loadEnvFiles: A boolean to load the environment files.
// Returns:
// - *dagger.Container: The terminal for the container.
func (m *Terragrunt) OpenTerminal(
	// ctx is the context for the operation.
	// +optional
	ctx context.Context,
	// srcDir is the source directory to be mounted in the container.
	// +optional
	srcDir *dagger.Directory,
	// loadEnvFiles is a boolean to load the environment files.
	// +optional
	loadEnvFiles bool,
) *dagger.Container {
	if srcDir == nil {
		srcDir = m.Src
	}

	if loadEnvFiles {
		m.WithDotEnvFile(ctx, m.Src)
	}

	return m.
		Ctr.
		Terminal()
}

// WithSRC mounts a source directory into the Terragrunt container.
//
// This method sets the working directory and mounts the provided directory,
// preparing the container for source code operations.
//
// Parameters:
//   - dir: A Dagger directory to be mounted in the container
//   - printPaths: A boolean to print the paths of the mounted directories.
//
// Returns:
//   - The updated Terragrunt instance with source directory mounted
func (m *Terragrunt) WithSRC(
	// ctx is the context for the Dagger container.
	ctx context.Context,
	// workdir is the working directory to set in the container.
	// +optional
	workdir string,
	// dir is the directory to mount in the container.
	dir *dagger.Directory,
) (*Terragrunt, error) {
	if workdir == "" {
		workdir = defaultMntPath
	} else {
		if workdir != defaultMntPath {
			workdir = filepath.Join(defaultMntPath, workdir)
		}
	}

	if err := isNonEmptyDaggerDir(ctx, dir); err != nil {
		return nil, WrapErrorf(err, "failed to validate the src/ directory passed")
	}

	m.Ctr = m.Ctr.
		WithWorkdir(workdir).
		WithMountedDirectory(workdir, dir)

	m.Src = dir

	return m, nil
}

// WithGitPkgInstalled installs the Git package in the container.
//
// This method adds the Git package to the container's package manager.
//
// Returns:
//   - The updated Terragrunt instance with Git installed
func (m *Terragrunt) WithGitPkgInstalled() *Terragrunt {
	m.Ctr = m.Ctr.
		WithExec([]string{"apk", "add", "git"})

	return m
}

// WithTFPluginCache mounts a cache volume for Terraform plugins.
//
// This method sets up a cache directory for Terraform plugins and mounts it into the container.
//
// Returns:
//   - The updated Terragrunt instance with the plugin cache mounted
func (m *Terragrunt) WithTerraformPluginCache() *Terragrunt {
	m.Ctr = m.Ctr.
		WithExec([]string{"mkdir", "-p", configterraformPluginCachePath}).
		WithExec([]string{"chmod", "755", configterraformPluginCachePath}).
		WithMountedCache(configterraformPluginCachePath, dag.CacheVolume("terraform-plugin-cache")).
		WithEnvVariable("TF_PLUGIN_CACHE_DIR", configterraformPluginCachePath)

	return m
}

// WithTerragruntProvidersCacheServerEnabled enables the Terragrunt providers cache server.
//
// This method sets the environment variable TG_PROVIDER_CACHE to "1" to enable the Terragrunt providers cache server.
//
// Returns:
//   - The updated Terragrunt instance with the providers cache server enabled
func (m *Terragrunt) WithTerragruntProvidersCacheServerEnabled() *Terragrunt {
	m.Ctr = m.Ctr.
		WithEnvVariable("TG_PROVIDER_CACHE", "1")

	return m
}

// WithTerragruntProvidersCacheServerDisabled disables the Terragrunt providers cache server.
//
// This method removes the environment variable TG_PROVIDER_CACHE from the container.
//
// Returns:
//   - The updated Terragrunt instance with the providers cache server disabled
func (m *Terragrunt) WithTerragruntProvidersCacheServerDisabled() *Terragrunt {
	m.Ctr = m.Ctr.
		WithoutEnvVariable("TG_PROVIDER_CACHE").
		WithEnvVariable("TG_PROVIDER_CACHE", "0")

	return m
}

// WithRegistriesToCacheProvidersFrom adds extra registries to cache providers from.
//
// This function appends the provided registries to the default list of registries in the Terragrunt configuration.
// By default, the Terragrunt provider's cache only caches registry.terraform.io and registry.opentofu.org.
//
// Parameters:
//   - registries: A slice of strings representing the registries to cache providers from.
//
// Returns:
//   - *Terragrunt: The updated Terragrunt instance with the extra registries to cache providers from.
func (m *Terragrunt) WithRegistriesToCacheProvidersFrom(
	// registries is a slice of strings representing the registries to cache providers from.
	registries []string,
) *Terragrunt {
	defaultRegistries := []string{
		"registry.terraform.io",
		"registry.opentofu.org",
	}

	registries = append(defaultRegistries, registries...)
	registryNames := strings.Join(registries, ",")

	m.Ctr = m.Ctr.
		WithoutEnvVariable("TERRAGRUNT_PROVIDER_CACHE_REGISTRY_NAMES").
		WithEnvVariable("TERRAGRUNT_PROVIDER_CACHE_REGISTRY_NAMES", registryNames)

	return m
}

// WithCacheBuster enables the cache buster for the container.
//
// This method sets the environment variable DAGGER_APT_CACHE_BUSTER to a unique value based on the current time.
//
// Returns:
//   - The updated Terragrunt instance with the cache buster enabled
func (m *Terragrunt) WithCacheBuster() *Terragrunt {
	m.Ctr = m.Ctr.
		WithEnvVariable("DAGGER_OPT_CACHE_BUSTER", fmt.Sprintf("%d", time.Now().Truncate(24*time.Hour).Unix()))

	return m
}

// WithTerragruntCache mounts a cache volume for Terragrunt.
//
// This method sets up a cache directory for Terragrunt and mounts it into the container.
//
// Returns:
//   - The updated Terragrunt instance with the cache mounted
func (m *Terragrunt) WithTerragruntCache() *Terragrunt {
	m.Ctr = m.Ctr.
		WithExec([]string{"mkdir", "-p", configterragruntCachePath}).
		WithExec([]string{"chmod", "755", configterragruntCachePath}).
		WithMountedCache(configterragruntCachePath, dag.CacheVolume("terragrunt-cache"))

	return m
}

// WithSecrets mounts secrets into the container.
//
// This method mounts secrets into the container for use by Terragrunt.
//
// Parameters:
//   - ctx: The context for the operation
//   - secrets: A slice of dagger.Secret instances to be mounted
//
// Returns:
//   - The updated Terragrunt instance with secrets mounted
func (m *Terragrunt) WithSecrets(ctx context.Context, secrets []*dagger.Secret) *Terragrunt {
	for _, secret := range secrets {
		// FIXME: This is suitable when secrets are created within dagger. Assumming there's a name set.
		secretName, _ := secret.Name(ctx)
		m.Ctr = m.Ctr.WithSecretVariable(secretName, secret)
	}

	return m
}

// WithEnvVars adds environment variables to the Terraformci container.
//
// This method allows setting multiple environment variables in key=value format.
// It performs validation to ensure each environment variable is correctly formatted.
//
// Parameters:
//   - envVars: A slice of environment variables in "KEY=VALUE" format
//
// Returns:
//   - The updated Terragrunt instance with environment variables set
//   - An error if any environment variable is incorrectly formatted
func (m *Terragrunt) WithEnvVars(envVars []string) (*Terragrunt, error) {
	envVarsDagger, err := getEnvVarsDaggerFromSlice(envVars)

	if err != nil {
		return nil, err
	}

	for _, envVar := range envVarsDagger {
		m.Ctr = m.Ctr.WithEnvVariable(envVar.Key, envVar.Value)
	}

	return m, nil
}

// WithToken adds a token to the Terragrunt container.
//
// This method adds a token to the container, making it available as an environment variable.
//
// Parameters:
//   - ctx: The context for the Dagger container.
//   - tokenValue: The value of the token to add to the container.
//
// Returns:
//   - The updated Terragrunt instance with the token added
func (m *Terragrunt) WithToken(ctx context.Context, tokenValue *dagger.Secret) *Terragrunt {
	return m.WithSecrets(ctx, []*dagger.Secret{tokenValue})
}

// WithTerraform sets the Terraform version to use and installs it.
// It takes a version string as an argument and returns a pointer to a dagger.Container.
func (m *Terragrunt) WithTerraform(version string) *Terragrunt {
	tfInstallationCmd := getTFInstallCmd(version)
	m.Ctr = m.Ctr.
		WithExec([]string{"/bin/sh", "-c", tfInstallationCmd}).
		WithExec([]string{"terraform", "--version"})

	return m
}

// WithTerragrunt sets the Terragrunt version to use and installs it.
// It takes a version string as an argument and returns a pointer to a dagger.Container.
func (m *Terragrunt) WithTerragrunt(version string) *Terragrunt {
	tgInstallationCmd := getTerragruntInstallationCommand(version)
	m.Ctr = m.Ctr.
		WithExec([]string{"/bin/sh", "-c", tgInstallationCmd}).
		WithExec([]string{"terragrunt", "--version"})

	return m
}

// WithNewNetrcFileGitHub creates a new .netrc file with the GitHub credentials.
//
// The .netrc file is created in the root directory of the container.
func (m *Terragrunt) WithNewNetrcFileGitHub(
	username string,
	password string,
) *Terragrunt {
	machineCMD := "machine github.com\nlogin " + username + "\npassword " + password + "\n"

	m.Ctr = m.Ctr.WithNewFile(configNetrcRootPath, machineCMD)

	return m
}

// WithNewNetrcFileAsSecretGitHub creates a new .netrc file with the GitHub credentials.
//
// The .netrc file is created in the root directory of the container.
// The argument 'password' is a secret that is not exposed in the logs.
func (m *Terragrunt) WithNewNetrcFileAsSecretGitHub(username string, password *dagger.Secret) *Terragrunt {
	passwordTxtValue, _ := password.Plaintext(context.Background())
	machineCMD := fmt.Sprintf("machine github.com\nlogin %s\npassword %s\n", username, passwordTxtValue)
	//nolint:exhaustruct // This is a method that is used to set the base image and version.
	m.Ctr = m.Ctr.WithNewFile(configNetrcRootPath, machineCMD)

	return m
}

// WithNewNetrcFileGitLab creates a new .netrc file with the GitLab credentials.
//
// The .netrc file is created in the root directory of the container.
func (m *Terragrunt) WithNewNetrcFileGitLab(
	username string,
	password string,
) *Terragrunt {
	machineCMD := "machine gitlab.com\nlogin " + username + "\npassword " + password + "\n"

	m.Ctr = m.Ctr.WithNewFile(configNetrcRootPath, machineCMD)

	return m
}

// WithNewNetrcFileAsSecretGitLab creates a new .netrc file with the GitLab credentials.
//
// The .netrc file is created in the root directory of the container.
// The argument 'password' is a secret that is not exposed in the logs.
func (m *Terragrunt) WithNewNetrcFileAsSecretGitLab(username string, password *dagger.Secret) *Terragrunt {
	passwordTxtValue, _ := password.Plaintext(context.Background())
	machineCMD := fmt.Sprintf("machine gitlab.com\nlogin %s\npassword %s\n", username, passwordTxtValue)

	//nolint:exhaustruct // This is a method that is used to set the base image and version.
	m.Ctr = m.Ctr.WithNewFile(configNetrcRootPath, machineCMD)

	return m
}

// WithSSHAuthSocket configures SSH authentication for Terraform modules with Git SSH sources.
//
// This function mounts an SSH authentication socket into the container, enabling Terraform to authenticate
// when fetching modules from Git repositories using SSH URLs (e.g., git@github.com:org/repo.git).
//
// Parameters:
//   - sshAuthSocket: The SSH authentication socket to mount in the container.
//   - socketPath: The path where the SSH socket will be mounted in the container.
//   - owner: Optional. The owner of the mounted socket in the container.
//
// Returns:
//   - *Terragrunt: The updated Terragrunt instance with SSH authentication configured for Terraform modules.
func (m *Terragrunt) WithSSHAuthSocket(
	// sshAuthSocket is the SSH socket to use for authentication.
	sshAuthSocket *dagger.Socket,
	// socketPath is the path where the SSH socket will be mounted in the container.
	// +optional
	socketPath string,
	// owner is the owner of the mounted socket in the container. Optional parameter.
	// +optional
	owner string,
) *Terragrunt {
	// Default the socket path if not provided
	if socketPath == "" {
		socketPath = "/ssh-agent.sock"
	}

	socketOpts := dagger.ContainerWithUnixSocketOpts{}

	if owner != "" {
		socketOpts.Owner = owner
	}

	m.Ctr = m.Ctr.
		WithUnixSocket(socketPath, sshAuthSocket, socketOpts).
		WithEnvVariable("SSH_AUTH_SOCK", socketPath)

	return m
}

// WithAWSCredentials sets the AWS credentials and region in the container.
//
// This method sets the AWS credentials and region in the container, making them available as environment variables.
// It also mounts the AWS credentials as secrets into the container.
//
// Parameters:
//   - ctx: The context for the Dagger container.
//   - awsAccessKeyID: The AWS access key ID.
//   - awsSecretAccessKey: The AWS secret access key.
//   - awsRegion: The AWS region.
//
// Returns:
//   - *Terragrunt: The updated Terragrunt instance with AWS credentials and region set
func (m *Terragrunt) WithAWSKeys(
	// ctx is the context for the Dagger container.
	// +optional
	ctx context.Context,
	// awsAccessKeyID is the AWS access key ID.
	awsAccessKeyID *dagger.Secret,
	// awsSecretAccessKey is the AWS secret access key.
	awsSecretAccessKey *dagger.Secret,
	// awsRegion is the AWS region.
	// +optional
	awsRegion string,
) *Terragrunt {
	awsRegion = getDefaultAWSRegionIfNotSet(awsRegion)

	m.Ctr = m.Ctr.
		WithEnvVariable("AWS_REGION", awsRegion).
		WithSecretVariable("AWS_ACCESS_KEY_ID", awsAccessKeyID).
		WithSecretVariable("AWS_SECRET_ACCESS_KEY", awsSecretAccessKey)

	return m
}

// WithAWSOIDC sets the AWS OIDC credentials in the container.
//
// This method sets the AWS OIDC credentials in the container, making them available as environment variables.
// It also mounts the AWS OIDC credentials as secrets into the container.
func (m *Terragrunt) WithAWSOIDC(
	// roleARN is the ARN of the IAM role to assume.
	roleARN string,
	// oidcToken is the Dagger Secret containing the OIDC JWT token from GitLab.
	oidcToken *dagger.Secret,
	// oidcTokenName is the name of the secret containing the OIDC JWT token from GitLab.
	// +optional
	oidcTokenName string,
	// awsRegion is the AWS region.
	// +optional
	awsRegion string,
	// awsRoleSessionName is an optional name for the assumed role session.
	// +optional
	awsRoleSessionName string,
) *Terragrunt {
	awsRegion = getDefaultAWSRegionIfNotSet(awsRegion)

	if oidcTokenName == "" {
		oidcTokenName = defaultAWSOidcTokenSecretName
	}

	if awsRoleSessionName == "" {
		awsRoleSessionName = fmt.Sprintf("terragrunt-dagger-%s", uuid.New().String())
	}

	oidcTokenPath := "run/secrets/" + oidcTokenName

	m.Ctr = m.Ctr.
		WithEnvVariable("AWS_REGION", awsRegion).
		WithEnvVariable("AWS_ROLE_ARN", roleARN).
		WithEnvVariable("AWS_ROLE_SESSION_NAME", awsRoleSessionName).
		WithEnvVariable("AWS_WEB_IDENTITY_TOKEN_FILE", oidcTokenPath).
		// cleaning ‚Äîif set‚Äî aws keys.
		WithoutEnvVariable("AWS_ACCESS_KEY_ID").
		WithoutEnvVariable("AWS_SECRET_ACCESS_KEY").
		WithoutEnvVariable("AWS_SESSION_TOKEN").
		WithSecretVariable(oidcTokenName, oidcToken)

	return m
}

// WithGitlabToken sets the GitLab token in the container.
//
// This method sets the GitLab token in the container, making it available as an environment variable.
//
// Parameters:
//   - ctx: The context for the Dagger container.
func (m *Terragrunt) WithGitlabToken(ctx context.Context, token *dagger.Secret) *Terragrunt {
	tokenTxtValue, _ := token.Plaintext(ctx)
	m.Ctr = m.Ctr.
		WithEnvVariable("GITLAB_TOKEN", tokenTxtValue)

	return m
}

// WithGitHubToken sets the GitHub token in the container.
//
// This method sets the GitHub token in the container, making it available as an environment variable.
//
// Parameters:
//   - ctx: The context for the Dagger container.
func (m *Terragrunt) WithGitHubToken(ctx context.Context, token *dagger.Secret) *Terragrunt {
	tokenTxtValue, _ := token.Plaintext(ctx)
	m.Ctr = m.Ctr.
		WithEnvVariable("GITHUB_TOKEN", tokenTxtValue)

	return m
}

// WithTerraformToken sets the Terraform token in the container.
//
// This method sets the Terraform token in the container, making it available as an environment variable.
//
// Parameters:
//   - ctx: The context for the Dagger container.
func (m *Terragrunt) WithTerraformToken(ctx context.Context, token *dagger.Secret) *Terragrunt {
	tokenTxtValue, _ := token.Plaintext(ctx)
	m.Ctr = m.Ctr.
		WithEnvVariable("TF_TOKEN", tokenTxtValue)

	return m
}

// WithTerragruntLogLevel sets the Terragrunt log level in the container.
//
// This method sets the Terragrunt log level in the container, making it available as an environment variable.
//
// Parameters:
//   - level: The log level to set.
func (m *Terragrunt) WithTerragruntLogLevel(level string) *Terragrunt {
	m.Ctr = m.Ctr.
		WithEnvVariable("TERRAGRUNT_LOG_LEVEL", level)

	return m
}

// WithTerragruntNonInteractive sets the Terragrunt non interactive option in the container.
//
// This method sets the Terragrunt non interactive option in the container, making it available as an environment variable.
//
// Parameters:
//   - level: The log level to set.
func (m *Terragrunt) WithTerragruntNonInteractive() *Terragrunt {
	m.Ctr = m.Ctr.
		WithEnvVariable("TERRAGRUNT_NON_INTERACTIVE", "true")

	return m
}

// WithTerragruntNoColor sets the Terragrunt no color option in the container.
//
// This method sets the Terragrunt no color option in the container, making it available as an environment variable.
//
// Parameters:
//   - level: The log level to set.
func (m *Terragrunt) WithTerragruntNoColor() *Terragrunt {
	m.Ctr = m.Ctr.
		WithEnvVariable("TERRAGRUNT_NO_COLOR", "true")

	return m
}

// WithDotEnvFile loads and processes environment variables from .env files in the provided directory.
//
// This method finds all .env files in the given directory, reads their contents, and sets
// environment variables in the Terragrunt container. Files containing "secret" in their name
// will have their values added as secret variables rather than regular environment variables.
//
// The method supports standard .env file formats with KEY=VALUE pairs on each line.
// Comments (lines starting with #) and empty lines are ignored. Values can be optionally
// quoted with single or double quotes, which will be automatically removed.
//
// Parameters:
//   - ctx: Context for the Dagger operations
//   - src: Directory containing the .env files to process
//
// Returns:
//   - *Terragrunt: The updated Terragrunt instance with environment variables set
//   - error: An error if file reading or parsing fails
func (m *Terragrunt) WithDotEnvFile(ctx context.Context, src *dagger.Directory) (*Terragrunt, error) {
	if src == nil {
		return nil, NewError("failed to load .env file, the source directory is nil")
	}

	// Check if there's any dotenv file on the source directory passed, or set.
	entries, err := src.Entries(ctx)
	if err != nil {
		return nil, WrapErrorf(err, "failed to list files in source directory")
	}

	foundDotEnvFiles := []string{}
	for _, entry := range entries {
		if strings.HasSuffix(entry, ".env") {
			foundDotEnvFiles = append(foundDotEnvFiles, entry)
		}
	}

	if len(foundDotEnvFiles) == 0 {
		return nil, NewError("No .env files found when inspecting the source directory")
	}

	dotEnvFilesInSrc, srcError := src.Glob(ctx, "*.env")

	if srcError != nil {
		return nil, WrapErrorf(srcError, "failed to glob dot env files")
	}

	ctrWithDotEnvFiles, dotEnvFilesParseErr := parseDotEnvFiles(ctx, m.Ctr, src, dotEnvFilesInSrc)

	if dotEnvFilesParseErr != nil {
		return nil, WrapErrorf(dotEnvFilesParseErr, "failed to parse dot env files")
	}

	m.Ctr = ctrWithDotEnvFiles

	return m, nil
}

// Exec executes a given command within a dagger container.
// It returns the output of the command or an error if the command is invalid or fails to execute.
//
//nolint:lll,cyclop // It's okay, since the ignore pattern is included
func (m *Terragrunt) Exec(
	// ctx is the context to use when executing the command.
	// +optional
	//nolint:contextcheck // It's okay, since the ignore pattern is included.
	ctx context.Context,
	// binary is the binary to execute, Possible and valid options are 'terragrunt', 'terraform'
	// +optional
	binary string,
	// command is the terragrunt command to execute. It's the actual command that comes after 'terragrunt'
	command string,
	// args are the arguments to pass to the command.
	// +optional
	args []string,
	// autoApprove is the flag to auto approve the command.
	// +optional
	autoApprove bool,
	// src is the source directory that includes the source code.
	src *dagger.Directory,
	// tgExecutionPath is the path to the terragrunt unit to execute.
	// +optional
	tgExecutionPath string,
	// envVars is the environment variables to pass to the container.
	// +optional
	envVars []string,
	// secrets is the secrets to pass to the container.
	// +optional
	secrets []*dagger.Secret,
	// tfToken is the Terraform registry token to pass to the container.
	// +optional
	tfToken *dagger.Secret,
	// ghToken is the GitHub token to pass to the container.
	// +optional
	ghToken *dagger.Secret,
	// glToken is the GitLab token to pass to the container.
	// +optional
	glToken *dagger.Secret,
	// gitSshSocket is the Git SSH socket that can be forwarded from the host, to allow private git support through SSH
	// +optional
	gitSshSocket *dagger.Socket,
	// printPaths is a boolean to print the paths of the mounted directories.
	// +optional
	printPaths bool,
	// loadEnvFiles is a boolean to load the environment files.
	// +optional
	loadEnvFiles bool,
	// tgOptLogLevel is the log level to set for the Terragrunt command.
	// +optional
	tgOptLogLevel string,
	// tgOptNoColor is a boolean to set the Terragrunt no color option.
	// +optional
	tgOptNoColor bool,
	// tgOptNonInteractive is a boolean to set the Terragrunt non interactive option.
	// +optional
	tgOptNonInteractive bool,
) (*dagger.Container, error) {
	cmdEntryPoint := getDefaultBinaryIfANotSet(binary)

	tgCmd := []string{cmdEntryPoint, command}

	if len(args) > 0 {
		tgCmd = append(tgCmd, args...)
	}

	if autoApprove && (command == "apply" || command == "destroy") {
		tgCmd = append(tgCmd, "--auto-approve")
	}

	if tgExecutionPath != "" && binary == "terragrunt" {
		tgCmd = append(tgCmd, "--working-dir", tgExecutionPath)
	}

	if binary == "terraform" {
		m.Ctr = m.Ctr.
			WithoutWorkdir().
			WithWorkdir(filepath.Join(defaultMntPath, tgExecutionPath))
	}

	if envVars != nil {
		modWithEnvVars, err := m.WithEnvVars(envVars)

		if err != nil {
			return nil, WrapError(err, "failed to set the environment variables")
		}

		m = modWithEnvVars
	}

	if secrets != nil {
		m = m.WithSecrets(ctx, secrets)
	}

	if tfToken != nil {
		m = m.WithTerraformToken(ctx, tfToken)
	}

	if ghToken != nil {
		m = m.WithGitHubToken(ctx, ghToken)
	}

	if glToken != nil {
		m = m.WithGitlabToken(ctx, glToken)
	}

	if gitSshSocket != nil {
		m = m.WithSSHAuthSocket(gitSshSocket, "/ssh-agent.sock", "root")
	}

	if tgOptLogLevel != "" {
		m = m.WithTerragruntLogLevel(tgOptLogLevel)
	}

	if tgOptNoColor {
		m = m.WithTerragruntNoColor()
	}

	if tgOptNonInteractive {
		m = m.WithTerragruntNonInteractive()
	}

	if loadEnvFiles {
		// At this point, m.Src is either the directory passed, or the one set at the constructor.
		modWithDotEnvLoaded, err := m.WithDotEnvFile(ctx, m.Src)

		if err != nil {
			return nil, WrapError(err, "failed to load .env files")
		}

		m = modWithDotEnvLoaded
	}

	return m.Ctr.WithExec(tgCmd), nil
}
</file>

<file path="ci/ci-terragrunt/utils.go">
package main

import (
	"context"
	"dagger/terragrunt/internal/dagger"
	"fmt"
	"path/filepath"
	"strings"
)

func getTFInstallCmd(tfVersion string) string {
	installDir := "/usr/local/bin/terraform"
	command := fmt.Sprintf(`apk add --no-cache curl unzip &&
	curl -L https://releases.hashicorp.com/terraform/%[1]s/terraform_%[1]s_linux_amd64.zip -o /tmp/terraform.zip &&
	unzip /tmp/terraform.zip -d /tmp &&
	mv /tmp/terraform %[2]s &&
	chmod +x %[2]s &&
	rm /tmp/terraform.zip`, tfVersion, installDir)

	return strings.TrimSpace(command)
}

func getTerragruntInstallationCommand(version string) string {
	installDir := "/usr/local/bin"
	installPath := filepath.Join(installDir, "terragrunt")
	command := fmt.Sprintf(`set -ex
curl -L https://github.com/gruntwork-io/terragrunt/releases/download/v%s/terragrunt_linux_amd64 -o %s
chmod +x %s`, version, installPath, installPath)

	return strings.TrimSpace(command)
}

func isNonEmptyDaggerDir(ctx context.Context, dir *dagger.Directory) error {
	if dir == nil {
		return fmt.Errorf("dagger directory cannot be nil")
	}

	entries, err := dir.Entries(ctx)
	if err != nil {
		return fmt.Errorf("failed to get entries from the dagger directory passed: %w", err)
	}

	if len(entries) == 0 {
		return fmt.Errorf("no entries found in the dagger directory passed")
	}

	return nil
}

func getDefaultAWSRegionIfNotSet(awsRegion string) string {
	if awsRegion == "" {
		return defaultAWSRegion
	}

	return awsRegion
}

func getDefaultBinaryIfANotSet(binary string) string {
	if binary == "" {
		return defaultBinary
	}

	return binary
}

func getTerragruntExecutionPath(env, layer, unit string) string {
	if env == "" {
		env = defaultRefArchEnv
	}

	if layer == "" {
		layer = defaulttRefArchLayer
	}

	if unit == "" {
		unit = defaulttRefArchUnit
	}

	return filepath.Join(configRefArchRootPath, env, layer, unit)
}

func getTerraformModulesExecutionPath(moduleName string) string {
	return filepath.Join(configRefArchATerraformModulesRootPath, moduleName)
}

type EnvVarDagger struct {
	Key   string
	Value string
}

func getEnvVarsDaggerFromSlice(envVars []string) ([]EnvVarDagger, error) {
	envVarsDagger := []EnvVarDagger{}
	for _, envVar := range envVars {
		trimmedEnvVar := strings.TrimSpace(envVar)
		if trimmedEnvVar == "" {
			return nil, NewError("environment variable cannot be empty")
		}

		if !strings.Contains(trimmedEnvVar, "=") {
			return nil, NewError(fmt.Sprintf("environment variable must be in the format ENVARKEY=VALUE: %s", trimmedEnvVar))
		}

		parts := strings.Split(trimmedEnvVar, "=")
		if len(parts) != 2 {
			return nil, NewError(fmt.Sprintf("environment variable must be in the format ENVARKEY=VALUE: %s", trimmedEnvVar))
		}

		envVarsDagger = append(envVarsDagger, EnvVarDagger{
			Key:   parts[0],
			Value: parts[1],
		})
	}

	return envVarsDagger, nil
}

// parseDotEnvFiles processes .env files found by WithDotEnvFile.
// It handles basic .env syntax including comments (#), empty lines,
// KEY=VALUE pairs, whitespace trimming, and basic quote removal (' or ").
func parseDotEnvFiles(ctx context.Context, container *dagger.Container, src *dagger.Directory, envFiles []string) (*dagger.Container, error) {
	for _, file := range envFiles {
		fileContent, err := src.File(file).Contents(ctx)
		if err != nil {
			// Wrap error for better context
			return nil, fmt.Errorf("failed to read dot env file '%s': %w", file, err)
		}

		lines := strings.Split(fileContent, "\n")

		for lineNum, line := range lines {
			trimmedLine := strings.TrimSpace(line)

			// Skip empty lines and comments
			if trimmedLine == "" || strings.HasPrefix(trimmedLine, "#") {
				continue
			}

			// Split line into key/value pair by the first '='
			parts := strings.SplitN(trimmedLine, "=", 2)
			if len(parts) != 2 {
				// Return error for lines without '='
				return nil, fmt.Errorf("invalid format in file '%s' on line %d: '%s'", file, lineNum+1, trimmedLine)
			}

			key := strings.TrimSpace(parts[0])
			value := strings.TrimSpace(parts[1])

			// Check for empty key
			if key == "" {
				return nil, fmt.Errorf("empty key found in file '%s' on line %d: '%s'", file, lineNum+1, trimmedLine)
			}

			// Trim surrounding quotes (basic handling)
			if len(value) >= 2 {
				if (value[0] == '"' && value[len(value)-1] == '"') || (value[0] == '\'' && value[len(value)-1] == '\'') {
					value = value[1 : len(value)-1]
				}
			}

			// Determine if it's a secret based on filename
			isSecret := strings.Contains(file, "secret")

			if isSecret {
				// Use a distinct name for the Dagger secret object itself
				secretName := fmt.Sprintf("%s_secret_%s", key, file)
				container = container.WithSecretVariable(key, dag.SetSecret(secretName, value))
			} else {
				container = container.WithEnvVariable(key, value)
			}
		}
	}

	return container, nil
}
</file>

<file path="docs/dagger.io/arguments.md">
---
slug: /api/arguments
---

# Arguments

Dagger Functions, just like regular functions, can accept arguments. In addition to basic types (string, boolean, integer, arrays...), Dagger also defines powerful core types which Dagger Functions can use for their arguments, such as `Directory`, `Container`, `Service`, `Secret`, and many more.

When calling a Dagger Function from the CLI, its arguments are exposed as command-line flags. How the flag is interpreted depends on the argument type.

> **Important:**
> Dagger Functions execute in containers and thus do not have default access to your host environment (host files, directories, environment variables, etc.). Access to these host resources can only be granted by explicitly passing them as argument values to the Dagger Function.
> - Files and directories: Dagger Functions can accept arguments of type `File` or `Directory`. Pass files and directories on your host by specifying their path as the value of the argument.
> - Environment variables: Pass environment variable values as argument values when invoking a function by just using the standard shell convention of using `$ENV_VAR_NAME.
> - Local network services: Dagger Functions that accept an argument of type `Service` can be passed local network services in the form `tcp://HOST:PORT`.

> **Note:**
> When passing values to Dagger Functions within Dagger Shell, required arguments are positional, while flags can be placed anywhere.

## String arguments

Here is an example of a Dagger Function that accepts a string argument:

### Go
```go
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"net/http"
)

type MyModule struct{}

type User struct {
	Title string `json:"title"`
	First string `json:"first"`
	Last  string `json:"last"`
}

// Returns users matching the provided gender
func (m *MyModule) GetUser(ctx context.Context, gender string) (*User, error) {
	resp, err := http.Get(fmt.Sprintf("https://randomuser.me/api/?gender=%s", gender))
	if err != nil {
		return nil, fmt.Errorf("failed to get user: %w", err)
	}
	defer resp.Body.Close()

	var result struct {
		Results []struct {
			Name User `json:"name"`
		} `json:"results"`
	}
	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return nil, fmt.Errorf("failed to decode response: %w", err)
	}

	if len(result.Results) == 0 {
		return nil, fmt.Errorf("no results found")
	}

	return &result.Results[0].Name, nil
}

```

### Python
```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def get_user(self, gender: str) -> str:
        """Returns users matching the provided gender"""
        # NOTE: this uses the http service included in the container, see
        # https://docs.dagger.io/manuals/developer/services/
        ctr = (
            dag.container()
            .from_("alpine")
            .with_exec(["apk", "add", "curl", "jq"])
            .with_exec(
                [
                    "curl",
                    "-s",
                    f"https://randomuser.me/api/?gender={gender}",
                ]
            )
        )

        # Use jq to extract the first name from the JSON response
        return await (
            ctr.with_exec(["jq", "-r", ".results[0].name | {title, first, last}"])
            .stdout()
        )

```

Even though the Python runtime doesn't enforce [type annotations][typing] at runtime,
it's important to define them with Dagger Functions. The Python SDK needs the
typing information at runtime to correctly report to the API. It can't rely on
[type inference][inference], which is only possible for external [static type
checkers][type-checker].

If a function doesn't have a return type annotation, it'll be declared as `None`,
which translates to the [dagger.Void][void] type in the API:

```python
@function
def hello(self):
    return "Hello world!"

# Error: cannot convert string to Void
```

It's fine however, when no value actually needs to be returned:

```python
@function
def hello(self):
    ...
    # no return
```

[@function]: https://dagger-io.readthedocs.io/en/latest/module.html#dagger.function
[@object_type]: https://dagger-io.readthedocs.io/en/latest/module.html#dagger.object_type
[typing]: https://docs.python.org/3/library/typing.html
[inference]: https://mypy.readthedocs.io/en/stable/type_inference_and_annotations.html
[type-checker]: https://realpython.com/python-type-checking/#static-type-checking
[void]: https://dagger-io.readthedocs.io/en/latest/client.html#dagger.Void


### TypeScript
```typescript
import { dag, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns users matching the provided gender
   */
  @func()
  async getUser(gender: string): Promise<string> {
    // NOTE: this uses the http service included in the container, see
    // https://docs.dagger.io/manuals/developer/services/
    const ctr = dag
      .container()
      .from("alpine")
      .withExec(["apk", "add", "curl", "jq"])
      .withExec([
        "curl",
        "-s",
        `https://randomuser.me/api/?gender=${gender}`,
      ])

    // Use jq to extract the first name from the JSON response
    return await ctr
      .withExec(["jq", "-r", ".results[0].name | {title, first, last}"])
      .stdout()
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns users matching the provided gender
     */
    #[DaggerFunction]
    public function getUser(string $gender): string
    {
        // NOTE: this uses the http service included in the container, see
        // https://docs.dagger.io/manuals/developer/services/
        $ctr = dag()
            ->container()
            ->from('alpine')
            ->withExec(['apk', 'add', 'curl', 'jq'])
            ->withExec([
                'curl',
                '-s',
                sprintf('https://randomuser.me/api/?gender=%s', $gender),
            ]);

        // Use jq to extract the first name from the JSON response
        return $ctr
            ->withExec(['jq', '-r', '.results[0].name | {title, first, last}'])
            ->stdout();
    }
}

```

Even though PHP doesn't enforce [type annotations][typing] at runtime,
it's important to define them with Dagger Functions. The PHP SDK needs the
typing information at runtime to correctly report to the API.

[typing]: https://www.php.net/manual/en/language.types.type-system.php


### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns users matching the provided gender
   */
  @Function
  public String getUser(String gender) throws Exception {
    // NOTE: this uses the http service included in the container, see
    // https://docs.dagger.io/manuals/developer/services/
    try (Client client = Dagger.connect()) {
      Container ctr = client
          .container()
          .from("alpine")
          .withExec(List.of("apk", "add", "curl", "jq"))
          .withExec(
              List.of(
                  "curl",
                  "-s",
                  String.format("https://randomuser.me/api/?gender=%s", gender)
              )
          );

      // Use jq to extract the first name from the JSON response
      return ctr
          .withExec(List.of("jq", "-r", ".results[0].name | {title, first, last}"))
          .stdout()
          .get();
    }
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'get-user male'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
get-user male
```

### Dagger CLI
```shell
dagger call get-user --gender=male
```

The result will look something like this:

```shell
{
  "title": "Mr",
  "first": "Hans-Werner",
  "last": "Thielen"
}
```

To pass host environment variables as arguments when invoking a Dagger Function, use the standard shell convention of `$ENV_VAR_NAME`.

Here is an example of passing a host environment variable containing a string value to a Dagger Function:

```shell
export GREETING=bonjour
```

### System shell
```shell
dagger -c 'github.com/shykes/daggerverse/hello@v0.3.0 | hello --greeting=$GREETING'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/shykes/daggerverse/hello@v0.3.0 | hello --greeting=$GREETING
```

### Dagger CLI
```shell
dagger -m github.com/shykes/daggerverse/hello@v0.3.0 call hello --greeting=$GREETING
```

## Boolean arguments

Here is an example of a Dagger Function that accepts a Boolean argument:

### Go
```go
package main

import (
	"strings"
)

type MyModule struct{}

// Returns a greeting message
func (m *MyModule) Hello(shout bool) string {
	msg := "Hello, world"
	if shout {
		msg = strings.ToUpper(msg)
	}
	return msg
}

```

### Python
```python
import dagger
from dagger import function, object_type


@object_type
class MyModule:
    @function
    def hello(self, shout: bool) -> str:
        """Returns a greeting message"""
        msg = "Hello, world"
        if shout:
            msg = msg.upper()
        return msg

```

### TypeScript
```typescript
import { func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a greeting message
   */
  @func()
  hello(shout: boolean): string {
    let msg = "Hello, world"
    if (shout) {
      msg = msg.toUpperCase()
    }
    return msg
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a greeting message
     */
    #[DaggerFunction]
    public function hello(bool $shout): string
    {
        $msg = 'Hello, world';
        if ($shout) {
            $msg = strtoupper($msg);
        }
        return $msg;
    }
}

```

### Java
> **Note:**
> You can either use the primitive `boolean` type or the boxed `java.lang.Boolean` type.

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

@Module
@Object
public class MyModule {

  /**
   * Returns a greeting message
   */
  @Function
  public String hello(boolean shout) {
    String msg = "Hello, world";
    if (shout) {
      msg = msg.toUpperCase();
    }
    return msg;
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'hello true'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
hello true
```

### Dagger CLI
```shell
dagger call hello --shout=true
```

The result will look like this:

```shell
HELLO, WORLD
```

> **Note:**
> When passing optional boolean flags:
> - To set the argument to true: `--foo=true` or `--foo`
> - To set the argument to false: `--foo=false`

## Integer arguments

Here is an example of a Dagger function that accepts an integer argument:

### Go
```go
package main

type MyModule struct{}

// Returns the sum of two integers
func (m *MyModule) AddInteger(a, b int) int {
	return a + b
}

```

### Python
```python
import dagger
from dagger import function, object_type


@object_type
class MyModule:
    @function
    def add_integer(self, a: int, b: int) -> int:
        """Returns the sum of two integers"""
        return a + b

```

### TypeScript
```typescript
import { func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns the sum of two integers
   */
  @func()
  addInteger(a: number, b: number): number {
    return a + b
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

#[DaggerObject]
class MyModule
{
    /**
     * Returns the sum of two integers
     */
    #[DaggerFunction]
    public function addInteger(int $a, int $b): int
    {
        return $a + $b;
    }
}

```

### Java
> **Note:**
> You can either use the primitive `int` type or the boxed `java.lang.Integer` type.

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

@Module
@Object
public class MyModule {

  /**
   * Returns the sum of two integers
   */
  @Function
  public int addInteger(int a, int b) {
    return a + b;
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'add-integer 1 2'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
add-integer 1 2
```

### Dagger CLI
```shell
dagger call add-integer --a=1 --b=2
```

The result will look like this:

```shell
3
```

## Floating-point number arguments

Here is an example of a Dagger function that accepts a floating-point number as argument:

### Go
```go
package main

type MyModule struct{}

// Returns the sum of two floats
func (m *MyModule) AddFloat(a, b float64) float64 {
	return a + b
}

```

### Python
```python
import dagger
from dagger import function, object_type


@object_type
class MyModule:
    @function
    def add_float(self, a: float, b: float) -> float:
        """Returns the sum of two floats"""
        return a + b

```

### TypeScript

> **Note:**
> There's no `float` type keyword in TypeScript because the type keyword `number` already supports floating point numbers.
>
> To declare a `float` argument on the function signature, import `float` from `@dagger.io/dagger` and use it as an argument's type.
> The imported `float` type is a `number` underneath, so you can use it as you would use a `number` inside your function.

```typescript
import { func, object, float } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns the sum of two floats
   */
  @func()
  addFloat(a: float, b: float): float {
    return a + b
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

#[DaggerObject]
class MyModule
{
    /**
     * Returns the sum of two floats
     */
    #[DaggerFunction]
    public function addFloat(float $a, float $b): float
    {
        return $a + $b;
    }
}

```

### Java
> **Note:**
> You can either use the primitive `float` type or the boxed `java.lang.Float` type.

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

@Module
@Object
public class MyModule {

  /**
   * Returns the sum of two floats
   */
  @Function
  public float addFloat(float a, float b) {
    return a + b;
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'add-float 1.4 2.7'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
add-float 1.4 2.7
```

### Dagger CLI
```shell
dagger call add-float --a=1.4 --b=2.7
```

The result will look like this:

```shell
4.1
```

## Array arguments

To pass an array argument to a Dagger Function, use a comma-separated list of values.

### Go
```go
package main

import (
	"strings"
)

type MyModule struct{}

// Returns a greeting message for a list of names
func (m *MyModule) Hello(names []string) string {
	return "Hello " + strings.Join(names, ", ")
}

```

### Python
```python
from typing import Annotated, List

import dagger
from dagger import Doc, function, object_type


@object_type
class MyModule:
    @function
    def hello(
        self,
        names: Annotated[
            List[str],
            Doc("List of names to greet"),
        ],
    ) -> str:
        """Returns a greeting message for a list of names"""
        return f"Hello {', '.join(names)}"

```

### TypeScript
```typescript
import { func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a greeting message for a list of names
   */
  @func()
  hello(names: string[]): string {
    return `Hello ${names.join(", ")}`
  }
}

```

### PHP
> **Note:**
> Lists must have their subtype specified by adding the `#[ListOfType]` attribute to the relevant function argument.
>
> The PHP SDK needs the typing information at runtime to correctly report to the API.

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Attribute\ListOfType;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a greeting message for a list of names
     *
     * @param string[] $names List of names to greet
     */
    #[DaggerFunction]
    public function hello(
        #[ListOfType('string')]
        array $names
    ): string {
        return 'Hello ' . implode(', ', $names);
    }
}

```

### Java
> **Note:**
> You can also use the `java.util.List` interface to represent a list of values.
> For instance instead of the `String[] names` argument in the example, you can have `List<String> names`.

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

@Module
@Object
public class MyModule {

  /**
   * Returns a greeting message for a list of names
   */
  @Function
  public String hello(String[] names) {
    return "Hello " + String.join(", ", names);
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'hello John,Jane'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
hello John,Jane
```

### Dagger CLI
```shell
dagger call hello --names=John,Jane
```

The result will look like this:

```shell
Hello John, Jane
```

## Directory arguments

You can also pass a directory argument from the command-line. To do so, add the corresponding flag, followed by a local filesystem path or a remote Git reference. In both cases, the CLI will convert it to an object referencing the contents of that filesystem path or Git repository location, and pass the resulting `Directory` object as argument to the Dagger Function.

Dagger Functions do not have access to the filesystem of the host you invoke the Dagger Function from (i.e. the host you execute a CLI command like `dagger call` from). Instead, host directories need to be explicitly passed as arguments to Dagger Functions.

Here's an example of a Dagger Function that accepts a `Directory` as argument. The Dagger Function returns a tree representation of the files and directories at that path.

### Go
```go
package main

import (
	"context"
	"fmt"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a tree representation of the directory's contents
func (m *MyModule) Tree(
	ctx context.Context,
	// Source directory
	src *dagger.Directory,
	// Depth of the tree
	depth int,
) (string, error) {
	return dag.Container().
		From("alpine:latest").
		WithExec([]string{"apk", "add", "tree"}).
		WithDirectory("/src", src).
		WithWorkdir("/src").
		WithExec([]string{"tree", "-L", fmt.Sprintf("%d", depth)}).
		Stdout(ctx)
}

```

### Python
```python
from typing import Annotated

import dagger
from dagger import Doc, dag, function, object_type


@object_type
class MyModule:
    @function
    async def tree(
        self,
        src: Annotated[
            dagger.Directory,
            Doc("Source directory"),
        ],
        depth: Annotated[
            int,
            Doc("Depth of the tree"),
        ],
    ) -> str:
        """Returns a tree representation of the directory's contents"""
        return await (
            dag.container()
            .from_("alpine:latest")
            .with_exec(["apk", "add", "tree"])
            .with_directory("/src", src)
            .with_workdir("/src")
            .with_exec(["tree", "-L", str(depth)])
            .stdout()
        )

```

### TypeScript
```typescript
import { dag, Directory, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a tree representation of the directory's contents
   *
   * @param src Source directory
   * @param depth Depth of the tree
   */
  @func()
  async tree(src: Directory, depth: number): Promise<string> {
    return await dag
      .container()
      .from("alpine:latest")
      .withExec(["apk", "add", "tree"])
      .withDirectory("/src", src)
      .withWorkdir("/src")
      .withExec(["tree", "-L", depth.toString()])
      .stdout()
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Directory;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a tree representation of the directory's contents
     */
    #[DaggerFunction]
    public function tree(
        /**
         * Source directory
         */
        Directory $src,

        /**
         * Depth of the tree
         */
        int $depth
    ): string {
        return dag()
            ->container()
            ->from('alpine:latest')
            ->withExec(['apk', 'add', 'tree'])
            ->withDirectory('/src', $src)
            ->withWorkdir('/src')
            ->withExec(['tree', '-L', (string) $depth])
            ->stdout();
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Directory;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a tree representation of the directory's contents
   */
  @Function
  public String tree(
      @Description("Source directory") Directory src,
      @Description("Depth of the tree") int depth) throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("alpine:latest")
          .withExec(List.of("apk", "add", "tree"))
          .withDirectory("/src", src)
          .withWorkdir("/src")
          .withExec(List.of("tree", "-L", String.valueOf(depth)))
          .stdout()
          .get();
    }
  }
}

```

Here is an example of passing a local directory to this Dagger Function as argument:

```shell
mkdir -p mydir/mysubdir
touch mydir/a mydir/b mydir/c mydir/mysubdir/y mydir/mysubdir/z
```

### System shell
```shell
dagger -c 'tree mydir 2'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
tree mydir 2
```

### Dagger CLI
```shell
dagger call tree --src=mydir --depth=2
```

The result will look like this:

```shell
.
‚îú‚îÄ‚îÄ a
‚îú‚îÄ‚îÄ b
‚îú‚îÄ‚îÄ c
‚îî‚îÄ‚îÄ mysubdir
    ‚îú‚îÄ‚îÄ y
    ‚îî‚îÄ‚îÄ z

2 directories, 5 files
```

Here is an example of passing a remote repository (Dagger's open-source repository) over HTTPS as a `Directory` argument:

### System shell
```shell
dagger <<EOF
container |
  from alpine:latest |
  with-directory /src https://github.com/dagger/dagger |
  with-exec ls /src |
  stdout
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
container |
  from alpine:latest |
  with-directory /src https://github.com/dagger/dagger |
  with-exec ls /src |
  stdout
```

### Dagger CLI
```shell
dagger core \
  container \
  from --address=alpine:latest \
  with-directory --path=/src --directory=https://github.com/dagger/dagger \
  with-exec --args="ls","/src" \
  stdout
```

The same repository can also be accessed using SSH. Note that this requires [SSH authentication to be properly configured](./remote-modules.md#ssh-authentication) on your Dagger host. Here is the same example, this time using SSH:

### System shell
```shell
dagger <<EOF
container |
  from alpine:latest |
  with-directory /src ssh://git@github.com/dagger/dagger |
  with-exec ls /src |
  stdout
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
container |
  from alpine:latest |
  with-directory /src ssh://git@github.com/dagger/dagger |
  with-exec ls /src |
  stdout
```

### Dagger CLI
```shell
dagger core \
  container \
  from --address=alpine:latest \
  with-directory --path=/src --directory=ssh://git@github.com/dagger/dagger \
  with-exec --args="ls","/src" \
  stdout
```

For more information about remote repository access, refer to the documentation on [reference schemes](#reference-schemes-for-remote-repositories) and [authentication methods](./remote-modules.md#authentication-methods).

> **Note:**
> Dagger offers two important features for working with `Directory` arguments:
> - [Default paths](./default-paths.md): Set a default directory path to use no value is specified for the argument.
> - [Filters](./fs-filters.md): Control which files and directories are uploaded to a Dagger Function.

## File arguments

File arguments work in the same way as [directory arguments](#directory-arguments). To pass a file to a Dagger Function as an argument, add the corresponding flag, followed by a local filesystem path or a remote Git reference. In both cases, the CLI will convert it to an object referencing that filesystem path or Git repository location, and pass the resulting `File` object as argument to the Dagger Function.

Here's an example of a Dagger Function that accepts a `File` as argument, reads it, and returns its contents:

### Go
```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns the contents of a file
func (m *MyModule) ReadFile(ctx context.Context, source *dagger.File) (string, error) {
	return source.Contents(ctx)
}

```

### Python
```python
from typing import Annotated

import dagger
from dagger import Doc, function, object_type


@object_type
class MyModule:
    @function
    async def read_file(
        self,
        source: Annotated[
            dagger.File,
            Doc("Source file"),
        ],
    ) -> str:
        """Returns the contents of a file"""
        return await source.contents()

```

### TypeScript
```typescript
import { File, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns the contents of a file
   *
   * @param source Source file
   */
  @func()
  async readFile(source: File): Promise<string> {
    return await source.contents()
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\File;

#[DaggerObject]
class MyModule
{
    /**
     * Returns the contents of a file
     */
    #[DaggerFunction]
    public function readFile(
        /**
         * Source file
         */
        File $source
    ): string {
        return $source->contents();
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.File;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

@Module
@Object
public class MyModule {

  /**
   * Returns the contents of a file
   */
  @Function
  public String readFile(@Description("Source file") File source) throws Exception {
    return source.contents().get();
  }
}

```

Here is an example of passing a local file to this Dagger Function as argument:

### System shell
```shell
dagger -c 'read-file /my/file/path/README.md'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
read-file /my/file/path/README.md
```

### Dagger CLI
```shell
dagger call read-file --source=/my/file/path/README.md
```

And here is an example of passing a file from a remote Git repository as argument:

### System shell
```shell
dagger -c 'read-file https://github.com/dagger/dagger.git#main:README.md'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
read-file https://github.com/dagger/dagger.git#main:README.md
```

### Dagger CLI
```shell
dagger call read-file --source=https://github.com/dagger/dagger.git#main:README.md
```

For more information about remote repository access, refer to the documentation on [reference schemes](#reference-schemes-for-remote-repositories) and [authentication methods](./remote-modules.md#authentication-methods).

> **Note:**
> Dagger offers two important features for working with `File` arguments:
> - [Default paths](./default-paths.md): Set a default file path to use no value is specified for the argument.
> - [Filters](./fs-filters.md): Control which files are uploaded to a Dagger Function.

## Container arguments

Just like directories, you can pass a container to a Dagger Function from the command-line. To do so, add the corresponding flag, followed by the address of an OCI image. The CLI will dynamically pull the image, and pass the resulting `Container` object as argument to the Dagger Function.

Here is an example of a Dagger Function that accepts a container image reference as an argument. The Dagger Function returns operating system information for the container.

### Go
```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns OS information for the container
func (m *MyModule) OsInfo(ctx context.Context, ctr *dagger.Container) (string, error) {
	return ctr.WithExec([]string{"uname", "-a"}).Stdout(ctx)
}

```

### Python
```python
from typing import Annotated

import dagger
from dagger import Doc, function, object_type


@object_type
class MyModule:
    @function
    async def os_info(
        self,
        ctr: Annotated[
            dagger.Container,
            Doc("Container to get OS information from"),
        ],
    ) -> str:
        """Returns OS information for the container"""
        return await ctr.with_exec(["uname", "-a"]).stdout()

```

### TypeScript
```typescript
import { Container, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns OS information for the container
   *
   * @param ctr Container to get OS information from
   */
  @func()
  async osInfo(ctr: Container): Promise<string> {
    return await ctr.withExec(["uname", "-a"]).stdout()
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Container;

#[DaggerObject]
class MyModule
{
    /**
     * Returns OS information for the container
     */
    #[DaggerFunction]
    public function osInfo(
        /**
         * Container to get OS information from
         */
        Container $ctr
    ): string {
        return $ctr->withExec(['uname', '-a'])->stdout();
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns OS information for the container
   */
  @Function
  public String osInfo(@Description("Container to get OS information from") Container ctr) throws Exception {
    return ctr.withExec(List.of("uname", "-a")).stdout().get();
  }
}

```

Here is an example of passing a container image reference to this Dagger Function as an argument.

### System shell
```shell
dagger -c 'os-info ubuntu:latest'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
os-info ubuntu:latest
```

### Dagger CLI
```shell
dagger call os-info --ctr=ubuntu:latest
```

The result will look like this:

```shell
Linux dagger 6.1.0-22-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21) x86_64 x86_64 x86_64 GNU/Linux
```

Here is another example of passing a container image reference to a Dagger Function as an argument. The Dagger Function scans the container using Trivy and reports any vulnerabilities found.

### System shell
```shell
dagger -c 'github.com/jpadams/daggerverse/trivy@v0.3.0 | scan-container index.docker.io/alpine:latest'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/jpadams/daggerverse/trivy@v0.3.0 | scan-container index.docker.io/alpine:latest
```

### Dagger CLI
```shell
dagger -m github.com/jpadams/daggerverse/trivy@v0.3.0 call scan-container --ctr=index.docker.io/alpine:latest
```

## Secret arguments

Dagger allows you to utilize confidential information, such as passwords, API keys, SSH keys and so on, in your Dagger [modules](../features/modules.md) and Dagger Functions, without exposing those secrets in plaintext logs, writing them into the filesystem of containers you're building, or inserting them into the cache.

Secrets can be passed to Dagger Functions as arguments using the `Secret` core type. Here is an example of a Dagger Function which accepts a GitHub personal access token as a secret, and uses the token to authorize a request to the GitHub API:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a list of issues from the Dagger repository
func (m *MyModule) GithubApi(ctx context.Context, token *dagger.Secret) (string, error) {
	return dag.Container().
		From("alpine:latest").
		WithExec([]string{"apk", "add", "curl"}).
		WithSecretVariable("GITHUB_API_TOKEN", token).
		WithExec([]string{
			"sh", "-c",
			`curl "https://api.github.com/repos/dagger/dagger/issues" -H "Authorization: Bearer $(cat /run/secrets/GITHUB_API_TOKEN)"`,
		}).
		Stdout(ctx)
}

```

### Python

```python
from typing import Annotated

import dagger
from dagger import Doc, Secret, dag, function, object_type


@object_type
class MyModule:
    @function
    async def github_api(
        self,
        token: Annotated[
            Secret,
            Doc("GitHub API token"),
        ],
    ) -> str:
        """Returns a list of issues from the Dagger repository"""
        return await (
            dag.container()
            .from_("alpine:latest")
            .with_exec(["apk", "add", "curl"])
            .with_secret_variable("GITHUB_API_TOKEN", token)
            .with_exec(
                [
                    "sh",
                    "-c",
                    'curl "https://api.github.com/repos/dagger/dagger/issues" -H "Authorization: Bearer $(cat /run/secrets/GITHUB_API_TOKEN)"',
                ]
            )
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, func, object, Secret } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a list of issues from the Dagger repository
   *
   * @param token GitHub API token
   */
  @func()
  async githubApi(token: Secret): Promise<string> {
    return await dag
      .container()
      .from("alpine:latest")
      .withExec(["apk", "add", "curl"])
      .withSecretVariable("GITHUB_API_TOKEN", token)
      .withExec([
        "sh",
        "-c",
        'curl "https://api.github.com/repos/dagger/dagger/issues" -H "Authorization: Bearer $(cat /run/secrets/GITHUB_API_TOKEN)"',
      ])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Secret;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a list of issues from the Dagger repository
     */
    #[DaggerFunction]
    public function githubApi(
        /**
         * GitHub API token
         */
        Secret $token
    ): string {
        return dag()
            ->container()
            ->from('alpine:latest')
            ->withExec(['apk', 'add', 'curl'])
            ->withSecretVariable('GITHUB_API_TOKEN', $token)
            ->withExec([
                'sh',
                '-c',
                'curl "https://api.github.com/repos/dagger/dagger/issues" -H "Authorization: Bearer $(cat /run/secrets/GITHUB_API_TOKEN)"',
            ])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Secret;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a list of issues from the Dagger repository
   */
  @Function
  public String githubApi(@Description("GitHub API token") Secret token) throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("alpine:latest")
          .withExec(List.of("apk", "add", "curl"))
          .withSecretVariable("GITHUB_API_TOKEN", token)
          .withExec(
              List.of(
                  "sh",
                  "-c",
                  "curl \"https://api.github.com/repos/dagger/dagger/issues\" -H \"Authorization: Bearer $(cat /run/secrets/GITHUB_API_TOKEN)\""))
          .stdout()
          .get();
    }
  }
}

```

The result will be a JSON-formatted list of issues from Dagger's repository.

When invoking the Dagger Function using the Dagger CLI, secrets can be sourced from multiple providers. Dagger can read secrets from the host environment, the host filesystem, and the result of host command execution, as well as from external secret managers [1Password](https://1password.com/) and [Vault](https://www.hashicorp.com/products/vault).

### Host secret providers

Here is an example call for this Dagger Function, with the secret sourced from a host environment variable named `GITHUB_API_TOKEN` via the `env` provider:

### System shell
```shell
dagger -c 'github-api env://GITHUB_API_TOKEN'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github-api env://GITHUB_API_TOKEN
```

### Dagger CLI
```shell
dagger call github-api --token=env://GITHUB_API_TOKEN
```

Secrets can also be passed from a host file using the `file` provider:

### System shell
```shell
dagger -c 'github-api file://./github.txt'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github-api file://./github.txt
```

### Dagger CLI
```shell
dagger call github-api --token=file://./github.txt
```

...or as the result of executing a command on the host using the `cmd` provider:

### System shell
```shell
dagger -c 'github-api cmd://"gh auth token"'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github-api cmd://"gh auth token"
```

### Dagger CLI
```shell
dagger call github-api --token=cmd://"gh auth token"
```

### External secret providers

Secrets can also be sourced from external secret managers. Currently, Dagger supports 1Password and Vault.

1Password requires creating a [service account](https://developer.1password.com/docs/service-accounts/get-started) and then setting the `OP_SERVICE_ACCOUNT_TOKEN` environment variable. Alternatively, if no `OP_SERVICE_ACCOUNT_TOKEN` is provided, the integration will attempt to execute the (official) `op` CLI if installed in the system.

1Password [secret references](https://developer.1password.com/docs/cli/secret-references/), in the format `op://VAULT-NAME/ITEM-NAME/[SECTION-NAME/]FIELD-NAME` are supported. Here is an example:

```shell
export OP_SERVICE_ACCOUNT_TOKEN="mytoken"
```

### System shell
```shell
dagger -c 'github-api op://infra/github/credential'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github-api op://infra/github/credential
```

### Dagger CLI
```shell
dagger call github-api --token=op://infra/github/credential
```

Vault can be authenticated with either token or AppRole methods. The Vault host can be specified by setting the environment variable `VAULT_ADDR`. For token authentication, set the environment variable `VAULT_TOKEN`. For AppRole authentication, set the environment variables `VAULT_APPROLE_ROLE_ID` and `VAULT_APPROLE_SECRET_ID`. Additional client configuration can be specified by the default environment variables accepted by Vault.

Vault KvV2 secrets are accessed with the scheme `vault://PATH/TO/SECRET.ITEM`. If your KvV2 is not mounted at `/secret`, specify the mount location with the environment variable `VAULT_PATH_PREFIX`. Here is an example:

```shell
export VAULT_ADDR='https://example.com:8200'
export VAULT_TOKEN=abcd_1234
```

### System shell
```shell
dagger -c 'github-api vault://infra/github.credential'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github-api vault://infra/github.credential
```

### Dagger CLI
```shell
dagger call github-api --token=vault://infra/github.credential
```

## Service arguments

Host network services or sockets can be passed to Dagger Functions as arguments. To do so, add the corresponding flag, followed by a service or socket reference.

### TCP and UDP services

To pass host TCP or UDP network services as arguments when invoking a Dagger Function, specify them in the form `tcp://HOST:PORT` or `udp://HOST:PORT`.

Assume that you have a PostgresQL database running locally on port 5432, as with:

```shell
docker run --rm -d -e POSTGRES_PASSWORD=postgres -p 5432:5432 postgres
```

Here is an example of passing this host service as argument to a PostgreSQL client Dagger Function, which drops you to a prompt from where you can execute SQL queries:

### System shell
```shell
dagger <<EOF
github.com/kpenfound/dagger-modules/postgres@v0.1.0 |
  client tcp://localhost:5432 postgres postgres postgres
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/kpenfound/dagger-modules/postgres@v0.1.0 | client tcp://localhost:5432 postgres postgres postgres
```

### Dagger CLI
```shell
dagger -m github.com/kpenfound/dagger-modules/postgres@v0.1.0 call \
  client --db=postgres --user=postgres --password=postgres --server=tcp://localhost:5432
```

### Unix sockets

Similar to host TCP/UDP services, Dagger Functions can also be granted access to host Unix sockets when the client is running on Linux or MacOS.

To pass host Unix sockets as arguments when invoking a Dagger Function, specify them by their path on the host.

For example, assuming you have Docker on your host with the Docker daemon listening on a Unix socket at `/var/run/docker.sock`, you can pass this socket to a Docker client Dagger Function as follows:

### System shell
```shell
dagger -c 'github.com/sipsma/daggerverse/docker-client@v0.0.1 /var/run/docker.sock | version'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/sipsma/daggerverse/docker-client@v0.0.1 /var/run/docker.sock | version
```

### Dagger CLI
```shell
dagger -m github.com/sipsma/daggerverse/docker-client@v0.0.1 call \
  --sock=/var/run/docker.sock version
```

## Optional arguments

Function arguments can be marked as optional. In this case, the Dagger CLI will not display an error if the argument is omitted in the function call.

Here's an example of a Dagger Function with an optional argument:

### Go
```go
package main

import (
	"fmt"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a greeting message
// +optional name
func (m *MyModule) Hello(name dagger.Optional[string]) string {
	val, ok := name.Get()
	if !ok {
		val = "world"
	}
	return fmt.Sprintf("Hello, %s", val)
}

```

### Python
```python
from typing import Annotated, Optional

import dagger
from dagger import Doc, function, object_type


@object_type
class MyModule:
    @function
    def hello(
        self,
        name: Annotated[
            Optional[str],
            Doc("Who to greet"),
        ] = None,
    ) -> str:
        """Returns a greeting message"""
        if name is None:
            name = "world"
        return f"Hello, {name}"

```

### TypeScript
```typescript
import { func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a greeting message
   *
   * @param name Who to greet
   */
  @func()
  hello(name?: string): string {
    if (!name) {
      name = "world"
    }
    return `Hello, ${name}`
  }
}

```

### PHP
> **Note:**
> The definition of optional varies between Dagger and PHP.
>
> An optional argument to PHP is one that has a default value.
>
> An optional argument to Dagger can be omitted entirely. It is truly optional.
>
> To specify a function argument as optional, simply make it nullable. When using the Dagger CLI, if the argument is omitted; the PHP SDK will treat this as receiving the value `null`.

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a greeting message
     */
    #[DaggerFunction]
    public function hello(
        /**
         * Who to greet
         */
        ?string $name = null
    ): string {
        if ($name === null) {
            $name = 'world';
        }
        return sprintf('Hello, %s', $name);
    }
}

```

### Java
> **Note:**
> Because of the usage of `Optional`, primitive types can not be marked as optional. You have to use the boxed types like `Integer` or `Boolean`.
>
> When an argument is not set as optional, Dagger will ensure the value is not `null` by adding a call to `Objects.requireNonNull` against the argument.

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

import java.util.Optional;

@Module
@Object
public class MyModule {

  /**
   * Returns a greeting message
   */
  @Function
  public String hello(@Description("Who to greet") Optional<String> name) {
    return String.format("Hello, %s", name.orElse("world"));
  }
}

```

Here is an example call for this Dagger Function, with the optional argument:

### System shell
```shell
dagger -c 'hello --name=John'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
hello --name=John
```

### Dagger CLI
```shell
dagger call hello --name=John
```

The result will look like this:

```shell
Hello, John
```

Here is an example call for this Dagger Function, without the optional argument:

### System shell
```shell
dagger -c hello
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
hello
```

### Dagger CLI
```shell
dagger call hello
```

The result will look like this:

```shell
Hello, world
```

## Default values

Function arguments can define a default value if no value is supplied for them.

Here's an example of a Dagger Function with a default value for a string argument:

### Go
```go
package main

import (
	"fmt"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a greeting message
// +default="world"
func (m *MyModule) Hello(name string) string {
	return fmt.Sprintf("Hello, %s", name)
}

```

### Python
```python
from typing import Annotated

import dagger
from dagger import Doc, function, object_type


@object_type
class MyModule:
    @function
    def hello(
        self,
        name: Annotated[
            str,
            Doc("Who to greet"),
        ] = "world",
    ) -> str:
        """Returns a greeting message"""
        return f"Hello, {name}"

```

### TypeScript
```typescript
import { func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a greeting message
   *
   * @param name Who to greet (default: "world")
   */
  @func()
  hello(name = "world"): string {
    return `Hello, ${name}`
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a greeting message
     */
    #[DaggerFunction]
    public function hello(
        /**
         * Who to greet
         */
        string $name = 'world'
    ): string {
        return sprintf('Hello, %s', $name);
    }
}

```

### Java
> **Note:**
> The default value provided must be a valid JSON string representation of the type.
>
> For instance if the argument is of type `Integer` and the default value is `123`, then the annotation must be `@Default("123")`.
> If the argument is of type `String` and the default value is `world`, then the annotation should be `@Default("\"world\"")`.
> In order to simplify this very specific case, if the argument is of type `String` and the value doesn't start with an escaped quote,
> then the SDK will automatically add the escaped quotes for you. That way you can simply write `@Default("world")`.

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Default;
import io.dagger.module.annotation.Description;

@Module
@Object
public class MyModule {

  /**
   * Returns a greeting message
   */
  @Function
  public String hello(@Description("Who to greet") @Default("world") String name) {
    return String.format("Hello, %s", name);
  }
}

```

Here is an example call for this Dagger Function, without the required argument:

### System shell
```shell
dagger -c hello
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
hello
```

### Dagger CLI
```shell
dagger call hello
```

The result will look like this:

```shell
Hello, world
```

Passing null to an optional argument signals that no default value should be used.

> **Note:**
> Dagger supports [default paths](./default-paths.md) for `Directory` or `File` arguments. Dagger will automatically use this default path when no value is specified for the corresponding argument.

## Reference schemes for remote repositories

Dagger supports the use of HTTP and SSH protocols for accessing files and directories in remote repositories, compatible with all major Git hosting platforms such as GitHub, GitLab, BitBucket, Azure DevOps, Codeberg, and Sourcehut. Dagger supports authentication via both HTTPS (using Git credential managers) and SSH (using a unified authentication approach).

Dagger supports the following reference schemes for file and directory arguments:

| Protocol | Scheme     | Authentication | Example |
|----------|------------|----------------|---------|
| HTTP(S)  | Git HTTP   | Git credential manager | `https://github.com/username/repo.git[#version[:subpath]]` |
| SSH      | Explicit   | SSH keys | `ssh://git@github.com/username/repo.git[#version[:subpath]]` |
| SSH      | SCP-like   | SSH keys | `git@github.com:username/repo.git[#version[:subpath]]`     |

> **Note:**
> The reference scheme for directory arguments is currently under discussion [here](https://github.com/dagger/dagger/issues/6957) and [here](https://github.com/dagger/dagger/issues/6944) and may change in future.

Dagger provides additional flexibility in referencing file and directory arguments through the following options:

- Version specification: Add `#version` to target a particular version of the repository. This can be a tag, branch name, or full commit hash. If omitted, the default branch is used.
- Monorepo support: Append `:subpath` after the version specification to access a specific subdirectory within the repository. Note that specifying a version is mandatory when including a subpath.

> **Important:**
> When referencing a specific subdirectory (subpath) within a repository, you must always include a version specification. The format is always `#version:subpath`.
</file>

<file path="docs/dagger.io/cache-volumes.md">
---
slug: /api/cache-volumes
---

# Cache Volumes

Volume caching involves caching specific parts of the filesystem and reusing them on subsequent function calls if they are unchanged. This is especially useful when dealing with package managers such as `npm`, `maven`, `pip` and similar. Since these dependencies are usually locked to specific versions in the application's manifest, re-downloading them on every session is inefficient and time-consuming. By using a cache volume for these dependencies, Dagger can reuse the cached contents across Dagger Function runs and reduce execution time.

Here's an example:

### System shell
```shell
dagger <<EOF
container |
  from node:21 |
  with-directory /src https://github.com/dagger/hello-dagger |
  with-workdir /src |
  with-mounted-cache /root/.npm node-21 |
  with-exec npm install
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
container |
  from node:21 |
  with-directory /src https://github.com/dagger/hello-dagger |
  with-workdir /src |
  with-mounted-cache /root/.npm node-21 |
  with-exec npm install
```

### Dagger CLI
```shell
dagger core container \
  from --address=node:21 \
  with-directory --path=/src --directory=https://github.com/dagger/hello-dagger \
  with-workdir --path=/src \
  with-mounted-cache --path=/root/.npm --cache=node-21 \
  with-exec --args="npm","install"
```

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

func (m *MyModule) Build(ctx context.Context) (string, error) {
	// create cache volume for npm dependencies
	npmCache := dag.CacheVolume("node-21")

	// get reference to source code directory
	src := dag.Git("https://github.com/dagger/hello-dagger").Branch("main").Tree()

	// build application
	return dag.Container().
		From("node:21").
		WithDirectory("/src", src).
		WithWorkdir("/src").
		// mount cache volume to /root/.npm
		WithMountedCache("/root/.npm", npmCache).
		WithExec([]string{"npm", "install"}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def build(self) -> str:
        """Build the application"""
        # create cache volume for npm dependencies
        npm_cache = dag.cache_volume("node-21")

        # get reference to source code directory
        src = dag.git("https://github.com/dagger/hello-dagger").branch("main").tree()

        # build application
        return await (
            dag.container()
            .from_("node:21")
            .with_directory("/src", src)
            .with_workdir("/src")
            # mount cache volume to /root/.npm
            .with_mounted_cache("/root/.npm", npm_cache)
            .with_exec(["npm", "install"])
            .stdout()
        )

```

### TypeScript

```typescript
import { CacheVolume, dag, Directory, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  @func()
  async build(): Promise<string> {
    // create cache volume for npm dependencies
    const npmCache: CacheVolume = dag.cacheVolume("node-21")

    // get reference to source code directory
    const src: Directory = dag
      .git("https://github.com/dagger/hello-dagger")
      .branch("main")
      .tree()

    // build application
    return await dag
      .container()
      .from("node:21")
      .withDirectory("/src", src)
      .withWorkdir("/src")
      // mount cache volume to /root/.npm
      .withMountedCache("/root/.npm", npmCache)
      .withExec(["npm", "install"])
      .stdout()
  }
}

```

This example will take some time to complete on the first run, as the cache volumes will not exist at that point. Subsequent runs will be significantly faster (assuming there is no other change), since Dagger will simply use the dependencies from the cache volumes instead of downloading them again.
</file>

<file path="docs/dagger.io/chaining.md">
---
slug: /api/chaining
---

# Chaining

Function chaining is one of Dagger's most powerful features, as it allows you to dynamically compose complex pipelines by connecting one Dagger Function with another. The following sections demonstrate a few more examples of function chaining with the Dagger CLI.

## Execute commands in containers

The Dagger CLI can add follow-up processing to a just-in-time container, essentially enabling you to continue the pipeline directly from the command-line. `Container` objects expose a `withExec()` API method, which lets you execute a command in the corresponding container.

Here is an example of chaining a `Container.withExec()` call to a container returned by a Wolfi container builder Dagger Function, to execute a command that displays the contents of the `/etc/` directory:

### System shell
```shell
dagger <<EOF
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container |
  with-exec ls /etc/ |
  stdout
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container |
  with-exec ls /etc/ |
  stdout
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call \
  container \
  with-exec --args="ls","/etc/" \
  stdout
```

Here is an example of chaining a `Container.withExec()` function call to a container returned by a Wolfi container builder Dagger Function, to execute a command that displays the contents of the `/etc/os-release` file:

### System shell
```shell
dagger <<EOF
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container |
  with-exec cat /etc/os-release |
  stdout
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container |
  with-exec cat /etc/os-release |
  stdout
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call \
  container \
  with-exec --args="cat","/etc/os-release" \
  stdout
```

Here is an example of chaining a `Container.withExec()` function call to do the reverse: modify a container returned by a Wolfi container builder Dagger Function, by removing the `/etc/os-release` file from the container filesysytem:

### System shell
```shell
dagger <<EOF
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container |
  with-exec rm /etc/os-release |
  with-exec ls /etc |
  stdout
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container |
  with-exec rm /etc/os-release |
  with-exec ls /etc |
  stdout
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call \
  container \
  with-exec --args="rm","/etc/os-release" \
  with-exec --args="ls","/etc" \
  stdout
```

Here is another example which chains multiple `Container.withExec()` calls to install the `curl` package in a Wolfi container, send an HTTP request, and return the output:

### System shell
```shell
dagger <<EOF
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container --packages curl |
  with-exec -- curl -L dagger.io |
  stdout
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container --packages curl |
  with-exec -- curl -L dagger.io |
  stdout
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call \
  container --packages="curl" \
  with-exec --args="curl","-L","dagger.io" \
  stdout
```

## Live-debug container builds

`Container` objects expose a `terminal()` API method, which lets you starting an ephemeral interactive terminal session in the corresponding container. This feature is very useful for debugging and experimenting since it allows you to inspect containers directly and at any stage of your Dagger Function execution.

Here is an example of chaining a `Container.terminal()` function call to start an interactive terminal in the container returned by a Wolfi container builder Dagger Function:

### System shell
```shell
dagger -c 'github.com/dagger/dagger/modules/wolfi@v0.16.2 | container --packages=cowsay | terminal'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/wolfi@v0.16.2 | container --packages=cowsay | terminal
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call \
  container --packages="cowsay" \
  terminal
```

By default, the terminal is started with the `sh` shell, although this can be overridden by adding the `--cmd` argument. To start the same terminal with the `zsh` shell, use:

### System shell
```shell
dagger -c 'github.com/dagger/dagger/modules/wolfi@v0.16.2 | container --packages=cowsay,zsh | terminal --cmd=zsh'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/wolfi@v0.16.2 | container --packages=cowsay,zsh | terminal --cmd=zsh
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call \
  container --packages="cowsay,zsh" \
  terminal --cmd=zsh
```

## Export directories, files and containers

When a host directory or file is copied or mounted to a container's filesystem, modifications made to it in the container do not automatically transfer back to the host. Data flows only one way between Dagger operations, because they are connected in a DAG. To transfer modifications back to the local host, you must explicitly export the directory or file back to the host filesystem.

Just-in-time artifacts such as containers, directories and files can be exported to the host filesystem from the Dagger Function that produced them using the `export` function. The destination path on the host is specified using the `--path` argument.

Here is an example of exporting the build directory returned by a Go builder Dagger Function to the `./my-build` directory on the host:

### System shell
```shell
dagger <<EOF
github.com/kpenfound/dagger-modules/golang@v0.2.1 |
  build ./cmd/dagger --source=https://github.com/dagger/dagger |
  export ./my-build
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/kpenfound/dagger-modules/golang@v0.2.1 |
  build ./cmd/dagger --source=https://github.com/dagger/dagger |
  export ./my-build
```

### Dagger CLI
```shell
dagger -m github.com/kpenfound/dagger-modules/golang@v0.2.1 call \
  build --source=https://github.com/dagger/dagger --args=./cmd/dagger \
  export --path=./my-build
```

By default, the `Directory.export()` method exports the files that exist in the returned directory to the host, but it does not modify or delete any files that already exist at that host path. To replace the contents of the target host directory, such that it exactly matches the directory being exported, add the optional `--wipe` argument.

Here is an example of exporting the build directory returned by the same Dagger Function above, deleting and replacing files as needed in the `./my-build` directory on the host:

### System shell
```shell
dagger <<EOF
github.com/kpenfound/dagger-modules/golang@v0.2.1 |
  build ./cmd/dagger --source=https://github.com/dagger/dagger |
  export ./my-build --wipe
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/kpenfound/dagger-modules/golang@v0.2.1 |
  build ./cmd/dagger --source=https://github.com/dagger/dagger |
  export ./my-build --wipe
```

### Dagger CLI
```shell
dagger -m github.com/kpenfound/dagger-modules/golang@v0.2.1 call \
  build --source=https://github.com/dagger/dagger --args=./cmd/dagger \
  export --path=./my-build --wipe
```

Instead of exporting an entire directory, you can also export a file. Here is an example of exporting the compiled binary file from the build directory, as `./my-file` on the host:

### System shell
```shell
dagger <<EOF
github.com/kpenfound/dagger-modules/golang@v0.2.1 |
  build ./cmd/dagger --source=https://github.com/dagger/dagger |
  file ./dagger |
  export ./my-file
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/kpenfound/dagger-modules/golang@v0.2.1 |
  build ./cmd/dagger --source=https://github.com/dagger/dagger |
  file ./dagger |
  export ./my-file
```

### Dagger CLI
```shell
dagger -m github.com/kpenfound/dagger-modules/golang@v0.2.1 call \
  build --source=https://github.com/dagger/dagger --args=./cmd/dagger \
  file ./dagger \
  export ./my-file
```

Here is another example, this time exporting the results of a `ruff` linter Dagger Function as `/tmp/report.json` on the host:

### System shell
```shell
dagger <<EOF
github.com/dagger/dagger/modules/ruff |
  lint https://github.com/dagger/dagger |
  report |
  export /tmp/report.json
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/ruff |
  lint https://github.com/dagger/dagger |
  report |
  export /tmp/report.json
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/ruff call \
  lint --source https://github.com/dagger/dagger \
  report \
  export --path=/tmp/report.json
```

Here is an example of exporting a container returned by a Wolfi container builder Dagger Function as an OCI tarball named `/tmp/tarball.tar` on the host:

### System shell
```shell
dagger -c 'github.com/dagger/dagger/modules/wolfi@v0.16.2 | container | export ./tarball.tar'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/wolfi@v0.16.2 | container | export ./tarball.tar
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call container export --path=./tarball.tar
```

## Inspect directories, files and containers

Here is an example of listing the contents of a directory returned by a Dagger Function, by chaining a call to the `Directory.entries()` method:

### System shell
```shell
dagger <<EOF
github.com/kpenfound/dagger-modules/golang@v0.2.1 |
  build . --source=https://github.com/golang/example#master:/hello |
  directory . |
  entries
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/kpenfound/dagger-modules/golang@v0.2.1 |
  build . --source=https://github.com/golang/example#master:/hello |
  directory . |
  entries
```

### Dagger CLI
```shell
dagger -m github.com/kpenfound/dagger-modules/golang@v0.2.1 call \
  build --source=https://github.com/golang/example#master:/hello --args=. \
  directory --path=. \
  entries
```

Here is an example of using the `File.contents()` method to print the JSON report of a linter run:

### System shell
```shell
dagger <<EOF
github.com/dagger/dagger/modules/ruff |
  lint https://github.com/dagger/dagger |
  report |
  contents
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/ruff |
  lint https://github.com/dagger/dagger |
  report |
  contents
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/ruff call \
  lint --source=https://github.com/dagger/dagger \
  report \
  contents
```

Similar, the `File` object exposes a method to return the size of the corresponding file. Here is an example of obtaining the size of the ZIP file returned by a file archiving Dagger Function, by chaining a call to `File.size()` method:

### System shell
```shell
dagger <<EOF
github.com/sagikazarmark/daggerverse/arc@40057665476af62e617cc8def9ef5a87735264a9 |
  archive-directory dagger-cli https://github.com/dagger/dagger#main:cmd/dagger |
  create zip |
  size
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/sagikazarmark/daggerverse/arc@40057665476af62e617cc8def9ef5a87735264a9 |
  archive-directory dagger-cli https://github.com/dagger/dagger#main:cmd/dagger |
  create zip |
  size
```

### Dagger CLI
```shell
dagger -m github.com/sagikazarmark/daggerverse/arc@40057665476af62e617cc8def9ef5a87735264a9 call \
  archive-directory --name=dagger-cli '--directory=https://github.com/dagger/dagger#main:cmd/dagger' \
  create --format=zip \
  size
```

## Publish containers

Every `Container` object exposes a `Container.publish()` API method, which publishes the container as a new image to a specified container registry. The registry address is passed to the function using the `--address` argument, and the return value is a string referencing the container image address in the registry.

Here is an example of publishing the container returned by a Wolfi container builder Dagger Function to the `ttl.sh` registry, by chaining a `Container.publish()` call:

### System shell
```shell
dagger -c 'github.com/dagger/dagger/modules/wolfi@v0.16.2 | container | publish ttl.sh/my-wolfi'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/wolfi@v0.16.2 | container | publish ttl.sh/my-wolfi
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call container publish --address=ttl.sh/my-wolfi
```

## Start containers as services

Every `Container` object exposes a `Container.asService()` API method, which turns the container into a `Service`. These services can then be spun up for use by other Dagger Functions or by clients on the Dagger host by forwarding their ports. This is akin to a "programmable docker-compose".

To start a `Service` returned by a Dagger Function and have it forward traffic to a specified address via the host, chain a call to the `Service.up()` API method.

Here is an example of starting an NGINX service on host port 80 by chaining calls to `Container.asService()` and `Service.up()`:

### System shell
```shell
dagger -c 'github.com/kpenfound/dagger-modules/nginx@v0.1.0 | container | as-service | up'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/kpenfound/dagger-modules/nginx@v0.1.0 | container | as-service | up
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call container as-service up
```

By default, each port maps to the same port on the host. To specify a different mapping, use the additional `--ports` argument with a list of host/service port mappings. To bind ports randomly, use the `--random` argument.

To start the same service and map NGINX port 80 to host port 8080, use:

### System shell
```shell
dagger -c 'github.com/kpenfound/dagger-modules/nginx@v0.1.0 | container | as-service | up --ports=8080:80'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/kpenfound/dagger-modules/nginx@v0.1.0 | container | as-service | up --ports=8080:80
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call container as-service up --ports=8080:80
```

The service can now be accessed on the specified port. For example, in another terminal, execute the following command to receive the default NGINX welcome page:

```shell
curl localhost:8080
```

To start the same service and map NGINX port 80 to a random port on the host, use:

### System shell
```shell
dagger -c 'github.com/kpenfound/dagger-modules/nginx@v0.1.0 | container | as-service | up --random'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/kpenfound/dagger-modules/nginx@v0.1.0 | container | as-service | up --random
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call container as-service up --random
```

## Modify container filesystems

Here is an example of modifying a container by adding the current directory from the host to the container filesysytem at `/src`, by chaining a call to the `Container.withDirectory()` method:

> **Warning:**
> The example below uploads the entire current directory to the container filesystem. This can take a significant amount of time with large directories. To reduce the time spent on upload, run this example from a directory containing only a few small files.

### System shell
```shell
dagger <<EOF
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container |
  with-directory /src . |
  with-exec ls /src |
  stdout
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container |
  with-directory /src . |
  with-exec ls /src |
  stdout
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call \
  container \
  with-directory --path=/src --directory=. \
  with-exec --args="ls","/src" \
  stdout
```

Here is an example of passing a `README.md` file from the host to a container builder Dagger Function, by chaining a call to the `Container.withFile()` function:

### System shell
```shell
dagger <<EOF
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container |
  with-file /README.md $(host | file ./README.md) |
  with-exec cat /README.md |
  stdout
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
github.com/dagger/dagger/modules/wolfi@v0.16.2 |
  container |
  with-file /README.md $(host | file ./README.md) |
  with-exec cat /README.md |
  stdout
```

### Dagger CLI
```shell
dagger -m github.com/dagger/dagger/modules/wolfi@v0.16.2 call \
  container \
  with-file --path=/README.md --source=./README.md \
  with-exec --args="cat","/README.md" \
  stdout
</file>

<file path="docs/dagger.io/circleci.md">
---
slug: /ci/integrations/circleci
---

# CircleCI

Dagger provides a programmable container engine that allows you to replace your YAML workflows in CircleCI with Dagger Functions written in a regular programming language. This allows you to execute your pipeline the same locally and in CI, with the additional benefit of intelligent caching.

## How it works

When running a CI pipeline with Dagger using CircleCI, the general workflow looks like this:

1. CircleCI receives a trigger based on a repository event.
1. CircleCI begins processing the jobs and steps in the `.circleci/config.yml` workflow file.
1. CircleCI downloads the Dagger CLI.
1. CircleCI executes one (or more) Dagger CLI commands, such as `dagger call ...`.
1. The Dagger CLI attempts to find an existing Dagger Engine or spins up a new one inside the CircleCI runner.
1. The Dagger CLI calls the specified Dagger Function and sends telemetry to Dagger Cloud if the `DAGGER_CLOUD_TOKEN` environment variable is set.
1. The pipeline completes with success or failure. Logs appear in CircleCI as usual.

> **Note:**
> In a Dagger context, you won't have access to CircleCI's test splitting functionality. You will need to implement your own test distribution logic or run all tests in a single execution.

## Prerequisites

- A CircleCI project
- A GitHub, Bitbucket or GitLab repository connected to the CircleCI project
- Docker, if using a [CircleCI execution environment](https://circleci.com/docs/executor-intro) other than `docker`

## Examples

The examples below use the `docker` executor, which come with a Docker execution environment preconfigured. If using a [different executor](https://circleci.com/docs/executor-intro), such as `machine`, you must install Docker in the execution environment before proceeding with the examples.

The following example demonstrates how to call a Dagger Function in a CircleCI workflow.

```yaml title=".circleci/config.yml"
# .circleci/config.yml
version: 2.1

jobs:
  hello:
    docker:
      - image: cimg/base:stable
    steps:
      - setup_remote_docker:
          version: 20.10.14
          docker_layer_caching: true
      - run:
          name: Install Dagger CLI
          command: |
            cd /usr/local
            curl -L https://dl.dagger.io/dagger/install.sh | sh
            cd bin
            sudo mv dagger /usr/local/bin
            dagger version
      - run:
          name: Run Dagger Function
          command: |
            # Replace with your Dagger Function call
            # Example: dagger call container-echo --string-arg="Hello from Dagger!" stdout
            # Example: dagger call test build publish --tag=ttl.sh/my-image
            dagger call container-echo --string-arg="Hello from Dagger!" stdout

workflows:
  dagger:
    jobs:
      - hello
```

The following is a more complex example demonstrating how to create a CircleCI workflow that checks out source code, calls a Dagger Function to test the project, and then calls another Dagger Function to build and publish a container image of the project. This example uses a simple [Go application](https://github.com/kpenfound/greetings-api) and assumes that you have already forked it in the repository connected to the CircleCI project.

```yaml title=".circleci/config.yml"
# .circleci/config.yml
version: 2.1

jobs:
  test:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - setup_remote_docker:
          version: 20.10.14
          docker_layer_caching: true
      - run:
          name: Install Dagger CLI
          command: |
            cd /usr/local
            curl -L https://dl.dagger.io/dagger/install.sh | sh
            cd bin
            sudo mv dagger /usr/local/bin
            dagger version
      - run:
          name: Run Dagger Function
          command: |
            # Replace with your Dagger Function call
            # Example: dagger call test
            dagger call test

  build:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - setup_remote_docker:
          version: 20.10.14
          docker_layer_caching: true
      - run:
          name: Install Dagger CLI
          command: |
            cd /usr/local
            curl -L https://dl.dagger.io/dagger/install.sh | sh
            cd bin
            sudo mv dagger /usr/local/bin
            dagger version
      - run:
          name: Run Dagger Function
          command: |
            # Replace with your Dagger Function call
            # Example: dagger call build publish --tag=ttl.sh/my-image
            dagger call build publish --tag=ttl.sh/my-image

workflows:
  dagger:
    jobs:
      - test
      - build:
          requires:
            - test
```

## Resources

If you have any questions about additional ways to use CircleCI with Dagger, join our [Discord](https://discord.gg/dagger-io) and ask your questions in our [help channel](https://discord.com/channels/707636530424053791/1030538312508776540).

## About CircleCI

[CircleCI](https://circleci.com/) is a popular CI/CD platform to test, build and deploy software applications.
</file>

<file path="docs/dagger.io/clients-cli.md">
---
slug: /api/cli
---

# Dagger CLI

The Dagger CLI lets you call both the core and extended Dagger API (the core APIs plus the new APIs provided by external Dagger modules) directly from the command-line.

You can call the API interactively (`dagger`) or non-interactively (`dagger -c`, `dagger call`, or `dagger core`).

Here are a few examples:

- Create a simple pipeline that is fully satisfied by the core Dagger API, without needing to program a Dagger module:

    ### System shell
    ```shell
    dagger <<EOF
    container |
      from cgr.dev/chainguard/wolfi-base |
      with-exec apk add go |
      with-directory /src https://github.com/golang/example#master |
      with-workdir /src/hello |
      with-exec -- go build -o hello . |
      file ./hello |
      export ./hello-from-dagger
    EOF
    ```

    ### Dagger Shell
    ```shell title="First type 'dagger' for interactive mode."
    container |
      from cgr.dev/chainguard/wolfi-base |
      with-exec apk add go |
      with-directory /src https://github.com/golang/example#master |
      with-workdir /src/hello |
      with-exec -- go build -o hello . |
      file ./hello |
      export ./hello-from-dagger
    ```

    ### Dagger CLI
    ```shell
    dagger core container from --address="cgr.dev/chainguard/wolfi-base" \
      with-exec --args="apk","add","go" \
      with-directory --path="/src" --directory="https://github.com/golang/example#master" \
      with-workdir --path="/src/hello" \
      with-exec --args="go","build","-o","hello","." \
      file --path="./hello" \
      export --path="./hello-from-dagger"
    ```

- Use Dagger as an alternative to `docker run`.

    ### System shell
    ```shell
    dagger -c 'container | from cgr.dev/chainguard/wolfi-base | terminal'
    ```

    ### Dagger Shell
    ```shell title="First type 'dagger' for interactive mode."
    container | from cgr.dev/chainguard/wolfi-base | terminal
    ```

    ### Dagger CLI
    ```shell
    dagger core container \
      from --address=cgr.dev/chainguard/wolfi-base \
      terminal
    ```

    > **Tip:**
    > If only the core Dagger API is needed, the `-M` (`--no-mod`) flag can be provided. This results in quicker startup, because the Dagger CLI doesn't try to find and load a current module. This also makes `dagger -M` equivalent to `dagger core`.

- Call one of the auto-generated Dagger Functions:

    ### System shell
    ```shell
    dagger -c 'container-echo "Welcome to Dagger!" | stdout'
    ```

    ### Dagger Shell
    ```shell title="First type 'dagger' for interactive mode."
    container-echo "Welcome to Dagger!" | stdout
    ```

    ### Dagger CLI
    ```shell
    dagger call container-echo --string-arg="Welcome to Dagger!" stdout
    ```

    > **Tip:**
    > When using the Dagger CLI, all names (functions, arguments, struct fields, etc) are converted into a shell-friendly "kebab-case" style.

- Modules don't need to be installed locally. Dagger lets you consume modules from GitHub repositories and call their Dagger Functions as though you were calling them locally:

    ### System shell
    ```shell
    dagger <<EOF
    github.com/jpadams/daggerverse/trivy@v0.5.0 |
      scan-image ubuntu:latest
    EOF
    ```

    ### Dagger Shell
    ```shell title="First type 'dagger' for interactive mode."
    github.com/jpadams/daggerverse/trivy@v0.5.0 | scan-image ubuntu:latest
    ```

    ### Dagger CLI
    ```shell
    dagger -m github.com/jpadams/daggerverse/trivy@v0.5.0 call \
      scan-image --image-ref=ubuntu:latest
    ```

- List all the Dagger Functions available in a module using context-sensitive help:

    ### System shell
    ```shell
    dagger -c '.help github.com/jpadams/daggerverse/trivy@v0.5.0'
    ```

    ### Dagger Shell
    ```shell title="First type 'dagger' for interactive mode."
    .help github.com/jpadams/daggerverse/trivy@v0.5.0
    ```

    ### Dagger CLI
    ```shell
    dagger -m github.com/jpadams/daggerverse/trivy@v0.5.0 call --help
    ```

- List all the optional and required arguments for a Dagger Function using context-sensitive help:

    ### System shell
    ```shell
    dagger -c 'github.com/jpadams/daggerverse/trivy@v0.5.0 | scan-image | .help'
    ```

    ### Dagger Shell
    ```shell title="First type 'dagger' for interactive mode."
    github.com/jpadams/daggerverse/trivy@v0.5.0 | scan-image | .help
    ```

    ### Dagger CLI
    ```shell
    dagger -m github.com/jpadams/daggerverse/trivy@v0.5.0 call scan-image --help
</file>

<file path="docs/dagger.io/clients-http.md">
---
slug: /api/http
---

# Raw HTTP

The Dagger API is an HTTP API that uses GraphQL as its low-level language-agnostic framework. Therefore, it's possible to call the Dagger API using raw HTTP queries, from [any language that supports GraphQL](https://graphql.org/code/). GraphQL has a large and growing list of client implementations in over 20 languages.

> **Note:**
> In practice, calling the API using HTTP or GraphQL is optional. Typically, you will instead use a custom Dagger function created with a type-safe Dagger SDK, or from the command line using the Dagger CLI.

Dagger creates a unique local API endpoint for GraphQL HTTP queries for every Dagger session. This API endpoint is served by the local host at the port specified by the `DAGGER_SESSION_PORT` environment variable, and can be directly read from the environment in your client code. For example, if `DAGGER_SESSION_PORT` is set to `12345`, the API endpoint can be reached at `http://127.0.0.1:$DAGGER_SESSION_PORT/query`

> **Warning:**
> Dagger protects the exposed API with an HTTP Basic authentication token which can be retrieved from the `DAGGER_SESSION_TOKEN` variable. Treat the `DAGGER_SESSION_TOKEN` value as you would any other sensitive credential. Store it securely and avoid passing it to, or over, insecure applications and networks.

## Command-line HTTP clients

This example demonstrates how to connect to the Dagger API and run a simple pipeline using `curl`:

```shell
echo '{"query":"{
  container {
    from(address:\"alpine:latest\") {
      file(path:\"/etc/os-release\") {
        contents
      }
    }
  }
}"}'|   dagger run sh -c 'curl -s \
    -u $DAGGER_SESSION_TOKEN: \
    -H "content-type:application/json" \
    -d @- \
    http://127.0.0.1:$DAGGER_SESSION_PORT/query'
```

## Language-native HTTP clients

This example demonstrates how to connect to the Dagger API and run a simple pipeline in the following languages:

- Rust, using the [gql_client library](https://github.com/arthurkhlghatyan/gql-client-rs) (MIT License)
- PHP, using the [php-graphql-client library](https://github.com/mghoneimy/php-graphql-client) (MIT License)

Create a new directory for the project and install the GraphQL client.

### Rust

```shell
mkdir my-project
cd my-project
cargo init
cargo add gql_client@1.0.7
cargo add serde_json@1.0.125
cargo add tokio@1.39.3 -F full
cargo add base64@0.22.1
```

### PHP

```shell
mkdir my-project
cd my-project
composer require gmostafa/php-graphql-client
```

Once the client library is installed, create a Dagger API client.

### Rust

Add the following code to `src/main.rs`:

```rust
use base64::{engine::general_purpose, Engine as _};
use gql_client::Client;
use serde_json::Value;
use std::collections::HashMap;
use std::env;

#[tokio::main]
async fn main() {
    let port = env::var("DAGGER_SESSION_PORT").expect("DAGGER_SESSION_PORT is not set");
    let token = env::var("DAGGER_SESSION_TOKEN").expect("DAGGER_SESSION_TOKEN is not set");
    let endpoint = format!("http://127.0.0.1:{}/query", port);

    let auth_header = format!(
        "Basic {}",
        general_purpose::STANDARD.encode(format!("{}:", token))
    );

    let mut headers = HashMap::new();
    headers.insert("Authorization", auth_header);

    let client = Client::new_with_headers(endpoint, headers);

    let query = r#"
        query {
            container {
                from(address: "alpine:latest") {
                    withExec(args: ["uname", "-a"]) {
                        stdout
                    }
                }
            }
        }
    "#;

    let response = client
        .query_unwrap::<Value>(query)
        .await
        .expect("GraphQL query failed");

    println!(
        "{}",
        response["container"]["from"]["withExec"]["stdout"]
            .as_str()
            .unwrap()
    );
}

```

### PHP

Create a new file named `client.php` and add the following code to it:

```php
<?php

require_once(__DIR__ . '/vendor/autoload.php');

use GraphQL\Client;
use GraphQL\Exception\QueryError;
use GraphQL\Query;

$port = getenv('DAGGER_SESSION_PORT');
$token = getenv('DAGGER_SESSION_TOKEN');
$endpoint = "http://127.0.0.1:$port/query";

$client = new Client(
    $endpoint,
    ['Authorization' => 'Basic ' . base64_encode($token . ':')]
);

$gql = (new Query('container'))
    ->setSelectionSet([
        (new Query('from'))
            ->setArguments(['address' => 'alpine:latest'])
            ->setSelectionSet([
                (new Query('withExec'))
                    ->setArguments(['args' => ['uname', '-a']])
                    ->setSelectionSet(['stdout'])
            ])
    ]);

try {
    $results = $client->runQuery($gql);
    print_r($results->getData()['container']['from']['withExec']['stdout']);
}
catch (QueryError $exception) {
    print_r($exception->getErrorDetails());
    exit;
}

?>

```

This code listing initializes the GraphQL client library and defines the Dagger pipeline to be executed as a Dagger API query. The `dagger run` command takes care of initializing a new local instance (or reusing a running instance) of the Dagger Engine on the host system and executing a specified command against it.

Run the Dagger API client using the Dagger CLI as follows:

### Rust
```shell
dagger run cargo run
```

### PHP
```shell
dagger run php client.php
```

Here is an example of the output:

```shell
dagger 6.1.0-23-cloud-amd64 unknown Linux
```

## Dagger CLI

The Dagger CLI offers a `dagger query` sub-command, which provides an easy way to send raw GraphQL queries to the Dagger API from the command line.

This example demonstrates how to build a Go application by cloning the [canonical Git repository for Go](https://go.googlesource.com/example/+/HEAD/hello) and building the "Hello, world" example program from it by calling the Dagger API via `dagger query`.

Create a new shell script named `build.sh` and add the following code to it:

```shell
#!/bin/sh

set -e

# get source code directory ID
src=$(dagger query <<EOF | jq -re '.git."https://go.googlesource.com/example".branch("master").tree("hello").id'
{
  git(url: "https://go.googlesource.com/example") {
    branch(name: "master") {
      tree(path: "hello") {
        id
      }
    }
  }
}
EOF
)

# build application using source code directory ID
dagger query <<EOF | jq -re '.container.build.file("hello").export("./dagger-builds-hello")'
{
  container {
    from(address: "golang:latest") {
      withDirectory(path: "/src", directory: "$src") {
        withWorkdir(path: "/src") {
          withExec(args: ["go", "build", "-o", "hello"]) {
            build: directory(path: ".") {
              file(path: "hello") {
                export(path: "./dagger-builds-hello")
              }
            }
          }
        }
      }
    }
  }
}
EOF

```

This script uses `dagger query` to send two GraphQL queries to the Dagger API. The first query returns a content-addressed identifier of the source code directory from the remote Git repository. This is interpolated into the second query, which initializes a new container, mounts the source code directory, compiles the source code, and writes the compiled binary back to the host filesystem.

Add the executable bit to the shell script and then run it by executing the commands below:

```shell
chmod +x ./build.sh
./build.sh
```

On completion, the built Go application will be available in the working directory on the host, as shown below:

```shell
tree
.
‚îú‚îÄ‚îÄ build.sh
‚îî‚îÄ‚îÄ dagger-builds-hello

1 directory, 2 files
</file>

<file path="docs/dagger.io/clients-sdk.md">
---
slug: /api/sdk
---

# Dagger SDKs

Dagger SDKs make it easy to call the Dagger API from your favorite programming language, by developing Dagger Functions or custom applications.

A Dagger SDK provides two components:

- A client library to call the Dagger API from your code
- Tooling to extend the Dagger API with your own Dagger Functions (bundled in a Dagger module)

The Dagger API uses GraphQL as its low-level language-agnostic framework, and can also be accessed using any standard GraphQL client. However, you do not need to know GraphQL to call the Dagger API; the translation to underlying GraphQL API calls is handled internally by the Dagger SDKs.

Official Dagger SDKs are currently available for Go, TypeScript and Python. There are also [experimental and community SDKs contributed by the Dagger community](https://github.com/dagger/dagger/tree/main/sdk).

## Dagger Functions

The recommended, and most common way, to interact with the Dagger API is through Dagger Functions. Dagger Functions are just regular code, written in your usual language using a type-safe Dagger SDK.

Dagger Functions are packaged, shared and reused using Dagger modules. A new Dagger module is initialized by calling `dagger init`. This creates a new `dagger.json` configuration file in the current working directory, together with sample Dagger Function source code. The configuration file will default the name of the module to the current directory name, unless an alternative is specified with the `--name` argument.

Once a module is initialized, `dagger develop --sdk=...` sets up or updates all the resources needed to develop the module locally using a Dagger SDK. By default, the module source code will be stored in the current working directory, unless an alternative is specified with the `--source` argument.

Here is an example of initializing a Dagger module:

### Go
```shell
dagger init --name=my-module
dagger develop --sdk=go
```

### Python
```shell
dagger init --name=my-module
dagger develop --sdk=python
```

### TypeScript
```shell
dagger init --name=my-module
dagger develop --sdk=typescript
```

### PHP
```shell
dagger init --name=my-module
dagger develop --sdk=php
```

### Java
```shell
dagger init --name=my-module
dagger develop --sdk=java
```

> **Warning:**
> Running `dagger develop` regenerates the module's code based on dependencies, the current state of the module, and the current Dagger API version. This can result in unexpected results if there are significant changes between the previous and latest installed Dagger API versions. Always refer to the [changelog](https://github.com/dagger/dagger/blob/main/CHANGELOG.md) for a complete list of changes (including breaking changes) in each Dagger release before running `dagger develop`, or use the `--compat=skip` option to bypass updating the Dagger API version.

The default template from `dagger develop` creates the following structure:

### Go

```
.
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ dagger.gen.go
‚îú‚îÄ‚îÄ dagger.json
‚îú‚îÄ‚îÄ go.mod
‚îú‚îÄ‚îÄ go.sum
‚îú‚îÄ‚îÄ internal
‚îÇ   ‚îú‚îÄ‚îÄ dagger
‚îÇ   ‚îú‚îÄ‚îÄ querybuilder
‚îÇ   ‚îî‚îÄ‚îÄ telemetry
‚îî‚îÄ‚îÄ main.go
```

In this structure:

- `dagger.json` is the [Dagger module configuration file](../configuration/modules.md).
- `go.mod`/`go.sum` manage the Go module and its dependencies.
- `main.go` is where your Dagger module code goes. It contains sample code to help you get started.
- `internal` contains automatically-generated types and helpers needed to configure and run the module:
    - `dagger` contains definitions for the Dagger API that's tied to the currently running Dagger Engine container.
    - `querybuilder` has utilities for building GraphQL queries (used internally by the `dagger` package).
    - `telemetry` has utilities for sending Dagger Engine telemetry.

### Python

```
.
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ uv.lock
‚îú‚îÄ‚îÄ sdk
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îî‚îÄ‚îÄ my_module
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ main.py
‚îî‚îÄ‚îÄ dagger.json
```

In this structure:

- `dagger.json` is the [Dagger module configuration file](../configuration/modules.md).
- `pyproject.toml` manages the Python project configuration.
- `uv.lock` manages the module's pinned dependencies.
- `src/my_module/` is where your Dagger module code goes. It contains sample code to help you get started.
- `sdk/` contains the vendored Python SDK [client library](https://pypi.org/project/dagger-io/).

This structure hosts a Python import package, with a name derived from the project name (in `pyproject.toml`), inside a `src` directory. This follows a [Python convention](https://packaging.python.org/en/latest/discussions/src-layout-vs-flat-layout/) that requires a project to be installed in order to run its code. This convention prevents accidental usage of development code since the Python interpreter includes the current working directory as the first item on the import path (more information is available in this [blog post on Python packaging](https://blog.ionelmc.ro/2014/05/25/python-packaging/)).

### TypeScript

```
.
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ dagger.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ sdk
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îî‚îÄ‚îÄ index.ts
‚îú‚îÄ‚îÄ tsconfig.json
‚îî‚îÄ‚îÄ yarn.lock
```

In this structure:

- `dagger.json` is the [Dagger module configuration file](../configuration/modules.md).
- `package.json` manages the module dependencies.
- `src/` is where your Dagger module code goes. It contains sample code to help you get started.
- `sdk/` contains the TypeScript SDK.

### PHP

```
.
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ composer.json
‚îú‚îÄ‚îÄ composer.lock
‚îú‚îÄ‚îÄ entrypoint.php
‚îú‚îÄ‚îÄ sdk
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îî‚îÄ‚îÄ MyModule.php
‚îî‚îÄ‚îÄ vendor
```

In this structure:

- `dagger.json` is the [Dagger module configuration file](../configuration/modules.md).
- `composer.json` manages the module dependencies.
- `src/` is where your Dagger module code goes. It contains sample code to help you get started.
- `sdk/` contains the PHP SDK.

### Java
```
.
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ dagger.json
‚îú‚îÄ‚îÄ pom.xml
‚îú‚îÄ‚îÄ src
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ main
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ java
‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ io
‚îÇ¬†¬†             ‚îî‚îÄ‚îÄ dagger
‚îÇ¬†¬†                 ‚îî‚îÄ‚îÄ modules
‚îÇ¬†¬†                     ‚îî‚îÄ‚îÄ mymodule
‚îÇ¬†¬†                         ‚îú‚îÄ‚îÄ MyModule.java
‚îÇ¬†¬†                         ‚îî‚îÄ‚îÄ package-info.java
‚îî‚îÄ‚îÄ target

9 directories, 5 files
```

In this structure:

- `dagger.json` is the [Dagger module configuration file](../configuration/modules.md).
- `pom.xml` manages the module dependencies.
- `src/main/java/io/dagger/modules/mymodule/` is where your Dagger module code goes. It contains sample code to help you get started.
- `target` contains the generated Java source classes.

> **Note:**
> While you can use the utilities defined in the automatically-generated code above, you *cannot* edit these files. Even if you edit them locally, any changes will not be persisted when you run the module.

You can now write Dagger Functions using the selected Dagger SDK. Here is an example, which calls a remote API method and returns the result:

### Go
```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns users matching the provided gender
func (m *MyModule) GetUser(ctx context.Context) (string, error) {
	// NOTE: this uses the http service included in the container, see
	// https://docs.dagger.io/manuals/developer/services/
	ctr := dag.Container().
		From("alpine").
		WithExec([]string{"apk", "add", "curl", "jq"}).
		WithExec([]string{
			"curl",
			"-s",
			"https://randomuser.me/api/",
		})

	// Use jq to extract the first name from the JSON response
	return ctr.
		WithExec([]string{"jq", "-r", ".results[0].name | {title, first, last}"}).
		Stdout(ctx)
}

```

This Dagger Function includes the context as input and error as return in its signature.

### Python
```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def get_user(self) -> str:
        """Returns users matching the provided gender"""
        # NOTE: this uses the http service included in the container, see
        # https://docs.dagger.io/manuals/developer/services/
        ctr = (
            dag.container()
            .from_("alpine")
            .with_exec(["apk", "add", "curl", "jq"])
            .with_exec(
                [
                    "curl",
                    "-s",
                    "https://randomuser.me/api/",
                ]
            )
        )

        # Use jq to extract the first name from the JSON response
        return await (
            ctr.with_exec(["jq", "-r", ".results[0].name | {title, first, last}"])
            .stdout()
        )

```

Dagger Functions are implemented as [@dagger.function][@function] decorated
methods, of a [@dagger.object_type][@object_type] decorated class.

It's possible for a module to implement multiple classes (*object types*), but
**the first one needs to have a name that matches the module's name**, in
*PascalCase*. This object is sometimes referred to as the *main object*.

For example, for a module initialized with `dagger init --name=my-module`,
the main object needs to be named `MyModule`.

[@function]: https://dagger-io.readthedocs.io/en/latest/module.html#dagger.function
[@object_type]: https://dagger-io.readthedocs.io/en/latest/module.html#dagger.object_type

### TypeScript
```typescript
import { dag, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns users matching the provided gender
   */
  @func()
  async getUser(): Promise<string> {
    // NOTE: this uses the http service included in the container, see
    // https://docs.dagger.io/manuals/developer/services/
    const ctr = dag
      .container()
      .from("alpine")
      .withExec(["apk", "add", "curl", "jq"])
      .withExec(["curl", "-s", "https://randomuser.me/api/"])

    // Use jq to extract the first name from the JSON response
    return await ctr
      .withExec(["jq", "-r", ".results[0].name | {title, first, last}"])
      .stdout()
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns users matching the provided gender
     */
    #[DaggerFunction]
    public function getUser(): string
    {
        // NOTE: this uses the http service included in the container, see
        // https://docs.dagger.io/manuals/developer/services/
        $ctr = dag()
            ->container()
            ->from('alpine')
            ->withExec(['apk', 'add', 'curl', 'jq'])
            ->withExec([
                'curl',
                '-s',
                'https://randomuser.me/api/',
            ]);

        // Use jq to extract the first name from the JSON response
        return $ctr
            ->withExec(['jq', '-r', '.results[0].name | {title, first, last}'])
            ->stdout();
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns users matching the provided gender
   */
  @Function
  public String getUser() throws Exception {
    // NOTE: this uses the http service included in the container, see
    // https://docs.dagger.io/manuals/developer/services/
    try (Client client = Dagger.connect()) {
      Container ctr = client
          .container()
          .from("alpine")
          .withExec(List.of("apk", "add", "curl", "jq"))
          .withExec(
              List.of(
                  "curl",
                  "-s",
                  "https://randomuser.me/api/"
              )
          );

      // Use jq to extract the first name from the JSON response
      return ctr
          .withExec(List.of("jq", "-r", ".results[0].name | {title, first, last}"))
          .stdout()
          .get();
    }
  }
}

```

> **Caution:**
> You can try this Dagger Function by copying it into the default template generated by `dagger init`, but remember that you must update the module name in the code samples above to match the name used when your module was first initialized.

In simple terms, this Dagger Function:

- initializes a new container from an `alpine` base image.
- executes the `apk add ...` command in the container to add the `curl` and `jq` utilities.
- uses the `curl` utility to send an HTTP request to the URL `https://randomuser.me/api/` and parses the response using `jq`.
- retrieves and returns the output stream of the last executed command as a string.

> **Important:**
> Every Dagger Function has access to the `dag` client, which is a pre-initialized Dagger API client. This client contains all the core types (like `Container`, `Directory`, etc.), as well as bindings to any dependencies your Dagger module has declared.

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'get-user'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
get-user
```

### Dagger CLI
```shell
dagger call get-user
```

Here's what you should see:

```shell
{
  "title": "Mrs",
  "first": "Beatrice",
  "last": "Lavigne"
}
```

> **Important:**
> Dagger Functions execute within containers spawned by the Dagger Engine. This "sandboxing" serves a few important purposes:
> 1. Reproducibility: Executing in a well-defined and well-controlled container ensures that a Dagger Function runs the same way every time it is invoked. It also guards against creating "hidden dependencies" on ambient properties of the execution environment that could change at any moment.
> 1. Caching: A reproducible containerized environment makes it possible to cache the result of Dagger Function execution, which in turn allows Dagger to automatically speed up operations.
> 1. Security: Even when running third-party Dagger Functions sourced from a Git repository, those Dagger Functions will not have default access to your host environment (host files, directories, environment variables, etc.). Access to these host resources can only be granted by explicitly passing them as argument values to the Dagger Function.

## Custom applications

An alternative approach is to develop a custom application using a Dagger SDK. This involves:

- installing the SDK for your selected language in your development environment
- initializing a Dagger API client in your application code
- calling and combining Dagger API methods from your application to achieve the required result
- executing your application using `dagger run`

### Go

> **Note:**
> The Dagger Go SDK requires [Go 1.22 or later](https://go.dev/doc/install).

From an existing Go module, install the Dagger Go SDK using the commands below:

```shell
go get dagger.io/dagger@latest
```

After importing `dagger.io/dagger` in your Go module code, run the following command to update `go.sum`:

```shell
go mod tidy
```

This example demonstrates how to build a Go application for multiple architectures and Go versions using the Go SDK.

Clone an example project and create a new Go module in the project directory:

```shell
git clone https://go.googlesource.com/example
cd example/hello
mkdir multibuild && cd multibuild
go mod init multibuild
```

Create a new file in the `multibuild` directory named `main.go` and add the following code to it:

```go
package main

import (
	"context"
	"fmt"
	"os"
	"runtime"

	"dagger.io/dagger"
	"golang.org/x/sync/errgroup"
)

func main() {
	if err := build(context.Background()); err != nil {
		panic(err)
	}
}

func build(ctx context.Context) error {
	client, err := dagger.Connect(ctx, dagger.WithLogOutput(os.Stderr))
	if err != nil {
		return err
	}
	defer client.Close()

	// get reference to the local project
	src := client.Host().Directory("../..")

	// create a matrix of Go versions and architectures
	goVersions := []string{"1.22", "1.23"}
	oses := []string{"linux", "darwin"}
	arches := []string{"amd64", "arm64"}

	eg, gctx := errgroup.WithContext(ctx)
	eg.SetLimit(runtime.NumCPU())

	for _, version := range goVersions {
		for _, goos := range oses {
			for _, goarch := range arches {
				// create a closure to capture the loop variables
				version, goos, goarch := version, goos, goarch

				// create a new container for each Go version, OS, and architecture
				image := fmt.Sprintf("golang:%s", version)
				builder := client.Container().From(image).
					WithDirectory("/src", src).
					WithWorkdir("/src/hello").
					WithEnvVariable("GOOS", goos).
					WithEnvVariable("GOARCH", goarch)

				// run the build command
				path := fmt.Sprintf("build/%s/%s/%s/", version, goos, goarch)
				eg.Go(func() error {
					outpath := fmt.Sprintf("%s/hello", path)
					build := builder.WithExec([]string{"go", "build", "-o", outpath})

					// get the build output directory
					output := build.Directory(path)

					// write the build output directory to the host
					_, err = output.Export(gctx, path)
					if err != nil {
						return err
					}
					return nil
				})
			}
		}
	}

	// wait for all builds to complete
	if err := eg.Wait(); err != nil {
		return err
	}

	return nil
}

```

This Go program imports the Dagger SDK and defines two functions. The `build()` function represents the pipeline and creates a Dagger client, which provides an interface to the Dagger API. It also defines the build matrix, consisting of two OSs (`darwin` and `linux`) and two architectures (`amd64` and `arm64`), and builds the Go application for each combination. The Go build process is instructed via the `GOOS` and `GOARCH` build variables, which are reset for each case.

Try the Go program by executing the command below from the project directory:

```shell
dagger run go run multibuild/main.go
```

The `dagger run` command executes the specified command in a Dagger session and displays live progress. The Go program builds the application for each OS/architecture combination and writes the build results to the host. You will see the build process run four times, once for each combination. Note that the builds are happening concurrently, because the builds do not depend on eachother.

Use the `tree` command to see the build artifacts on the host, as shown below:

```shell
tree build
build
‚îú‚îÄ‚îÄ 1.22
‚îÇ   ‚îú‚îÄ‚îÄ darwin
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ amd64
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hello
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ arm64
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ hello
‚îÇ   ‚îî‚îÄ‚îÄ linux
‚îÇ       ‚îú‚îÄ‚îÄ amd64
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ hello
‚îÇ       ‚îî‚îÄ‚îÄ arm64
‚îÇ           ‚îî‚îÄ‚îÄ hello
‚îî‚îÄ‚îÄ 1.23
    ‚îú‚îÄ‚îÄ darwin
    ‚îÇ   ‚îú‚îÄ‚îÄ amd64
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hello
    ‚îÇ   ‚îî‚îÄ‚îÄ arm64
    ‚îÇ       ‚îî‚îÄ‚îÄ hello
    ‚îî‚îÄ‚îÄ linux
        ‚îú‚îÄ‚îÄ amd64
        ‚îÇ   ‚îî‚îÄ‚îÄ hello
        ‚îî‚îÄ‚îÄ arm64
            ‚îî‚îÄ‚îÄ hello
```

### Python

> **Note:**
> The Dagger Python SDK requires [Python 3.10 or later](https://docs.python.org/3/using/index.html).

Install the Dagger Python SDK in your project:

```shell
uv add dagger-io
```

If you prefer, you can alternatively add the Dagger Python SDK in your Python program. This is useful in case of dependency conflicts, or to keep your Dagger code self-contained.

```shell
uv add --script myscript.py dagger-io
```

This example demonstrates how to test a Python application against multiple Python versions using the Python SDK.

Clone an example project:

```shell
git clone --branch 0.101.0 https://github.com/tiangolo/fastapi
cd fastapi
```

Create a new file named `test.py` in the project directory and add the following code to it.

```python
import sys
from typing import List

import anyio
import dagger


async def test(versions: List[str]):
    async with dagger.Connection(dagger.Config(log_output=sys.stderr)) as client:
        # get reference to the local project
        src = client.host().directory(".")

        async def test_version(version: str):
            print(f"Starting tests for Python {version}")

            python = (
                client.container().from_(f"python:{version}-slim-buster")
                # mount cloned repository
                .with_directory("/src", src)
                # set current working directory
                .with_workdir("/src")
                # install test dependencies
                .with_exec(["pip", "install", "-r", "requirements.txt"])
                # run tests
                .with_exec(["pytest", "tests"])
            )

            # execute
            await python.sync()

            print(f"Tests for Python {version} succeeded!")

        # create task group to run tests in parallel
        async with anyio.create_task_group() as tg:
            for version in versions:
                tg.start_soon(test_version, version)

    print("All tasks have finished")


async def main():
    versions = ["3.8", "3.9", "3.10", "3.11"]
    await test(versions)


if __name__ == "__main__":
    anyio.run(main)

```

This Python program imports the Dagger SDK and defines an asynchronous function named `test()`. This `test()` function creates a Dagger client, which provides an interface to the Dagger API. It also defines the test matrix, consisting of Python versions `3.8` to `3.11` and iterates over this matrix, downloading a Python container image for each specified version and testing the source application in that version.

Add the dependency:

```shell
uv add --script test.py dagger-io
```

Run the Python program by executing the command below from the project directory:

```shell
dagger run uv run test.py
```

The `dagger run` command executes the specified command in a Dagger session and displays live progress. The tool tests the application against each version concurrently and displays the following final output:

```shell
Starting tests for Python 3.8
Starting tests for Python 3.9
Starting tests for Python 3.10
Starting tests for Python 3.11
Tests for Python 3.8 succeeded!
Tests for Python 3.9 succeeded!
Tests for Python 3.11 succeeded!
Tests for Python 3.10 succeeded!
All tasks have finished
```

### TypeScript
> **Note:**
> The Dagger TypeScript SDK requires [TypeScript 5.0 or later](https://www.typescriptlang.org/download/). This SDK currently only [supports Node.js (stable) and Bun (experimental)](../configuration/modules.md). To execute the TypeScript program, you must also have an TypeScript executor like `ts-node` or `tsx`.

Install the Dagger TypeScript SDK in your project using `npm` or `yarn`:

```shell
// using npm
npm install @dagger.io/dagger@latest --save-dev

// using yarn
yarn add @dagger.io/dagger --dev
```

This example demonstrates how to test a Node.js application against multiple Node.js versions using the TypeScript SDK.

Create an example React project (or use an existing one) in TypeScript:

```shell
npx create-react-app my-app --template typescript
cd my-app
```

In the project directory, create a new file named `build.mts` and add the following code to it:

```typescript
import { connect, Directory, Container } from "@dagger.io/dagger"

connect(
  async (client) => {
    // get reference to the local project
    const src: Directory = client.host().directory(".")

    // define the Node versions to test against
    const nodeVersions = ["16", "18", "20"]

    // initialize a matrix of Node containers
    const nodeMatrix: Container[] = nodeVersions.map((version) => {
      return client
        .container()
        .from(`node:${version}`)
        .withDirectory("/src", src)
        .withWorkdir("/src")
        .withExec(["npm", "install"])
    })

    // run tests and build the application for each Node version
    await Promise.all(
      nodeMatrix.map(async (node) => {
        const version = await node.withExec(["node", "-v"]).stdout()

        // run tests
        await node.withExec(["npm", "test", "--", "--watchAll=false"]).sync()

        // build the application
        const buildDir = await node.withExec(["npm", "run", "build"]).directory("build")

        // write the build output to the host
        await buildDir.export(`./build-node-${version.trim()}`)
      }),
    )
  },
  { LogOutput: process.stderr },
)

```

This TypeScript program imports the Dagger SDK and defines an asynchronous function. This function creates a Dagger client, which provides an interface to the Dagger API. It also defines the test/build matrix, consisting of Node.js versions `16`, `18` and `20`, and iterates over this matrix, downloading a Node.js container image for each specified version and testing and building the source application against that version.

Run the program with a Typescript executor like `ts-node`, as shown below:

```shell
dagger run node --loader ts-node/esm ./build.mts
```

The `dagger run` command executes the specified command in a Dagger session and displays live progress. The program tests and builds the application against each version in sequence. At the end of the process, a built application is available for each Node.js version in a `build-node-XX` folder in the project directory, as shown below:

```shell
tree -L 2 -d build-*
build-node-16
‚îî‚îÄ‚îÄ static
    ‚îú‚îÄ‚îÄ css
    ‚îú‚îÄ‚îÄ js
    ‚îî‚îÄ‚îÄ media
build-node-18
‚îî‚îÄ‚îÄ static
    ‚îú‚îÄ‚îÄ css
    ‚îú‚îÄ‚îÄ js
    ‚îî‚îÄ‚îÄ media
build-node-20
‚îî‚îÄ‚îÄ static
    ‚îú‚îÄ‚îÄ css
    ‚îú‚îÄ‚îÄ js
    ‚îî‚îÄ‚îÄ media
```

### PHP

> **Note:**
> The Dagger PHP SDK requires [PHP 8.2 or later](https://www.php.net/downloads.php).

Install the Dagger PHP SDK in your project using `composer`:

```shell
composer require dagger/dagger
```

This example demonstrates how to test a PHP application against multiple PHP versions using the PHP SDK.

Clone an example project:

```shell
git clone https://github.com/slimphp/Slim-Skeleton.git
cd Slim-Skeleton
```

Create a new file named `test.php` in the project directory and add the following code to it.

```php
<?php

require_once(__DIR__ . '/vendor/autoload.php');

use Dagger\Client;
use Dagger\Connection;
use Dagger\Container;
use Dagger\Directory;
use Dagger\Dagger;

use function Amp\async;
use function Amp\Future\await;

function test(array $versions): void
{
    $config = new \Dagger\Config();
    $client = Dagger::connect($config);

    // get reference to the local project
    $src = $client->host()->directory('.');

    $futures = [];
    foreach ($versions as $version) {
        $futures[] = async(function () use ($client, $src, $version) {
            echo "Starting tests for PHP $version...\n";

            $php = $client->container()->from("php:$version-alpine")
                // mount cloned repository
                ->withDirectory('/src', $src)
                // set current working directory
                ->withWorkdir('/src')
                // install dependencies
                ->withExec(['composer', 'install'])
                // run tests
                ->withExec(['vendor/bin/phpunit', 'tests']);

            // execute
            $php->sync();

            echo "Completed tests for PHP $version\n";
            echo "**********\n";
        });
    }

    await($futures);
}

$versions = ['8.2', '8.3'];
test($versions);

```

This PHP program imports the Dagger SDK and defines a function named `test()`. This `test()` function creates a Dagger client, which provides an interface to the Dagger API. It also defines the test matrix, consisting of PHP versions `8.2` to `8.4` and iterates over this matrix, downloading a PHP container image for each specified version and testing the source application in that version.

Run the PHP program by executing the command below from the project directory:

```shell
dagger run php test.php
```

The `dagger run` command executes the specified command in a Dagger session and displays live progress. The program tests the application against each version concurrently and displays the following final output:

```shell
Starting tests for PHP 8.2...
PHPUnit 9.6.22 by Sebastian Bergmann and contributors.

Warning:       Your XML configuration validates against a deprecated schema.
Suggestion:    Migrate your XML configuration using "--migrate-configuration"!

...................                                               19 / 19 (100%)

Time: 00:00.038, Memory: 12.00 MB

OK (19 tests, 37 assertions)
Completed tests for PHP 8.2
**********
Starting tests for PHP 8.3...
PHPUnit 9.6.22 by Sebastian Bergmann and contributors.

Warning:       Your XML configuration validates against a deprecated schema.
Suggestion:    Migrate your XML configuration using "--migrate-configuration"!

...................                                               19 / 19 (100%)

Time: 00:00.039, Memory: 12.00 MB

OK (19 tests, 37 assertions)
Completed tests for PHP 8.3
**********
```

## Differences

Here is a quick summary of differences between these two approaches.

|  | Dagger Functions | Custom applications |
|:---|:---|:---|
| Pre-initialized Dagger API client | Y | N |
| Direct host access | N | Y |
| Direct third-party module access | Y | N |
| Cross-language interoperability | Y | N |
</file>

<file path="docs/dagger.io/constructors.md">
---
slug: /api/constructor
---

# Constructors

Every Dagger module has a constructor. The default one is generated automatically and has no arguments.

It's possible to write a custom constructor. The mechanism to do this is SDK-specific.

This is a simple way to accept module-wide configuration, or just to set a few attributes without having to create setter functions for them.

## Simple constructor

The default constructor for a module can be overridden by registering a custom constructor. Its parameters are available as flags in the `dagger` command directly.

> **Important:**
> Dagger modules have only one constructors. Constructors of [custom types](./custom-types.md) are not registered; they are constructed by the function that [chains](./index.md#chaining) them.

Here is an example module with a custom constructor:

### Go

```go
package main

import (
	"fmt"
)

type MyModule struct {
	Greeting string
	Name     string
}

func New(
	// +optional
	// +default="Hello"
	greeting string,
	// +optional
	// +default="World"
	name string,
) *MyModule {
	return &MyModule{
		Greeting: greeting,
		Name:     name,
	}
}

// Returns the greeting message
func (m *MyModule) Message() string {
	return fmt.Sprintf("%s, %s!", m.Greeting, m.Name)
}

```

### Python

```python
from typing import Annotated, Optional

import dagger
from dagger import Doc, field, function, object_type


@object_type
class MyModule:
    greeting: Annotated[
        str,
        Doc("The greeting to use"),
    ] = field(default="Hello")

    name: Annotated[
        str,
        Doc("Who to greet"),
    ] = field(default="World")

    @function
    def message(self) -> str:
        """Returns the greeting message"""
        return f"{self.greeting}, {self.name}!"

```

> **Info:**
> In the Python SDK, the [`@dagger.object_type`](https://dagger-io.readthedocs.io/en/latest/module.html#dagger.object_type) decorator wraps [`@dataclasses.dataclass`](https://docs.python.org/3/library/dataclasses.html), which means that an `__init__()` method is automatically generated, with parameters that match the declared class attributes.

The code listing above is an example of an object that has typed attributes.

If a constructor argument needs an asynchronous call to set the default value, it's
possible to replace the default constructor function from `__init__()` to
a factory class method named `create`, as in the following code listing:

> **Warning:**
> This factory class method must be named `create`.

```python
import dagger
from dagger import dag, field, function, object_type


@object_type
class MyModule:
    version: str = field()

    @classmethod
    async def create(cls) -> "MyModule":
        """Create a new instance of MyModule."""
        version = await dag.container().from_("alpine").with_exec(["cat", "/etc/alpine-release"]).stdout()
        return cls(version=version.strip())

    @function
    def version(self) -> str:
        """Return the version."""
        return self.version

```

### TypeScript

```typescript
import { field, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  @field()
  greeting: string = "Hello"

  @field()
  name: string = "World"

  constructor(greeting?: string, name?: string) {
    if (greeting) {
      this.greeting = greeting
    }
    if (name) {
      this.name = name
    }
  }

  /**
   * Returns the greeting message
   */
  @func()
  message(): string {
    return `${this.greeting}, ${this.name}!`
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Attribute\DaggerArgument;

#[DaggerObject]
class MyModule
{
    public function __construct(
        #[DaggerArgument("The greeting to use")]
        public string $greeting = 'Hello',

        #[DaggerArgument("Who to greet")]
        public string $name = 'World',
    ) {
    }

    /**
     * Returns the greeting message
     */
    #[DaggerFunction]
    public function message(): string
    {
        return sprintf('%s, %s!', $this->greeting, $this->name);
    }
}

```

> **Info:**
> In the PHP SDK the constructor must be the [magic method `__construct`](https://www.php.net/manual/en/language.oop5.decon.php#object.construct).
> As with any method, only public methods with the `#[DaggerFunction]` attribute will be registered with Dagger.

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Default;
import io.dagger.module.annotation.Description;

@Module
@Object
public class MyModule {

  private String greeting;
  private String name;

  public MyModule() {
    this("Hello", "World");
  }

  public MyModule(
      @Description("The greeting to use") @Default("Hello") String greeting,
      @Description("Who to greet") @Default("World") String name) {
    this.greeting = greeting;
    this.name = name;
  }

  /**
   * Returns the greeting message
   */
  @Function
  public String message() {
    return String.format("%s, %s!", this.greeting, this.name);
  }
}

```

> **Info:**
> In the Java SDK, the constructor must be public. A public **empty** constructor is also required in order to create the object from the serialized data.

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c '. --name=Foo | message'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
. --name=Foo | message
```

### Dagger CLI
```shell
dagger call --name=Foo message
```

The result will be:

```shell
Hello, Foo!
```

> **Important:**
> If you plan to use constructor fields in other module functions, ensure that they are declared as public (in Go and TypeScript). This is because Dagger stores fields using serialization and private fields are omitted during the serialization process. As a result, if a field is not declared as public, calling methods that use it will produce unexpected results.

## Default values for complex types

Constructors can be passed both simple and complex types (such as `Container`, `Directory`, `Service` etc.) as arguments. Default values can be assigned in both cases.

Here is an example of a Dagger module with a default constructor argument of type `Container`:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct {
	Ctr *dagger.Container
}

func New(
	// +optional
	ctr *dagger.Container,
) *MyModule {
	if ctr == nil {
		ctr = dag.Container().From("alpine:3.14.0")
	}
	return &MyModule{
		Ctr: ctr,
	}
}

// Returns the container's alpine version
func (m *MyModule) Version(ctx context.Context) (string, error) {
	return m.Ctr.
		WithExec([]string{"cat", "/etc/alpine-release"}).
		Stdout(ctx)
}

```

### Python

```python
from typing import Annotated, Optional

import dagger
from dagger import Doc, dag, field, function, object_type


@object_type
class MyModule:
    ctr: dagger.Container = field(default=lambda: dag.container().from_("alpine:3.14.0"))

    @function
    async def version(self) -> str:
        """Returns the container's alpine version"""
        return await self.ctr.with_exec(["cat", "/etc/alpine-release"]).stdout()

```

For default values that are more complex, dynamic or just [mutable](https://docs.python.org/3/library/dataclasses.html#mutable-default-values),
use a [factory function](https://docs.python.org/3/library/dataclasses.html#default-factory-functions) without arguments in
[dataclasses.field(default_factory=...)](https://docs.python.org/3/library/dataclasses.html#dataclasses.field):

```python
import random
from dataclasses import dataclass, field
from typing import List

import dagger
from dagger import function, object_type


def random_words() -> List[str]:
    return random.sample(["foo", "bar", "baz"], 2)


@object_type
class MyModule:
    words: List[str] = field(default_factory=random_words)

    @function
    def words(self) -> List[str]:
        return self.words

```

### TypeScript

```typescript
import { Container, dag, field, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  @field()
  ctr: Container

  constructor(ctr?: Container) {
    this.ctr = ctr ?? dag.container().from("alpine:3.14.0")
  }

  /**
   * Returns the container's alpine version
   */
  @func()
  async version(): Promise<string> {
    return await this.ctr.withExec(["cat", "/etc/alpine-release"]).stdout()
  }
}

```

This default value can also be assigned directly in the field:

```typescript
import { Container, dag, field, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  @field()
  ctr: Container = dag.container().from("alpine:3.14.0")

  constructor(ctr?: Container) {
    if (ctr) {
      this.ctr = ctr
    }
  }

  /**
   * Returns the container's alpine version
   */
  @func()
  async version(): Promise<string> {
    return await this.ctr.withExec(["cat", "/etc/alpine-release"]).stdout()
  }
}

```

> **Important:**
> When assigning default values to complex types in TypeScript, it is necessary to use the `??` notation for this assignment. It is not possible to use the classic TypeScript notation for default arguments because the argument in this case is not a TypeScript primitive.

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Container;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    public Container $ctr;

    public function __construct(?Container $ctr = null)
    {
        $this->ctr = $ctr ?? dag()->container()->from('alpine:3.14.0');
    }

    /**
     * Returns the container's alpine version
     */
    #[DaggerFunction]
    public function version(): string
    {
        return $this->ctr
            ->withExec(['cat', '/etc/alpine-release'])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;
import java.util.Optional;

@Module
@Object
public class MyModule {

  private Container ctr;

  public MyModule() {
    this(Optional.empty());
  }

  public MyModule(Optional<Container> ctr) {
    try (Client client = Dagger.connect()) {
      this.ctr = ctr.orElse(client.container().from("alpine:3.14.0"));
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
  }

  /**
   * Returns the container's alpine version
   */
  @Function
  public String version() throws Exception {
    return this.ctr.withExec(List.of("cat", "/etc/alpine-release")).stdout().get();
  }
}

```

It is necessary to explicitly declare the type even when a default value is assigned, so that the Dagger SDK can extend the GraphQL schema correctly.

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c version
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
version
```

### Dagger CLI
```shell
dagger call version
```

The result will be:

```shell
VERSION_ID=3.14.0
</file>

<file path="docs/dagger.io/custom-functions.md">
---
slug: /api/custom-functions
---

# Custom Functions

In addition to providing a set of core functions and types, the Dagger API can also be extended with custom Dagger Functions and custom types. These custom Dagger Functions are just regular code, written in your usual language using a type-safe Dagger SDK, and packaged and shared in Dagger [modules](../features/modules.md).

When a Dagger module is loaded into a Dagger session, the Dagger API is [dynamically extended](./internals.md#api-extension-with-dagger-functions) with new functions served by that module. So, after loading a Dagger module, an API client can now call all of the original core functions plus the new functions provided by that module.

## Initialize a Dagger module

<!-- Content from ../partials/_dagger_module_init.mdx would be inserted here if available -->
<!-- Assuming removal as per confirmed rules -->

## Create a Dagger Function

Here's an example of a Dagger Function which calls a remote API method and returns the result:

### Go

Update the `main.go` file with the following code:

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns users matching the provided gender
func (m *MyModule) GetUser(ctx context.Context) (string, error) {
	// NOTE: this uses the http service included in the container, see
	// https://docs.dagger.io/manuals/developer/services/
	ctr := dag.Container().
		From("alpine").
		WithExec([]string{"apk", "add", "curl", "jq"}).
		WithExec([]string{
			"curl",
			"-s",
			"https://randomuser.me/api/",
		})

	// Use jq to extract the first name from the JSON response
	return ctr.
		WithExec([]string{"jq", "-r", ".results[0].name | {title, first, last}"}).
		Stdout(ctx)
}

```

This Dagger Function includes the context as input and error as return in its signature.

### Python

Update the `src/my_module/main.py` file with the following code:

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def get_user(self) -> str:
        """Returns users matching the provided gender"""
        # NOTE: this uses the http service included in the container, see
        # https://docs.dagger.io/manuals/developer/services/
        ctr = (
            dag.container()
            .from_("alpine")
            .with_exec(["apk", "add", "curl", "jq"])
            .with_exec(
                [
                    "curl",
                    "-s",
                    "https://randomuser.me/api/",
                ]
            )
        )

        # Use jq to extract the first name from the JSON response
        return await (
            ctr.with_exec(["jq", "-r", ".results[0].name | {title, first, last}"])
            .stdout()
        )

```

Dagger Functions are implemented as [@dagger.function][@function] decorated
methods, of a [@dagger.object_type][@object_type] decorated class.

It's possible for a module to implement multiple classes (*object types*), but
**the first one needs to have a name that matches the module's name**, in
*PascalCase*. This object is sometimes referred to as the *main object*.

For example, for a module initialized with `dagger init --name=my-module`,
the main object needs to be named `MyModule`.

[@function]: https://dagger-io.readthedocs.io/en/latest/module.html#dagger.function
[@object_type]: https://dagger-io.readthedocs.io/en/latest/module.html#dagger.object_type

### TypeScript

Update the `src/index.ts` file with the following code:

```typescript
import { dag, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns users matching the provided gender
   */
  @func()
  async getUser(): Promise<string> {
    // NOTE: this uses the http service included in the container, see
    // https://docs.dagger.io/manuals/developer/services/
    const ctr = dag
      .container()
      .from("alpine")
      .withExec(["apk", "add", "curl", "jq"])
      .withExec(["curl", "-s", "https://randomuser.me/api/"])

    // Use jq to extract the first name from the JSON response
    return await ctr
      .withExec(["jq", "-r", ".results[0].name | {title, first, last}"])
      .stdout()
  }
}

```

### PHP

Update the `src/MyModule.php` file with the following code:

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns users matching the provided gender
     */
    #[DaggerFunction]
    public function getUser(): string
    {
        // NOTE: this uses the http service included in the container, see
        // https://docs.dagger.io/manuals/developer/services/
        $ctr = dag()
            ->container()
            ->from('alpine')
            ->withExec(['apk', 'add', 'curl', 'jq'])
            ->withExec([
                'curl',
                '-s',
                'https://randomuser.me/api/',
            ]);

        // Use jq to extract the first name from the JSON response
        return $ctr
            ->withExec(['jq', '-r', '.results[0].name | {title, first, last}'])
            ->stdout();
    }
}

```

### Java

Update the `src/main/java/io/dagger/modules/mymodule/MyModule.java` file with the following code:

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns users matching the provided gender
   */
  @Function
  public String getUser() throws Exception {
    // NOTE: this uses the http service included in the container, see
    // https://docs.dagger.io/manuals/developer/services/
    try (Client client = Dagger.connect()) {
      Container ctr = client
          .container()
          .from("alpine")
          .withExec(List.of("apk", "add", "curl", "jq"))
          .withExec(
              List.of(
                  "curl",
                  "-s",
                  "https://randomuser.me/api/"
              )
          );

      // Use jq to extract the first name from the JSON response
      return ctr
          .withExec(List.of("jq", "-r", ".results[0].name | {title, first, last}"))
          .stdout()
          .get();
    }
  }
}

```

Dagger Functions must be public. The function must be decorated with the `@Function` annotation
and the class containing the functions must be decorated with the `@Object` annotation.

> **Caution:**
> You can try this Dagger Function by copying it into the default template generated by `dagger init`, but remember that you must update the module name in the code samples above to match the name used when your module was first initialized.

In simple terms, here is what this Dagger Function does:

- It initializes a new container from an `alpine` base image.
- It executes the `apk add ...`   command in the container to add the `curl` and `jq` utilities.
- It uses the `curl` utility to send an HTTP request to the URL `https://randomuser.me/api/` and parses the response using `jq`.
- It retrieves and returns the output stream of the last executed command as a string.

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'get-user'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
get-user
```

### Dagger CLI
```shell
dagger call get-user
```

Here's what you should see:

```shell
{
  "title": "Mrs",
  "first": "Beatrice",
  "last": "Lavigne"
}
```

> **Important:**
> Dagger Functions execute within containers spawned by the Dagger Engine. This "sandboxing" serves a few important purposes:
> 1. Reproducibility: Executing in a well-defined and well-controlled container ensures that a Dagger Function runs the same way every time it is invoked. It also guards against creating "hidden dependencies" on ambient properties of the execution environment that could change at any moment.
> 1. Caching: A reproducible containerized environment makes it possible to cache the result of Dagger Function execution, which in turn allows Dagger to automatically speed up operations.
> 1. Security: Even when running third-party Dagger Functions sourced from a Git repository, those Dagger Functions will not have default access to your host environment (host files, directories, environment variables, etc.). Access to these host resources can only be granted by explicitly passing them as argument values to the Dagger Function.

When implementing Dagger Functions, you are free to write arbitrary code that will execute inside the Dagger module's container. You have access to the Dagger API to make calls to the core Dagger API or other Dagger modules you depend on, but you are also free to just use the language's standard library and/or imported third-party libraries.

The process your code executes in will currently be with the `root` user, but without a full set of Linux capabilities and other standard container sandboxing provided by `runc`.

The current working directory of your code will be an initially empty directory. You can write and read files and directories in this directory if needed. This includes using the `Container.export()`, `Directory.export()` or `File.export()` APIs to write those artifacts to this local directory if needed.
</file>

<file path="docs/dagger.io/custom-types.md">
---
slug: /api/custom-types
---

# Custom Types

A Dagger module can have multiple object types defined. It's important to understand that they are only accessible through [chaining](./index.md#chaining), starting from a function in the main object.

### Go

Here is an example of a `github` Dagger module, with a function named `DaggerOrganization`
that returns a custom `Organization` type, itself containing a collection of
`Account` types:

```go
package main

import (
	"fmt"
)

// Github module
type Github struct{}

// Github organization
type Organization struct {
	Name    string
	Members []*Account
}

// Github account
type Account struct {
	Login string
	URL   string
}

// Returns the Dagger organization
func (m *Github) DaggerOrganization() *Organization {
	return &Organization{
		Name: "Dagger",
		Members: []*Account{
			{Login: "jane", URL: "https://github.com/jane"},
			{Login: "john", URL: "https://github.com/john"},
		},
	}
}

// Returns the organization's members
func (o *Organization) Members() []*Account {
	return o.Members
}

// Returns the account's login
func (a *Account) Login() string {
	return a.Login
}

// Returns the account's URL
func (a *Account) URL() string {
	return a.URL
}

```

### Python

Here is an example of a `github` Dagger module, with a function named `dagger_organization`
that returns a custom `Organization` type, itself containing a collection of
`Account` types:

```python
from typing import List

import dagger
from dagger import field, function, object_type


@object_type
class Account:
    login: str = field()
    url: str = field()

    @function
    def login(self) -> str:
        """Returns the account's login"""
        return self.login

    @function
    def url(self) -> str:
        """Returns the account's URL"""
        return self.url


@object_type
class Organization:
    name: str = field()
    members: List[Account] = field()

    @function
    def members(self) -> List[Account]:
        """Returns the organization's members"""
        return self.members


@object_type
class Github:
    @function
    def dagger_organization(self) -> Organization:
        """Returns the Dagger organization"""
        return Organization(
            name="Dagger",
            members=[
                Account(login="jane", url="https://github.com/jane"),
                Account(login="john", url="https://github.com/john"),
            ],
        )

```

The [`dagger.field`](https://dagger-io.readthedocs.io/en/latest/module.html#dagger.field) descriptors expose getter functions without arguments, for their [attributes](./state.md).

### TypeScript

Here is an example of a `github` Dagger module, with a function named `daggerOrganization`
that returns a custom `Organization` type, itself containing a collection of
`Account` types:

```typescript
import { field, func, object } from "@dagger.io/dagger"

@object()
class Account {
  @field()
  login: string

  @field()
  url: string

  constructor(login: string, url: string) {
    this.login = login
    this.url = url
  }

  /**
   * Returns the account's login
   */
  @func()
  getLogin(): string {
    return this.login
  }

  /**
   * Returns the account's URL
   */
  @func()
  getUrl(): string {
    return this.url
  }
}

@object()
class Organization {
  @field()
  name: string

  @field()
  members: Account[]

  constructor(name: string, members: Account[]) {
    this.name = name
    this.members = members
  }

  /**
   * Returns the organization's members
   */
  @func()
  getMembers(): Account[] {
    return this.members
  }
}

@object()
class Github {
  /**
   * Returns the Dagger organization
   */
  @func()
  daggerOrganization(): Organization {
    return new Organization("Dagger", [
      new Account("jane", "https://github.com/jane"),
      new Account("john", "https://github.com/john"),
    ])
  }
}

```

TypeScript has multiple ways to support complex data types. Use a `class` when you need methods and privacy, use `type` for plain data objects with only public fields.

> **Note:**
> When the Dagger Engine extends the Dagger API schema with these types, it prefixes
> their names with the name of the main object:
> - Github
> - GithubAccount
> - GithubOrganization
>
> This is to prevent possible naming conflicts when loading multiple modules,
> which is reflected in code generation (for example, when using this module in
> another one as a dependency).

Here's an example of calling a Dagger Function from this module to get all member URLs:

### System shell
```shell
dagger -c 'dagger-organization | members | url'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
dagger-organization | members | url
```

### Dagger CLI
```shell
dagger call dagger-organization members url
```


```shell
dagger call dagger-organization members url
```

The result will be:

```
https://github.com/jane
https://github.com/john
</file>

<file path="docs/dagger.io/daggerverse.md">
---
slug: /api/daggerverse
---

# The Daggerverse

Dagger Functions are packaged and shared using Dagger [modules](../features/modules.md).

## Ranking

The Daggerverse is a free service run by Dagger, which indexes all publicly available Dagger Functions, and lets you easily search and consume them. To help users find the best quality modules, a basic rank is applied to modules for sorting the module index.

To make sure your module gets the best possible rank, consider the following for your module:

1. A module should have a [description](./documentation.md#inline-documentation)

1. Module functions should have [descriptions](./documentation.md#inline-documentation)

1. Modules versions should be tagged with [semver](./daggerverse.md#semantic-versioning)

1. Modules should have hand-crafted [examples](./daggerverse.md#examples)

## Examples

Daggerverse will automatically show basic examples for each function in a module. If you would like to provide hand-crafted examples, the following section will describe how to set these up.


### Go

A Go example must be a Dagger module located at `/examples/go` within the module you're creating examples for.

If you have a module called `Foo` and a function called `Bar`, you can create the following functions in your example module:

1. A function `Foo_Baz` will create a top level example for the `Foo` module called Baz.

1. A function `FooBar` will create an example for function `Bar`.

1. Functions `FooBar_Baz` will create a Baz example for the function `Bar`.

### Python

A Python example must be a Dagger module located at `/examples/python` within the module you're creating examples for.

If you have a module called `foo` and a function called `bar`, you can create the following functions in your example module:

1. A function `foo__baz` will create a top level example for the `foo` module called baz.

1. A function `foo_bar` will create an example for function `bar`.

1. Functions `foo_bar__baz` will create a baz example for the function `bar`.

> **Note:**
> Python function names in example modules use double underscores (`__`) as separators since by convention, Python uses single underscores to represent spaces in function names (snake case).

### Typescript

A Typescript example must be a Dagger module located at `/examples/typescript` within the module you're creating examples for.

If you have a module called `foo` and a function called `bar`, you can create the following functions in your example module:

1. A function `foo_baz` will create a top level example for the `foo` module called baz.

1. A function `fooBar` will create an example for function `bar`.

1. Functions `fooBar_baz` and will create a baz example for the function `bar`.

### Shell

A Shell example must be a shell script located at `/examples/shell` within the module you're creating examples for.

If you have a module called `foo` and a function called `bar`, you can create the following script in your example directory:

1. A file `foo_baz.sh` will create a top level example for the `foo` module called baz.

1. A file `foo_bar.sh` will create an example for function `bar`.

1. Files `foo_bar_baz.sh` and will create a baz example for the function `bar`.

For an example of a module with hand-crafted function examples, see the [proxy module](https://daggerverse.dev/mod/github.com/kpenfound/dagger-modules/proxy)

## Publishing

Two processes are available to publish your Dagger module to the Daggerverse, manual and automatic on use.

### Manual publishing

To manually publish a module to the Daggerverse, follow the steps below:

1. To publish a Dagger module to the Daggerverse, the module must be pushed to a public git repository. Dagger is agnostic to repository layout, and any number of Dagger modules can peacefully coexist in a repository. It's up to you how to organize your module's source code in Git. Some like to publish each module as a dedicated repository; others like to organize all their modules together, with the git repository acting as a "catalog". These repositories are often named "daggerverse" by convention.

1. Navigate to the [Daggerverse](https://daggerverse.dev) and click the `Publish` button. On the resulting page, paste the URL to the Git repository containing your module in the format `GITSERVER/USERNAME/REPOSITORY[/SUBPATH][@VERSION]`. For example, `github.com/shykes/hello@v0.3.0` or `github.com/shykes/daggerverse/termcast@main`

1. Click "Publish" to have your Dagger module published to the Daggerverse. This process may take a few minutes. Once complete, your Dagger module will appear in the Daggerverse module listing.

> **Important:**
> - Most Git servers should "just work", but please let us know if you encounter any issues.
> - The Daggerverse only fetches publicly available information from Git servers. Modules are not hosted on the Daggerverse. If you need a module removed from the Daggerverse for some reason, let the Dagger team know in [Discord](https://discord.gg/dagger-io).

### Publishing on use

Every time that a user executes `dagger ...`, if any of the Dagger Functions which are invoked in the execution come from a remote Dagger module (here, a remote module is defined as a module whose location is specified by a URL like `GITSERVER/USERNAME/daggerverse/...`), that Dagger module will automatically be published to the Daggerverse.

> **Note:**
> Under this process, it is possible for some Dagger modules to appear in the Daggerverse even when they're not fully ready. An example of this is when the module developer is developing the module in a local development environment and pushing work-in-progress to the Git repository. In this case, as soon as the module developer tags the module with a [valid semantic version number](#semantic-versioning), the module will be considered released and the latest version will be published to the Daggerverse.

### Semantic versioning

Dagger modules should be versioned according to [semantic versioning principles](https://semver.org/). This means that the published module reference should be tagged with a pattern matching `vMAJOR.MINOR.PATCH`, such as `v1.2.3`.

> **Important:**
> At this time, only version numbers matching the `vMAJOR.MINOR.PATCH` versioning pattern are considered valid.

In monorepos of modules, modules can be independently versioned by prefixing the tag with the subpath. For example a module named `foo` can be tagged with `foo/v1.2.3` and referenced as `GITSERVER/USERNAME/REPOSITORY/foo@v1.2.3`.
</file>

<file path="docs/dagger.io/default-paths.md">
---
slug: /api/default-paths
---

# Default Paths

It is possible to assign a default path for a `Directory` or `File` argument in a Dagger Function. Dagger will automatically use this default path when no value is specified for the argument. The `Directory` or `File` loaded in this manner is not merely a string, but the actual filesystem state of the directory or file.

> **Important:**
> Default paths are only available for `Directory` and `File` arguments. They are commonly used to load constant filesystem locations, such as an application's source code directory. Additionally, when a value is explicitly passed for the argument, it always overrides the default path.

Here's an example:

### Go

The default path is set by adding a `defaultPath` pragma on the corresponding Dagger Function `source` argument.

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns the list of files in the directory
func (m *MyModule) ListFiles(
	ctx context.Context,
	// Source directory
	// +defaultPath=.
	source *dagger.Directory,
) ([]string, error) {
	return source.Entries(ctx)
}

```

### Python

The default path is set by adding a `DefaultPath` annotation on the corresponding Dagger Function `source` argument.

```python
from typing import Annotated, List

import dagger
from dagger import DefaultPath, Doc, function, object_type


@object_type
class MyModule:
    @function
    async def list_files(
        self,
        source: Annotated[
            dagger.Directory,
            Doc("Source directory"),
            DefaultPath("."),
        ],
    ) -> List[str]:
        """Returns the list of files in the directory"""
        return await source.entries()

```

### TypeScript

The default path is set by adding an `@argument` decorator with a `defaultPath` parameter on the corresponding Dagger Function `source` argument.

```typescript
import { Directory, func, object, argument } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns the list of files in the directory
   *
   * @param source Source directory
   */
  @func()
  async listFiles(
    @argument({ description: "Source directory", defaultPath: "." })
    source: Directory,
  ): Promise<string[]> {
    return await source.entries()
  }
}

```

### PHP

The default path is set by adding a `#[DefaultPath]` Attribute on the corresponding Dagger Function `source` argument.

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Attribute\DefaultPath;
use Dagger\Client\Directory;

#[DaggerObject]
class MyModule
{
    /**
     * Returns the list of files in the directory
     *
     * @param Directory $source Source directory
     * @return string[]
     */
    #[DaggerFunction]
    public function listFiles(
        #[DefaultPath('.')]
        Directory $source
    ): array {
        return $source->entries();
    }
}

```

When determining how to resolve a default path, Dagger first identifies a "context directory", and then resolves the path starting from the context directory.

- For Git repositories (defined by the presence of a `.git` sub-directory), the context directory is the repository root (for absolute paths), or the directory containing a `dagger.json` file (for relative paths).
- For all other cases, the context directory is the directory containing a `dagger.json` file.

```mermaid
flowchart TD
    B[Git repository?] -->|Yes| C[Absolute path?]
    C -->|Yes| D[Context directory = Repository root]
    C -->|No| E[Context directory = Directory containing dagger.json]
    B -->|No| F[Context directory = Directory containing dagger.json]
```

> **Important:**
> For security reasons, it is not possible to retrieve files or directories outside the context directory.

The following sections contain examples of how a `Directory` argument is resolved for different default path values. The same rules are followed for `File` arguments.

### For Git repositories

| Default path | Context directory | Resolved path |
| --- | --- | --- |
| `/` | Repository root (`/`) | `/` |
| `/src` | Repository root (`/`) | `/src` |
| `.` | Directory with `dagger.json` (`/my-module`) | `/my-module` |
| `..` | Directory with `dagger.json` (`/my-module`) | `/` |

- If the default path is an absolute path `/` (or `/src`), the context directory is the repository root (`/`). The resolved path will then be `/` (or `/src`).
- If the default path is the relative path `.`, the context directory is the directory containing `dagger.json` (say, `/my-module`). The resolved path will then be `/my-module`.
- If the default path is the relative path `..`, the context directory is still the directory containing `dagger.json`. The resolved path will then be the parent of the context directory (`/`).

### For all other cases

| Default path | Context directory | Resolved path |
| --- | --- | --- |
| `/` | Directory with `dagger.json` (`/my-module`) | `/my-module` |
| `/src` | Directory with `dagger.json` (`/my-module`) | `/my-module/src` |
| `.` | Directory with `dagger.json` (`/my-module`) | `/my-module` |
| `..` | Directory with `dagger.json` (`/my-module`) | Outside context directory; error |

- If the default path is an absolute path `/` (or `/src`),  the context directory is the directory containing `dagger.json` (say, `/my-module`). The resolved path will then be `/my-module` (or `/my-module/src`).
- If the default path is the relative path `.`, the context directory is still the directory containing `dagger.json`. The resolved path will then be `/my-module`.
- If the default path is the relative path `..`, the context directory is still the directory containing `dagger.json`. The resolved path will then be the parent of the context directory. This will trigger an error, since Dagger does not permit access to paths outside the context directory.

> **Tip:**
> It's also possible to provide an `ignore` parameter to a contextual argument of type `Directory` to [automatically ignore or include files in the directory](./fs-filters.md).
</file>

<file path="docs/dagger.io/documentation.md">
---
slug: /api/documentation
---

# Inline Documentation

Dagger modules and Dagger Functions should be documented so that descriptions are shown in the API and the CLI - for example, when calling `dagger functions`, `dagger call ... --help`, or `.help`.

### Go

The following code snippet shows how to add documentation for:

- The whole module
- Function methods
- Function arguments

```go
package main

import (
	"context"
	"fmt"
)

// A simple example module to say hello.
//
// Further documentation for the module here.
type MyModule struct{}

// Return a greeting.
func (m *MyModule) Hello(
	ctx context.Context,
	// The greeting to display.
	greeting string,
	// Who to greet.
	name string,
) string {
	return fmt.Sprintf("%s, %s!", greeting, name)
}

// Return a loud greeting.
func (m *MyModule) LoudHello(
	ctx context.Context,
	// The greeting to display.
	greeting string,
	// Who to greet.
	name string,
) string {
	return fmt.Sprintf("%s, %s!", greeting, name)
}

```

### Python

The following code snippet shows how to use Python's [documentation string conventions](https://peps.python.org/pep-0008/#documentation-strings) for adding descriptions to:
- The whole [module](https://docs.python.org/3/tutorial/modules.html) or [package](https://docs.python.org/3/tutorial/modules.html#packages)
- Object type classes (group of functions)
- Function methods
- Function arguments

For function arguments, [annotate](https://peps.python.org/pep-0727/#specification) with the [`dagger.Doc`](https://dagger-io.readthedocs.io/en/latest/module.html#dagger.Doc) metadata.

> **Note:**
> [`dagger.Doc`](https://dagger-io.readthedocs.io/en/latest/module.html#dagger.Doc) is just an alias for [`typing_extensions.Doc`](https://typing-extensions.readthedocs.io/en/latest/#Doc).

<!-- loud_hello has a multi-line docstring on purpose -->
```python
"""A simple example module to say hello.

Further documentation for the module here.
"""

from typing import Annotated

import dagger
from dagger import Doc, function, object_type


@object_type
class MyModule:
    """MyModule functions"""

    @function
    def hello(
        self,
        greeting: Annotated[str, Doc("The greeting to display")],
        name: Annotated[str, Doc("Who to greet")],
    ) -> str:
        """Return a greeting."""
        return f"{greeting}, {name}!"

    @function
    def loud_hello(
        self,
        greeting: Annotated[str, Doc("The greeting to display")],
        name: Annotated[str, Doc("Who to greet")],
    ) -> str:
        """Return a loud greeting.

        This is a multi-line docstring.
        """
        return f"{greeting.upper()}, {name.upper()}!"

```

### TypeScript

The following code snippet shows how to add documentation for:
- The whole module
- Function methods
- Function arguments

```typescript
import { func, object, field } from "@dagger.io/dagger"

/**
 * A simple example module to say hello.
 *
 * Further documentation for the module here.
 */
@object()
class MyModule {
  /**
   * Return a greeting.
   *
   * @param greeting The greeting to display.
   * @param name Who to greet.
   */
  @func()
  hello(greeting: string, name: string): string {
    return `${greeting}, ${name}!`
  }

  /**
   * Return a loud greeting.
   *
   * @param greeting The greeting to display.
   * @param name Who to greet.
   */
  @func()
  loudHello(greeting: string, name: string): string {
    return `${greeting.toUpperCase()}, ${name.toUpperCase()}!`
  }
}

```

### PHP

The following code snippet shows how to add documentation for:

- The whole module
- Function methods
- Function arguments

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Attribute\DaggerArgument;

/**
 * A simple example module to say hello.
 *
 * Further documentation for the module here.
 */
#[DaggerObject]
class MyModule
{
    /**
     * Return a greeting.
     */
    #[DaggerFunction]
    public function hello(
        #[DaggerArgument("The greeting to display.")]
        string $greeting,

        #[DaggerArgument("Who to greet.")]
        string $name
    ): string {
        return sprintf('%s, %s!', $greeting, $name);
    }

    /**
     * Return a loud greeting.
     */
    #[DaggerFunction]
    public function loudHello(
        #[DaggerArgument("The greeting to display.")]
        string $greeting,

        #[DaggerArgument("Who to greet.")]
        string $name
    ): string {
        return sprintf('%s, %s!', strtoupper($greeting), strtoupper($name));
    }
}

```

### Java

The following code snippet shows how to add documentation for:

- Function methods
- Function arguments

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

@Module
@Object
public class MyModule {

  /**
   * Return a greeting.
   */
  @Function
  public String hello(
      @Description("The greeting to display.") String greeting,
      @Description("Who to greet.") String name) {
    return String.format("%s, %s!", greeting, name);
  }

  /**
   * Return a loud greeting.
   */
  @Function
  public String loudHello(
      @Description("The greeting to display.") String greeting,
      @Description("Who to greet.") String name) {
    return String.format("%s, %s!", greeting.toUpperCase(), name.toUpperCase());
  }
}

```

The documentation of the module is added in the `package-info.java` file.

```java
/**
 * A simple example module to say hello.
 *
 * Further documentation for the module here.
 */
@Module
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;

```

Here is an example of the result from `dagger functions`:

```
Name         Description
hello        Return a greeting.
loud-hello   Return a loud greeting.
```

Here is an example of the result from `dagger call hello --help`:

```
Return a greeting.

USAGE
  dagger call hello [arguments]

ARGUMENTS
      --greeting string   The greeting to display [required]
      --name string       Who to greet [required]
```


The following code snippet shows how to add documentation for an object and its fields in your Dagger module:

### Go

```go
package main

// User represents a user with a name and age.
type User struct {
	// The name of the user.
	Name string
	// The age of the user.
	Age int
}

```

### Python

```python
from typing import Annotated

import dagger
from dagger import Doc, field, object_type


@object_type
class User:
    """User represents a user with a name and age."""

    name: Annotated[str, Doc("The name of the user.")] = field()
    age: Annotated[int, Doc("The age of the user.")] = field()

```

### TypeScript

```typescript
import { field, object } from "@dagger.io/dagger"

/**
 * User represents a user with a name and age.
 */
@object()
export class User {
  /**
   * The name of the user.
   */
  @field()
  name: string

  /**
   * The age of the user.
   */
  @field()
  age: number

  constructor(name: string, age: number) {
    this.name = name
    this.age = age
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerObject;
use Dagger\Attribute\DaggerField;
use Dagger\Attribute\DaggerArgument;

/**
 * User represents a user with a name and age.
 */
#[DaggerObject]
class User
{
    public function __construct(
        /**
         * The name of the user.
         */
        #[DaggerField]
        public string $name,

        /**
         * The age of the user.
         */
        #[DaggerField]
        public int $age,
    ) {
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Description;

/**
 * User represents a user with a name and age.
 */
@Object
public class User {

  /**
   * The name of the user.
   */
  @Description("The name of the user.")
  public String name;

  /**
   * The age of the user.
   */
  @Description("The age of the user.")
  public int age;

  public User(String name, int age) {
    this.name = name;
    this.age = age;
  }
}

```

Here is an example of the result from `dagger call --help`:

```
ARGUMENTS
      --age int       The age of the user. [required]
      --name string   The name of the user. [required]
</file>

<file path="docs/dagger.io/engine.md">
---
slug: /api/engine
---

# Engine

The `Engine` type represents the Dagger Engine configuration and state. It provides fields to interact with a running Dagger Engine.

## Caching

Dagger caches two types of data:

1. Layers: This refers to build instructions and the results of some API calls.
2. Volumes: This refers to the contents of a Dagger filesystem volume and is persisted across Dagger Engine sessions.

The `Engine` type can be used to inspect or manually prune the cache.

To show all the cache entry metadata, use the following command:

```shell
dagger query <<EOF
{
  engine {
    localCache {
      entrySet {
        entries {
          description
          diskSpaceBytes
        }
      }
    }
  }
}
EOF
```

To see high level summaries of cache usage, use the following command:

```shell
dagger query <<EOF
{
  engine {
    localCache {
      entrySet {
        entryCount
        diskSpaceBytes
      }
    }
  }
}
EOF
```

To manually free up disk space used by the cache, use the following command:

```shell
dagger query <<EOF
{
  engine {
    localCache {
      prune
    }
  }
}
EOF
</file>

<file path="docs/dagger.io/enumerations.md">
---
slug: /api/enumerations
displayed_sidebar: "current"
toc_max_heading_level: 2
title: "Enumerations"
---

# Enumerations

> **Important:**
> The information on this page is only applicable to Go, Python and TypeScript SDKs. Enumerations are not currently supported in the PHP SDK.

Dagger supports custom enumeration (enum) types, which can be used to restrict possible values for a string argument. Enum values are strictly validated, preventing common mistakes like accidentally passing null, true, or false.

> **Note:**
> Following the [GraphQL specification](https://spec.graphql.org/October2021/#Name), enums are represented as strings in the Dagger API GraphQL schema and follow these rules:
> - Enum names cannot start with digits, and can only be composed of alphabets, digits or `_`.
> - Enum values are case-sensitive, and by convention should be upper-cased.

Here is an example of a Dagger Function that takes two arguments: an image reference and a severity filter. The latter is defined as an enum named `Severity`:

### Go
```go
package main

import (
	"context"
	"fmt"

	"dagger.io/dagger"
)

type MyModule struct{}

type Severity string

const (
	UnknownSeverity  Severity = "UNKNOWN"
	LowSeverity      Severity = "LOW"
	MediumSeverity   Severity = "MEDIUM"
	HighSeverity     Severity = "HIGH"
	CriticalSeverity Severity = "CRITICAL"
)

// Scan an image and return a report
func (m *MyModule) Scan(
	ctx context.Context,
	// Image reference
	ref string,
	// Severity filter
	severity Severity,
) (string, error) {
	return dag.Container().
		From("aquasec/trivy:latest").
		WithExec([]string{
			"image",
			"--quiet",
			"--format", "json",
			"--severity", string(severity),
			ref,
		}).
		Stdout(ctx)
}

```

### Python
```python
from enum import Enum
from typing import Annotated

import dagger
from dagger import Doc, dag, function, object_type


@dagger.enum
class Severity(Enum):
    """Severity choices"""

    UNKNOWN = "UNKNOWN"
    LOW = "LOW"
    MEDIUM = "MEDIUM"
    HIGH = "HIGH"
    CRITICAL = "CRITICAL"


@object_type
class MyModule:
    @function
    async def scan(
        self,
        ref: Annotated[str, Doc("Image reference")],
        severity: Annotated[Severity, Doc("Severity filter")],
    ) -> str:
        """Scan an image and return a report"""
        return await (
            dag.container()
            .from_("aquasec/trivy:latest")
            .with_exec(
                [
                    "image",
                    "--quiet",
                    "--format",
                    "json",
                    "--severity",
                    severity.value,
                    ref,
                ]
            )
            .stdout()
        )

```

> **Note:**
> `dagger.Enum` is a convenience base class for defining documentation, but you can also use `enum.Enum` directly.

### TypeScript
```typescript
import { dag, func, object, enumType } from "@dagger.io/dagger"

@enumType()
export enum Severity {
  UNKNOWN = "UNKNOWN",
  LOW = "LOW",
  MEDIUM = "MEDIUM",
  HIGH = "HIGH",
  CRITICAL = "CRITICAL",
}

@object()
class MyModule {
  /**
   * Scan an image and return a report
   *
   * @param ref Image reference
   * @param severity Severity filter
   */
  @func()
  async scan(ref: string, severity: Severity): Promise<string> {
    return await dag
      .container()
      .from("aquasec/trivy:latest")
      .withExec([
        "image",
        "--quiet",
        "--format",
        "json",
        "--severity",
        severity,
        ref,
      ])
      .stdout()
  }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Enum;

@Enum
public enum Severity {
  UNKNOWN,
  LOW,
  MEDIUM,
  HIGH,
  CRITICAL
}

```

> **Note:**
> Please note the `@Enum` annotation that is required for Dagger to recognize the enum.

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Scan an image and return a report
   */
  @Function
  public String scan(
      @Description("Image reference") String ref,
      @Description("Severity filter") Severity severity) throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("aquasec/trivy:latest")
          .withExec(
              List.of(
                  "image",
                  "--quiet",
                  "--format",
                  "json",
                  "--severity",
                  severity.name(),
                  ref))
          .stdout()
          .get();
    }
  }
}

```

Enumeration choices will be displayed when calling `--help` or `.help` on a Dagger Function:

### System shell
```shell
dagger -c '.help scan'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
.help scan
```

### Dagger CLI
```shell
dagger call scan --help
```

The result will be:

### System shell
```shell
USAGE
  scan <ref> <severity>

REQUIRED ARGUMENTS
  ref string
  severity MyModuleSeverity   (possible values: UNKNOWN, LOW, MEDIUM, HIGH, CRITICAL)

RETURNS
  string - Primitive type.
```

### Dagger Shell
```shell
USAGE
  scan <ref> <severity>

REQUIRED ARGUMENTS
  ref string
  severity MyModuleSeverity   (possible values: UNKNOWN, LOW, MEDIUM, HIGH, CRITICAL)

RETURNS
  string - Primitive type.
```

### Dagger CLI
```shell
USAGE
  dagger call scan [arguments]

ARGUMENTS
      --ref string                                  [required]
      --severity UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL   [required]
```

Here's an example of calling the Dagger Function with an invalid enum argument:

### System shell
```shell
dagger -c 'scan hello-world:latest FOO'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
scan hello-world:latest FOO
```

### Dagger CLI
```shell
dagger call scan --ref=hello-world:latest --severity=FOO
```

This will result in an error that displays possible values, as follows:

### System shell
```shell
! function "scan": invalid argument "FOO" for "--severity" flag: value should be one of UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL
! Usage: scan <ref> <severity>
```

### Dagger Shell
```shell
! function "scan": invalid argument "FOO" for "--severity" flag: value should be one of UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL
! Usage: scan <ref> <severity>
```

### Dagger CLI
```shell
Error: invalid argument "FOO" for "--severity" flag: value should be one of UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL
Run 'dagger call scan --help' for usage.
</file>

<file path="docs/dagger.io/error-handling.md">
---
slug: /api/error-handling
---

# Error Handling

Dagger modules handle errors in the same way as the language they are written in. This allows you to support any kind of error handling that your application requires. You can also use error handling to verify user input.

Here is an example Dagger Function that performs division and throws an error if the denominator is zero:

### Go

```go
package main

import (
	"errors"
)

type MyModule struct{}

// Divide two numbers
func (m *MyModule) Divide(a, b int) (int, error) {
	if b == 0 {
		return 0, errors.New("cannot divide by zero")
	}
	return a / b, nil
}

```

Error handling in Go modules follows typical Go error patterns with explicit `error` return values and `if err != nil` checks. You can also use error handling to verify user input.

### Python

```python
import dagger
from dagger import function, object_type


@object_type
class MyModule:
    @function
    def divide(self, a: int, b: int) -> int:
        """Divide two numbers"""
        if b == 0:
            raise ValueError("cannot divide by zero")
        return a // b

```

### TypeScript

```typescript
import { func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Divide two numbers
   */
  @func()
  divide(a: number, b: number): number {
    if (b === 0) {
      throw new Error("cannot divide by zero")
    }
    return Math.floor(a / b)
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

#[DaggerObject]
class MyModule
{
    /**
     * Divide two numbers
     */
    #[DaggerFunction]
    public function divide(int $a, int $b): int
    {
        if ($b === 0) {
            throw new \InvalidArgumentException('cannot divide by zero');
        }
        return intdiv($a, $b);
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

@Module
@Object
public class MyModule {

  /**
   * Divide two numbers
   */
  @Function
  public int divide(int a, int b) {
    if (b == 0) {
      throw new IllegalArgumentException("cannot divide by zero");
    }
    return a / b;
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'divide 4 2'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
divide 4 2
```

### Dagger CLI
```shell
dagger call divide --a=4 --b=2
```

The result will be:

```shell
2
```

Here is another example call for this Dagger Function, this time dividing by zero:

### System shell
```shell
dagger -c 'divide 4 0'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
divide 4 0
```

### Dagger CLI
```shell
dagger call divide --a=4 --b=0
```

The result will be:

```
cannot divide by zero
</file>

<file path="docs/dagger.io/faq.md">
---
slug: /faq
pagination_next: null
pagination_prev: null
---

# FAQ


## General

### What is the Dagger Platform?

We're building the devops operating system, an integrated platform to orchestrate the delivery of applications to the cloud from start to finish. The Dagger Platform includes the Dagger Engine, Dagger Cloud, and the Dagger SDKs. Soon we will deliver the capability to publish and leverage prebuilt modules to further accelerate the adoption of Dagger across an organization's pipelines.

### How do I install, update, or uninstall Dagger?

Refer to the [installation documentation](./install.md).

### Does Dagger send telemetry?

By default, the Dagger CLI sends anonymized telemetry to dagger.io. This allows us to improve Dagger by understanding how it is used. Telemetry is optional and can be disabled at any time. If you are willing and able to leave telemetry enabled: thank you! This will help us better understand how Dagger is used, and will allow us to improve your experience.

### What telemetry does Dagger send?

The following information is included in telemetry:

- Dagger version
- Platform information
- Command run
- Anonymous device ID

We use telemetry for aggregate analysis, and do not tie telemetry events to a specific identity. Our telemetry implementation is open-source and can be reviewed in our [GitHub repository](https://github.com/dagger/dagger).

### Can Dagger telemetry be disabled?

Dagger implements the [Console Do Not Track (DNT) standard](https://consoledonottrack.com/). As a result, you can disable the telemetry by setting the environment variable `DO_NOT_TRACK=1` before running the Dagger CLI.

### Can I configure the Dagger Engine?

Yes. [Read more about Dagger Engine configuration](https://github.com/dagger/dagger/blob/main/core/docs/d7yxc-operator_manual.md).

### Can I use Dagger Engine to build Windows Containers?

Unfortunately, not right now. Dagger runs on top of BuildKit and support for Windows Containers is still experimental in BuildKit. In addition, Dagger has a lot of custom code written on top of BuildKit such as networking, init systems, and Linux namespace entries, that do not have exact parallels on Windows.

### I am stuck. How can I get help?

Join us on [Discord](https://discord.com/invite/dagger-io), and ask your question in our [help forum](https://discord.com/channels/707636530424053791/1030538312508776540). Our team will be happy to help you there!

## Dagger Cloud

### What is Dagger Cloud?

Dagger Cloud complements the Dagger Engine with a production-grade control plane. Features of Dagger Cloud include pipeline visualization and operational insights.

### Is Dagger Cloud a hosting service for Dagger Engines?

No, Dagger Cloud is a "bring your own compute" service. The Dagger Engine can run on a wide variety of machines, including most development and CI platforms. If the Dagger Engine can run on it, then Dagger Cloud supports it.

### Which CI providers does Dagger Cloud work with?

Because the Dagger Engine can integrate seamlessly with practically any CI, there is no limit to the type and number of CI providers that Dagger Cloud can work with to provide Dagger pipeline visualization and operational insights. Users report successfully leveraging Dagger with: GitLab, CircleCI, GitHub Actions, Jenkins,Tekton and many more.

### What is pipeline visualization?

Traces, a browser-based interface focused on tracing and debugging Dagger pipeline runs. A Trace contains detailed information about the steps performed by the pipeline. Traces let you visualize each step of your pipeline, drill down to detailed logs, understand how long operations took to run, and whether operations were cached.

### What operational insights does Dagger Cloud provide?

Dagger Cloud collects telemetry from all your organization's Dagger Engines, whether they run in development or CI, and presents it all to you in one place. This gives you a unique view on all pipelines, both pre-push and post-push.

### Why am I seeing `dagger functions` in the local trace list?

All commands that require module initialization at an engine level will send telemetry to Dagger Cloud.
Dagger needs to introspect a module to be able to print the available functions in a module, so it calls `dagger functions`. This happens for both local and remote runs, which is why the calls appears in the local trace list.

### How does Dagger classify Traces as originating from "CI" or "local"?

Dagger is aware of the context it runs on. When it runs in a CI context like GitHub, GitLab, CircleCI, or Jenkins, additional Trace metadata is displayed based on the Git repository information available. For this reason, it is important for Dagger to run in a Git context when running in CI.

### What are "orphaned Traces"?

You might see a warning message in Dagger Cloud about orphaned Traces. Orphaned Traces are Traces emitted in a CI context, that contain incomplete or no Git metadata. This generally happens when Git is not properly set up in the CI context that Dagger runs in. In GitHub Actions, for example, this context can be provided by using the `checkout` action in the step where Dagger is called.

### My CI provider is not supported in Dagger Cloud. Is there a way I can "force" my Traces into the Dagger Cloud dashboard?

It‚Äôs possible to send Traces by setting the `CI=true` variable in Dagger's runtime environment. However, Traces with incomplete Git repository data will show up as orphaned, so it is important to ensure that Dagger is running in a properly-set Git context.

## Dagger SDKs

### What language SDKs are available for Dagger?

We have [three types of SDKs](https://dagger.io/community-sdks), with varying levels of parity and support: official, community and experimental. We currently offer official SDKs for Go, TypeScript and Python. A community SDK is available for PHP, and an experimental SDK is available for Java.

### How can I move my SDK to a Dagger Community SDK?

To ensure a great experience for developers using Community SDKs, maintainers must meet the following requirements if you would like your SDK to graduate to the Community SDK level:

- Community Support ‚Äì The maintainer must be active in the Dagger Discord, providing support and answering questions about the SDK.
- Version Compatibility ‚Äì The SDK must stay up to date with the latest Dagger releases to ensure continued functionality.
- Documentation Maintenance ‚Äì The maintainer is responsible for writing and updating documentation, including code snippets and examples. See full list of documentation requirements [here](https://docs.google.com/spreadsheets/d/1pvpzZbWarkuws811NEEbnv2D-ggec4iuZeYhUyVwsWc/edit?gid=245490315#gid=245490315).
- Openness to Contributions ‚Äì Community SDKs should be open-source and encourage contributions from other developers.

If you want to kick off this process for your SDK, email community@dagger.io and we'll discuss further.

### How do I log in to a container registry using a Dagger SDK?

There are two options available:

1. Use the [`Container.withRegistryAuth()`](https://docs.dagger.io/api/reference/#Container-withRegistryAuth) GraphQL API method. A native equivalent of this method is available in each Dagger SDK.
1. Dagger SDKs can use your existing Docker credentials without requiring separate authentication. Simply execute `docker login` against your container registry on the host where your Dagger pipelines are running.

### How do I uninstall a Dagger SDK?

To uninstall a Dagger SDK, follow the same procedure that you would follow to uninstall any other SDK package in your chosen development environment.

## Dagger API

### What API query language does Dagger use?

Dagger uses GraphQL as its low-level language-agnostic API query language.

### Do I need to know GraphQL to use Dagger?

No. You only need to know one of Dagger's supported SDKs languages to use Dagger. The translation to underlying GraphQL API calls is handled internally by the Dagger SDK of your choice.

### There's no SDK for <language> yet. Can I still use Dagger?

Yes. It's possible to use the Dagger GraphQL API from any language that [supports GraphQL](https://graphql.org/code/)  or from the [Dagger CLI](./install.md).
</file>

<file path="docs/dagger.io/fs-filters.md">
---
slug: /api/filters
---

# Directory Filters

When you pass a directory to a Dagger Function as argument, Dagger uploads everything in that directory tree to the Dagger Engine. For large monorepos or directories containing large-sized files, this can significantly slow down your Dagger Function while filesystem contents are transferred. To mitigate this problem, Dagger lets you apply filters to control which files and directories are uploaded.

## Directory arguments

Dagger Functions do not have access to the filesystem of the host you invoke the Dagger Function from (i.e. the host you execute a CLI command like `dagger` from). Instead, host files and directories need to be explicitly passed as command-line arguments to Dagger Functions.

There are two important reasons for this.

- Reproducibility: By providing a call-time mechanism to define and control the files available to a Dagger Function, Dagger guards against creating hidden dependencies on ambient properties of the host filesystem that could change at any moment.
- Security: By forcing you to explicitly specify which host files and directories a Dagger Function "sees" on every call, Dagger ensures that you're always 100% in control. This reduces the risk of third-party Dagger Functions gaining access to your data.

To tell a Dagger Function which directory to use, specify its path as an argument when using `dagger call`. Here's a simple example, which passes a directory from the host (`./example/hello`) to a Dagger Function:

```
git clone https://github.com/golang/example
dagger -m github.com/kpenfound/dagger-modules/golang@v0.2.0 call build --source=./example/hello --args=. directory --p
```

The important thing to know here is that, by default, Dagger will copy and upload everything in the specified directory and its sub-directories to the Dagger Engine. For complex directory trees, directories containing a large number of files, or directories containing large-sized files, this can add minutes to your Dagger Function execution time while the contents are transferred.

Dagger offers pre- and post-call filtering to mitigate this problem and optimize how your directories are handled.

## Why filter?

Filtering improves the performance of your Dagger Functions in three ways:

- It reduces the size of the files being transferred from the host to the Dagger Engine, allowing the upload step to complete faster.
- It ensures that minor unrelated changes in the source directory don't invalidate Dagger's build cache.
- It enables different use-cases, such as setting up component/feature/service-specific pipelines for monorepos.

It is worth noting that Dagger already uses caching to optimize file uploads. Subsequent calls to a Dagger Function will only upload files that have changed since the preceding call. Filtering is an additional optimization that you can apply to improve the performance of your Dagger Function.

## Pre-call filtering

Pre-call filtering means that a directory is filtered before it's uploaded to the Dagger Engine container. This is useful for:

- Large monorepos. Typically your Dagger Function only operates on a subset of the monorepo, representing a specific component or feature. Uploading the entire worktree imposes a prohibitive cost.

- Large files, such as audio/video files and other binary content. These files take time to upload. If they're not directly relevant, you'll usually want your Dagger Function to ignore them.

  > **Tip:**
  > The `.git` directory is a good example of both these cases. It contains a lot of data, including large binary objects, and for projects with a long version history, it can sometimes be larger than your actual source code.

- Dependencies. If you're developing locally, you'll typically have your project dependencies installed locally: `node_modules` (Node.js), `.venv` (Python), `vendor` (PHP) and so on. When you call your Dagger Function locally, Dagger will upload all these installed dependencies as well. This is both bad practice and inefficient. Typically, you'll want your Dagger Function to ignore locally-installed dependencies and only operate on the project source code.

> **Note:**
> At the time of writing, Dagger [does not read exclusion patterns from existing `.dockerignore`/`.gitignore` files](https://github.com/dagger/dagger/issues/6627). If you already use these files, you'll need to manually implement the same patterns in your Dagger Function.

To implement a pre-call filter in your Dagger Function, add an `ignore` parameter to your `Directory` argument. The `ignore` parameter follows the [`.gitignore` syntax](https://git-scm.com/docs/gitignore). Some important points to keep in mind are:

- The order of arguments is significant: the pattern `"**", "!**"` includes everything but `"!**", "**"` excludes everything.
- Prefixing a path with `!` negates a previous ignore: the pattern `"!foo"` has no effect, since nothing is previously ignored, while the pattern `"**", "!foo"` excludes everything except `foo`.

### Go

Here's an example of a Dagger Function that excludes everything in a given directory except Go source code files:

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns the list of Go files in the directory
func (m *MyModule) ListGoFiles(
	ctx context.Context,
	// Source directory
	// +ignore=["*", "!**/*.go"]
	source *dagger.Directory,
) ([]string, error) {
	return source.Entries(ctx)
}

```

### Python

Here's an example of a Dagger Function that excludes everything in a given directory except Python source code files:

```python
from typing import Annotated, List

import dagger
from dagger import Doc, Ignore, function, object_type


@object_type
class MyModule:
    @function
    async def list_python_files(
        self,
        source: Annotated[
            dagger.Directory,
            Doc("Source directory"),
            Ignore(["*", "!**/*.py"]),
        ],
    ) -> List[str]:
        """Returns the list of Python files in the directory"""
        return await source.entries()

```

### TypeScript

Here's an example of a Dagger Function that excludes everything in a given directory except TypeScript source code files:

```typescript
import { Directory, func, object, argument } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns the list of TypeScript files in the directory
   *
   * @param source Source directory
   */
  @func()
  async listTypescriptFiles(
    @argument({ description: "Source directory", ignore: ["*", "!**/*.ts"] })
    source: Directory,
  ): Promise<string[]> {
    return await source.entries()
  }
}

```

### PHP

Here's an example of a Dagger Function that excludes everything in a given directory except PHP source code files:

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Attribute\Ignore;
use Dagger\Client\Directory;

#[DaggerObject]
class MyModule
{
    /**
     * Returns the list of PHP files in the directory
     *
     * @param Directory $source Source directory
     * @return string[]
     */
    #[DaggerFunction]
    public function listPhpFiles(
        #[Ignore('*', '!**/*.php')]
        Directory $source
    ): array {
        return $source->entries();
    }
}

```

### Java

Here's an example of a Dagger Function that excludes everything in a given directory except Java source code files:

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Directory;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;
import io.dagger.module.annotation.Ignore;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns the list of Java files in the directory
   */
  @Function
  public List<String> listJavaFiles(
      @Description("Source directory") @Ignore({"*", "!**/*.java"}) Directory source) throws Exception {
    return source.entries().get();
  }
}

```

Here are a few examples of useful patterns:

### Go
```go
// exclude Go tests and test data
+ignore=["**_test.go", "**/testdata/**"]

// exclude binaries
+ignore=["bin"]

// exclude Python dependencies
+ignore=["**/.venv", "**/__pycache__"]

// exclude Node.js dependencies
+ignore=["**/node_modules"]

// exclude Git metadata
+ignore=[".git", "**/.gitignore"]
```

### Python
```python
# exclude Pytest tests and test data
Ignore(["tests/", ".pytest_cache"])

# exclude binaries
Ignore(["bin"])

# exclude Python dependencies
Ignore(["**/.venv", "**/__pycache__"])

# exclude Node.js dependencies
Ignore(["**/node_modules"])

# exclude Git metadata
Ignore([".git", "**/.gitignore"])
```

### TypeScript
```typescript
// exclude Mocha tests
@argument({ ignore: ["**.spec.ts"] })

// exclude binaries
@argument({ ignore: ["bin"] })

// exclude Python dependencies
@argument({ ignore: ["**/.venv", "**/__pycache__"] })

// exclude Node.js dependencies
@argument({ ignore: ["**/node_modules"] })

// exclude Git metadata
@argument({ ignore: [".git", "**/.gitignore"] })
```

### PHP
```php
// exclude PHPUnit tests and test data
#[Ignore('tests/', '.phpunit.cache', '.phpunit.result.cache')]

// exclude binaries
#[Ignore('bin')]

// exclude Composer dependencies
#[Ignore('vendor/')]

// exclude Node.js dependencies
#[Ignore('**/node_modules')]

// exclude Git metadata
#[Ignore('.git/', '**/.gitignore')]
```

### Java
```java
// exclude Java tests and test data
@Ignore({"src/test"})

// exclude binaries
@Ignore({"bin"})

// exclude Python dependencies
@Ignore({"**/.venv", "**/__pycache__"})

// exclude Node.js dependencies
@Ignore({"**/node_modules"})

// exclude Git metadata
@Ignore({".git", "**/.gitignore"})
```

## Post-call filtering

Post-call filtering means that a directory is filtered after it's uploaded to the Dagger Engine.

This is useful when working with directories that are modified "in place" by a Dagger Function. When building an application, your Dagger Function might modify the source directory during the build by adding new files to it. A post-call filter allows you to use that directory in another operation, only fetching the new files and ignoring the old ones.

A good example of this is a multi-stage build. Imagine a Dagger Function that reads and builds an application from source, placing the compiled binaries in a new sub-directory (stage 1). Instead of then transferring everything to the final container image for distribution (stage 2), you could use a post-call filter to transfer only the compiled files.

### Go

To implement a post-call filter in your Dagger Function, use the `DirectoryWithDirectoryOpts` or `ContainerWithDirectoryOpts` structs, which support `Include` and `Exclude` patterns for `Directory` objects. Here's an example:

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Build the application and return the build directory
func (m *MyModule) Build(ctx context.Context, source *dagger.Directory) (*dagger.Directory, error) {
	return dag.Container().
		From("golang:latest").
		WithDirectory("/src", source).
		WithWorkdir("/src").
		WithExec([]string{"go", "build", "-o", "build/app"}).
		Directory("build"). // return the build directory
		Sync(ctx)
}

// Build and package the application in a container
func (m *MyModule) Package(ctx context.Context, source *dagger.Directory) (*dagger.Container, error) {
	buildDir, err := m.Build(ctx, source)
	if err != nil {
		return nil, err
	}

	return dag.Container().
		From("alpine:latest").
		// copy only the compiled binary from the build directory
		WithDirectory("/usr/local/bin", buildDir, dagger.ContainerWithDirectoryOpts{
			Include: []string{"app"},
		}).
		Sync(ctx)
}

```

### Python

To implement a post-call filter in your Dagger Function, use the `include` and `exclude` parameters when working with `Directory` objects. Here's an example:

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def build(self, source: dagger.Directory) -> dagger.Directory:
        """Build the application and return the build directory"""
        return await (
            dag.container()
            .from_("golang:latest")
            .with_directory("/src", source)
            .with_workdir("/src")
            .with_exec(["go", "build", "-o", "build/app"])
            .directory("build")  # return the build directory
        )

    @function
    async def package(self, source: dagger.Directory) -> dagger.Container:
        """Build and package the application in a container"""
        build_dir = await self.build(source)

        return (
            dag.container()
            .from_("alpine:latest")
            # copy only the compiled binary from the build directory
            .with_directory(
                "/usr/local/bin",
                build_dir,
                include=["app"],
            )
        )

```

### TypeScript

To implement a post-call filter in your Dagger Function, use the `include` and `exclude` parameters when working with `Directory` objects. Here's an example:

```typescript
import { dag, Directory, Container, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Build the application and return the build directory
   */
  @func()
  async build(source: Directory): Promise<Directory> {
    return await dag
      .container()
      .from("golang:latest")
      .withDirectory("/src", source)
      .withWorkdir("/src")
      .withExec(["go", "build", "-o", "build/app"])
      .directory("build") // return the build directory
  }

  /**
   * Build and package the application in a container
   */
  @func()
  async package(source: Directory): Promise<Container> {
    const buildDir = await this.build(source)

    return dag
      .container()
      .from("alpine:latest")
      // copy only the compiled binary from the build directory
      .withDirectory("/usr/local/bin", buildDir, {
        include: ["app"],
      })
  }
}

```

### PHP

To implement a post-call filter in your Dagger Function, use the `include` and `exclude` parameters when working with `Directory` objects. Here's an example:

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Container;
use Dagger\Client\Directory;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Build the application and return the build directory
     */
    #[DaggerFunction]
    public function build(Directory $source): Directory
    {
        return dag()
            ->container()
            ->from('golang:latest')
            ->withDirectory('/src', $source)
            ->withWorkdir('/src')
            ->withExec(['go', 'build', '-o', 'build/app'])
            ->directory('build'); // return the build directory
    }

    /**
     * Build and package the application in a container
     */
    #[DaggerFunction]
    public function package(Directory $source): Container
    {
        $buildDir = $this->build($source);

        return dag()
            ->container()
            ->from('alpine:latest')
            // copy only the compiled binary from the build directory
            ->withDirectory(
                '/usr/local/bin',
                $buildDir,
                include: ['app']
            );
    }
}

```

### Java

To implement a post-call filter in your Dagger Function, use the `Container.WithDirectoryArguments` class which support `withInclude` and `withExclude` functions when working with `Directory` objects. Here's an example:

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.client.Directory;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Build the application and return the build directory
   */
  @Function
  public Directory build(Directory source) throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("golang:latest")
          .withDirectory("/src", source)
          .withWorkdir("/src")
          .withExec(List.of("go", "build", "-o", "build/app"))
          .directory("build") // return the build directory
          .sync();
    }
  }

  /**
   * Build and package the application in a container
   */
  @Function
  public Container package_(Directory source) throws Exception {
    Directory buildDir = this.build(source);
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("alpine:latest")
          // copy only the compiled binary from the build directory
          .withDirectory(
              "/usr/local/bin",
              buildDir,
              new Container.WithDirectoryArguments().withInclude(List.of("app")))
          .sync();
    }
  }
}

```

Here are a few examples of useful patterns:

### Go
```go
// exclude all Markdown files
dirOpts := dagger.ContainerWithDirectoryOpts{
  Exclude: "*.md*",
}

// include only the build output directory
dirOpts := dagger.ContainerWithDirectoryOpts{
  Include: "build",
}

// include only ZIP files
dirOpts := dagger.DirectoryWithDirectoryOpts{
  Include: "*.zip",
}

// exclude Git metadata
dirOpts := dagger.DirectoryWithDirectoryOpts{
  Exclude: "*.git",
}
```

### Python
```python
# exclude all Markdown files
dir_opts = {"exclude": ["*.md*"]}

# include only the build output directory
dir_opts = {"include": ["build"]}

# include only ZIP files
dir_opts = {"include": ["*.zip"]}

# exclude Git metadata
dir_opts = {"exclude": ["*.git"]}
```

### TypeScript
```typescript
// exclude all Markdown files
const dirOpts = { exclude: ["*.md*"] }

// include only the build output directory
const dirOpts = { include: ["build"] }

// include only ZIP files
const dirOpts = { include: ["*.zip"] }

// exclude Git metadata
const dirOpts = { exclude: ["*.git"] }
```

### PHP
```php
// exclude all Markdown files
$dirOpts = ['exclude' => ['*.md*']];

// include only the build output directory
$dirOpts = ['include' => ['build']];

// include only ZIP files
$dirOpts = ['include' => ['*.zip']];

// exclude Git metadata
$dirOpts = ['exclude' => ['*.git']];
```

### Java
```java
// exclude all Markdown files
var dirOpts = new Container.WithDirectoryArguments()
    .withExclude(List.of("*.md*"));

// include only the build output directory
var dirOpts = new Container.WithDirectoryArguments()
    .withInclude(List.of("build"));

// include only ZIP files
var dirOpts = new Container.WithDirectoryArguments()
    .withInclude(List.of("*.zip"));

// exclude Git metadata
var dirOpts = new Container.WithDirectoryArguments()
    .withExclude(List.of("*.git"));
```

## Debugging

### Using logs

Both Dagger Cloud and the Dagger TUI provide detailed information on the patterns Dagger uses to filter your directory uploads - look for the upload step in the TUI logs or Trace:

![Dagger TUI](/img/current_docs/api/fs-filters-tui.png)

![Dagger Cloud Trace](/img/current_docs/api/fs-filters-trace.png)

### Inspecting directory contents

Another way to debug how directories are being filtered is to create a function that receives a `Directory` as input, and returns the same `Directory`:

### Go
```go
func (m *MyModule) Debug(
  ctx context.Context,
  // +ignore=["*", "!analytics"]
  source *dagger.Directory,
) *dagger.Directory {
  return source
}
```

### Python
```python
@function
async def foo(
    self,
    source: Annotated[
        dagger.Directory, Ignore(["*", "!analytics"])
    ],
) -> dagger.Directory:
    return source
```

### TypeScript
```typescript
@func()
debug(
   @argument({ ignore: ["*", "!analytics"] }) source: Directory,
): Directory {
  return source
}
```

### PHP
```php
    #[DaggerFunction]
    public function debug(
        #[Ignore('*'/, '!analytics')]
        Directory $source,
    ): Directory {
        return $source;
    }
```

### Java
```java
@Function
public Directory debug(@Ignore({"*", "!analytics"}) Directory source) {
    return source;
}
```

Calling the function will show you the directory‚Äôs digest and top level entries. The digest is content addressed, so it changes if there are changes in the contents of the directory. Looking at the entries field you may be able to spot an interloper:

### System shell
```shell
dagger -c 'debug .'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
debug .
```

### Dagger CLI
```shell
dagger call debug --source=.
```

You can also list all files, recursively to check it more deeply:

### System shell
```shell
dagger -c 'debug . | glob "**/*"'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
debug . | glob "**/*"
```

### Dagger CLI
```shell
dagger call debug --source=. glob --pattern="**/*"
```

You can open the directory in an interactive terminal to inspect the filesystem:

### System shell
```shell
dagger -c 'debug . | terminal'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
debug . | terminal
```

### Dagger CLI
```shell
dagger call debug --source=. terminal
```

You can export the filtered directory to your host and check it with local tools:

### System shell
```shell
dagger -c 'debug . | export audit'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
debug . | export audit
```

### Dagger CLI
```shell
dagger call debug --source=. export --path=audit
</file>

<file path="docs/dagger.io/github-actions.md">
---
slug: /ci/integrations/github-actions
---

# GitHub Actions

Dagger provides a GitHub Action that can be used in any GitHub Actions workflow to call one or more Dagger Functions on specific events, such as a new commit.

Dagger has also partnered with [Depot](https://depot.dev/) to provide managed, Dagger Powered GitHub Actions runners. These runners, which serve as drop-in replacements for GitHub's own runners, come with Dagger pre-installed and pre-configured to best practices, automatic persistent layer caching, and multi-architecture support. They make it faster and easier to run Dagger pipelines in GitHub repositories.

## How it works

Depending on whether you choose a standard GitHub Actions runner or a Dagger Powered Depot GitHub Actions Runner, here's how it works.

### GitHub

When running a CI pipeline with Dagger using a standard GitHub Actions runner, the general workflow looks like this:

1. GitHub receives a workflow trigger based on a repository event.
1. GitHub begins processing the jobs and steps in the workflow.
1. GitHub finds the "Dagger for GitHub" action and passes control to it.
1. The "Dagger for GitHub" action calls the Dagger CLI with the specified sub-command, module, function name, and arguments.
1. The Dagger CLI attempts to find an existing Dagger Engine or spins up a new one inside the GitHub Actions runner.
1. The Dagger CLI executes the specified sub-command and sends telemetry to Dagger Cloud if the `DAGGER_CLOUD_TOKEN` environment variable is set.
1. The workflow completes with success or failure. Logs appear in GitHub as usual.

### Depot

When running a CI pipeline on a Dagger Powered Depot GitHub Actions Runner, the general workflow looks like this:

1. GitHub emits a webhook event which triggers a job in a GitHub Actions workflow
1. Depot receives this event, processes it, and launches an isolated Depot GitHub Actions Runner.
1. Each Depot GitHub Actions Runner is automatically connected to an already running Dagger Engine (this exists outside of the Depot GitHub Actions Runner).
    * The external Dagger Engine comes with persistent caching pre-configured.
1. The Depot GitHub Actions Runner has the Dagger CLI pre-installed.
    * The Dagger CLI running inside the Depot GitHub Actions Runner gets automatically connected to Dagger Cloud.
    * The Dagger CLI is also pre-configured to connect to the external Dagger Engine with persistent caching
1. The "Dagger for GitHub" GitHub Action calls the Dagger CLI with the specified sub-command, module, function name, and arguments.
1. The Dagger CLI executes the specified sub-command and sends telemetry to Dagger Cloud.
1. The workflow completes with success or failure. Logs appear in GitHub as usual. They also appear in Dagger Cloud.

## Prerequisites

### GitHub

- A GitHub repository

### Depot

- A GitHub repository in a GitHub organization
- A Depot account and [organization](https://depot.dev/docs/github-actions/quickstart#create-an-organization)
- The [Depot organization connected to the GitHub organization](https://depot.dev/docs/github-actions/quickstart#connect-to-github)

## Examples

### GitHub

The following example demonstrates how to call a Dagger Function on a standard GitHub runner in a GitHub Actions workflow.

```yaml title=".github/workflows/dagger.yml"
# .github/workflows/dagger.yml
name: Dagger

on:
  push:
    branches: ["main"]

jobs:
  hello:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Dagger CLI
        uses: dagger/setup-dagger@v5

      - name: Run Dagger Function
        uses: dagger/dagger-for-github@v5
        with:
          # Replace with your Dagger Function call
          # Example: call container-echo --string-arg="Hello from Dagger!" stdout
          # Example: call test build publish --tag=ttl.sh/my-image
          args: "call container-echo --string-arg='Hello from Dagger!' stdout"
          # Optional: Dagger Cloud token
          # cloud-token: ${{ secrets.DAGGER_CLOUD_TOKEN }}
```

The following is a more complex example demonstrating how to create a GitHub Actions workflow that checks out source code, calls a Dagger Function to test the project, and then calls another Dagger Function to build and publish a container image of the project. This example uses a simple [Go application](https://github.com/kpenfound/greetings-api) and assumes that you have already forked it in your own GitHub repository.

```yaml title=".github/workflows/dagger.yml"
# .github/workflows/dagger.yml
name: Dagger

on:
  push:
    branches: ["main"]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Dagger CLI
        uses: dagger/setup-dagger@v5

      - name: Run Dagger Function
        uses: dagger/dagger-for-github@v5
        with:
          # Replace with your Dagger Function call
          # Example: call test
          args: "call test"
          # Optional: Dagger Cloud token
          # cloud-token: ${{ secrets.DAGGER_CLOUD_TOKEN }}

  build:
    runs-on: ubuntu-latest
    needs: [test]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Dagger CLI
        uses: dagger/setup-dagger@v5

      - name: Run Dagger Function
        uses: dagger/dagger-for-github@v5
        with:
          # Replace with your Dagger Function call
          # Example: call build publish --tag=ttl.sh/my-image
          args: "call build publish --tag=ttl.sh/my-image"
          # Optional: Dagger Cloud token
          # cloud-token: ${{ secrets.DAGGER_CLOUD_TOKEN }}
```

More information is available in the [Dagger for GitHub page](https://github.com/marketplace/actions/dagger-for-github).

### Depot

The following example demonstrates how to call a Dagger Function on a Dagger Powered Depot runner in a GitHub Actions workflow. The `runs-on` clause specifies the runner and Dagger versions.

```yaml title=".github/workflows/dagger.yml"
# .github/workflows/dagger.yml
name: Dagger

on:
  push:
    branches: ["main"]

jobs:
  hello:
    runs-on: depot-ubuntu-2204-m # Replace with your Depot runner label
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run Dagger Function
        uses: dagger/dagger-for-github@v5
        with:
          # Replace with your Dagger Function call
          # Example: call container-echo --string-arg="Hello from Dagger!" stdout
          # Example: call test build publish --tag=ttl.sh/my-image
          args: "call container-echo --string-arg='Hello from Dagger!' stdout"
          # Optional: Dagger Cloud token
          # cloud-token: ${{ secrets.DAGGER_CLOUD_TOKEN }}
```

The following is a more complex example demonstrating how to create a GitHub Actions workflow that checks out source code, calls a Dagger Function to test the project, and then calls another Dagger Function to build and publish a container image of the project. This example uses a simple [Go application](https://github.com/kpenfound/greetings-api) and assumes that you have already forked it in your own GitHub repository.

```yaml title=".github/workflows/dagger.yml"
# .github/workflows/dagger.yml
name: Dagger

on:
  push:
    branches: ["main"]

jobs:
  test:
    runs-on: depot-ubuntu-2204-m # Replace with your Depot runner label
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run Dagger Function
        uses: dagger/dagger-for-github@v5
        with:
          # Replace with your Dagger Function call
          # Example: call test
          args: "call test"
          # Optional: Dagger Cloud token
          # cloud-token: ${{ secrets.DAGGER_CLOUD_TOKEN }}

  build:
    runs-on: depot-ubuntu-2204-m # Replace with your Depot runner label
    needs: [test]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run Dagger Function
        uses: dagger/dagger-for-github@v5
        with:
          # Replace with your Dagger Function call
          # Example: call build publish --tag=ttl.sh/my-image
          args: "call build publish --tag=ttl.sh/my-image"
          # Optional: Dagger Cloud token
          # cloud-token: ${{ secrets.DAGGER_CLOUD_TOKEN }}
```

> **Warning:**
> Always ensure that the same Dagger versions are specified in the `runs-on` and `version` clauses of the workflow definition.

More information is available in the [Dagger for GitHub page](https://github.com/marketplace/actions/dagger-for-github) and in the [Depot documentation](https://depot.dev/docs/).

### SSH configuration

When using SSH keys in GitHub Actions, ensure proper SSH agent setup:

```yaml
- name: Set up SSH
  run: |
    eval "$(ssh-agent -s)"
    ssh-add - <<< '${{ secrets.SSH_PRIVATE_KEY }}'
```

Replace `${{ secrets.SSH_PRIVATE_KEY }}` with your provider secret containing the private key.

## Resources

Below are some resources from the Dagger community that may help as well. If you have any questions about additional ways to use GitHub with Dagger, join our [Discord](https://discord.gg/dagger-io) and ask your questions in our [GitHub channel](https://discord.com/channels/707636530424053791/1117139064274034809).

- [Video: Boost Dagger Performance on GitHub Actions with Depot](https://www.youtube.com/watch?v=55fQqPnJ4XU): This video demonstrates how to run your Dagger pipelines on Depot's Dagger Powered runners.
- [Video: Set up Dagger Cloud with Depot](https://www.youtube.com/watch?v=Y59gmLLOOT8): This video provides a step-by-step walkthrough of connecting Dagger Cloud to Depot and GitHub Actions.

## About GitHub

[GitHub](https://github.com/) is a popular Web-based platform used for version control and collaboration. It allows developers to store and manage their code in repositories, track changes over time, and collaborate with other developers on projects.

## About Depot

[Depot](https://depot.dev/) is a build acceleration platform focused on making builds more efficient and performant.
</file>

<file path="docs/dagger.io/github.md">
---
slug: /ci/integrations/github
---

# GitHub

Dagger can directly interact with GitHub pull requests, making it easy to test the functionality of specific forks or branches of a GitHub repository.

Dagger also supports publishing container images to [GitHub Container Registry](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry).

## How it works

GitHub contains a shorthand redirect at the `/merge` endpoint that allows you to reference the correct branch of a repository from a pull request (PR), without needing to know anything about the fork or branch where the PR came from.

By default, the Dagger `Directory` type works with both local directories and [remote Git repositories](../../cookbook/cookbook.md#copy-a-directory-or-remote-repository-to-a-container). This makes it possible to work with the directory tree at the root of a Git repository or a given branch.

By combining these two features, Dagger users can write Dagger Functions that directly use GitHub pull requests as arguments.

## Prerequisites

- A GitHub repository

## Examples

Given a Dagger Function called `foo` that accepts a `Directory` as argument, you can pass it a GitHub pull request URL as argument like this:

```shell
dagger call foo --directory=https://github.com/ORGANIZATION/REPOSITORY#pull/NUMBER/merge
```

If your GitHub repository contains a Dagger module, you can test the functionality of a specific branch by calling the Dagger module with the corresponding pull request URL, as shown below:

```shell
dagger call -m github.com/ORGANIZATION/REPOSITORY@pull/NUMBER/merge --help
```

You can also use a Dagger Function in a GitHub Actions workflow to publish a container image to GitHub Container Registry.

```yaml title=".github/workflows/dagger.yml"
# .github/workflows/dagger.yml
name: Dagger

on:
  push:
    branches: ["main"]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Dagger CLI
        uses: dagger/setup-dagger@v5

      - name: Run Dagger Function
        uses: dagger/dagger-for-github@v5
        with:
          # Replace with your Dagger Function call
          # Example: call container-echo --string-arg="Hello from Dagger!" stdout
          # Example: call test build publish --tag=ttl.sh/my-image
          args: "call build publish --tag=ghcr.io/${{ github.repository }}:${{ github.sha }}"
          # Optional: Dagger Cloud token
          # cloud-token: ${{ secrets.DAGGER_CLOUD_TOKEN }}
        env:
          # Required: GitHub token to publish container image to GitHub Container Registry
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

## Resources

If you have any questions about additional ways to use GitHub with Dagger, join our [Discord](https://discord.gg/dagger-io) and ask your questions in our [GitHub channel](https://discord.com/channels/707636530424053791/1117139064274034809).

## About GitHub

[GitHub](https://github.com/) is a popular Web-based platform used for version control and collaboration. It allows developers to store and manage their code in repositories, track changes over time, and collaborate with other developers on projects.
</file>

<file path="docs/dagger.io/gitlab.md">
---
slug: /ci/integrations/gitlab
---

# GitLab CI

Dagger provides a programmable container engine that allows you to replace your YAML pipeline definitions in GitLab with Dagger Functions written in a regular programming language. This allows you to execute your pipeline the same locally and in GitLab, with the additional benefit of intelligent caching.

## How it works

When running a CI pipeline with Dagger using GitLab CI, the general workflow looks like this:

1. GitLab receives a trigger based on a repository event.
1. GitLab begins processing the stages and jobs in the `.gitlab-ci.yml` file.
1. GitLab downloads the Dagger CLI.
1. GitLab executes one (or more) Dagger CLI commands, such as `dagger call ...`.
1. The Dagger CLI attempts to find an existing Dagger Engine or spins up a new one inside the GitLab runner.
1. The Dagger CLI calls the specified Dagger Function and sends telemetry to Dagger Cloud if the `DAGGER_CLOUD_TOKEN` environment variable is set.
1. The pipeline completes with success or failure. Logs appear in GitLab as usual.

## Prerequisites

- A GitLab repository
- Any one of the following:
  - [GitLab-hosted runners](https://docs.gitlab.com/ee/ci/runners/index.html) using the (default) [Docker Machine executor](https://docs.gitlab.com/runner/executors/docker_machine.html)
  - [Self-managed GitLab Runners](https://docs.gitlab.com/runner/install/index.html) using the [Docker executor](https://docs.gitlab.com/runner/executors/docker.html).
  - [Self-managed GitLab Runners](https://docs.gitlab.com/runner/install/index.html) in a Kubernetes cluster and using the [Kubernetes executor](https://docs.gitlab.com/runner/executors/kubernetes/).

## Examples

### Docker executor

The following example demonstrates how to call a Dagger Function in a GitLab CI/CD pipeline using the (default) [Docker Machine executor](https://docs.gitlab.com/runner/executors/docker_machine.html) or the [Docker executor](https://docs.gitlab.com/runner/executors/docker.html). In both these cases, the Dagger Engine is provisioned "just in time" using a Docker-in-Docker (`dind`) service.

```yaml title=".gitlab-ci.yml"
# .gitlab-ci.yml
stages:
  - hello

hello:
  stage: hello
  image: alpine:latest
  services:
    - docker:dind
  variables:
    # When using dind service we need to instruct docker to talk with the daemon started by the service.
    # The daemon is available with a network connection instead of the default /var/run/docker.sock socket.
    # See https://docs.gitlab.com/ee/ci/docker/using_docker_build.html#use-docker-in-docker-executor
    DOCKER_HOST: tcp://docker:2375
    # Instruct Docker not to start over TLS.
    DOCKER_TLS_CERTDIR: ""
    # Improve performance with overlayfs.
    DOCKER_DRIVER: overlay2
  before_script:
    # Install Dagger CLI
    - apk add --update curl && curl -L https://dl.dagger.io/dagger/install.sh | sh
    - mv bin/dagger /usr/local/bin
    - dagger version
  script:
    # Replace with your Dagger Function call
    # Example: dagger call container-echo --string-arg="Hello from Dagger!" stdout
    # Example: dagger call test build publish --tag=ttl.sh/my-image
    - dagger call container-echo --string-arg="Hello from Dagger!" stdout
```

The following is a more complex example demonstrating how to create a GitLab pipeline that checks out source code, calls a Dagger Function to test the project, and then calls another Dagger Function to build and publish a container image of the project. This example uses a simple [Go application](https://github.com/kpenfound/greetings-api) and assumes that you have already forked it in your own GitLab repository.

```yaml title=".gitlab-ci.yml"
# .gitlab-ci.yml
stages:
  - test
  - build

variables:
  # When using dind service we need to instruct docker to talk with the daemon started by the service.
  # The daemon is available with a network connection instead of the default /var/run/docker.sock socket.
  # See https://docs.gitlab.com/ee/ci/docker/using_docker_build.html#use-docker-in-docker-executor
  DOCKER_HOST: tcp://docker:2375
  # Instruct Docker not to start over TLS.
  DOCKER_TLS_CERTDIR: ""
  # Improve performance with overlayfs.
  DOCKER_DRIVER: overlay2

before_script:
  # Install Dagger CLI
  - apk add --update curl && curl -L https://dl.dagger.io/dagger/install.sh | sh
  - mv bin/dagger /usr/local/bin
  - dagger version

test:
  stage: test
  image: alpine:latest
  services:
    - docker:dind
  script:
    # Replace with your Dagger Function call
    # Example: dagger call test
    - dagger call test

build:
  stage: build
  image: alpine:latest
  services:
    - docker:dind
  script:
    # Replace with your Dagger Function call
    # Example: dagger call build publish --tag=ttl.sh/my-image
    - dagger call build publish --tag=ttl.sh/my-image
```

### Kubernetes executor

The following example demonstrates how to call a Dagger Function in a GitLab CI/CD pipeline using the [Kubernetes executor](https://docs.gitlab.com/runner/executors/kubernetes/).

```yaml title=".gitlab-ci.yml"
# .gitlab-ci.yml
stages:
  - hello

hello:
  stage: hello
  image: alpine:latest
  tags:
    - dagger-node
  before_script:
    # Install Dagger CLI
    - apk add --update curl && curl -L https://dl.dagger.io/dagger/install.sh | sh
    - mv bin/dagger /usr/local/bin
    - dagger version
  script:
    # Replace with your Dagger Function call
    # Example: dagger call container-echo --string-arg="Hello from Dagger!" stdout
    # Example: dagger call test build publish --tag=ttl.sh/my-image
    - dagger call container-echo --string-arg="Hello from Dagger!" stdout
```

The following is a more complex example demonstrating how to create a GitLab pipeline that checks out source code, calls a Dagger Function to test the project, and then calls another Dagger Function to build and publish a container image of the project. This example uses a simple [Go application](https://github.com/kpenfound/greetings-api) and assumes that you have already forked it in your own GitLab repository.


```yaml title=".gitlab-ci.yml"
# .gitlab-ci.yml
stages:
  - test
  - build

before_script:
  # Install Dagger CLI
  - apk add --update curl && curl -L https://dl.dagger.io/dagger/install.sh | sh
  - mv bin/dagger /usr/local/bin
  - dagger version

test:
  stage: test
  image: alpine:latest
  tags:
    - dagger-node
  script:
    # Replace with your Dagger Function call
    # Example: dagger call test
    - dagger call test

build:
  stage: build
  image: alpine:latest
  tags:
    - dagger-node
  script:
    # Replace with your Dagger Function call
    # Example: dagger call build publish --tag=ttl.sh/my-image
    - dagger call build publish --tag=ttl.sh/my-image
```

In both cases, each GitLab Runner must be configured to only run on nodes with pre-provisioned instances of the Dagger Engine. This is achieved using taints and tolerations on the nodes, and pod affinity.

The following code listings illustrate the configuration to be applied to each GitLab Runner, with taints, tolerations and pod affinity set via the `dagger-node` key. For an example of the corresponding node configuration, refer to the [OpenShift](./openshift.md) integration page.

To use this configuration, replace the YOUR-GITLAB-URL placeholder with the URL of your GitLab instance and replace the YOUR-GITLAB-RUNNER-TOKEN-REFERENCE placeholder with your [GitLab Runner authentication token](https://docs.gitlab.com/ee/ci/runners/runners_scope.html#create-a-shared-runner-with-a-runner-authentication-token).

```yaml title="runner-config.yml"
# runner-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gitlab-runner-config
  namespace: gitlab-runner
data:
  config.toml: |
    concurrent = 10
    check_interval = 30
    log_level = "info"
    listen_address = '[::]:9252'

    [session_server]
      session_timeout = 1800

    [[runners]]
      name = "dagger-runner"
      url = "YOUR-GITLAB-URL"
      token = "YOUR-GITLAB-RUNNER-TOKEN-REFERENCE"
      executor = "kubernetes"
      [runners.kubernetes]
        namespace = "gitlab-runner"
        privileged = true
        image = "alpine:latest"
        [runners.kubernetes.node_selector]
          "dagger/enabled" = "true"
        [runners.kubernetes.node_tolerations]
          "dagger=enabled:NoSchedule" = ""
        [runners.kubernetes.affinity]
          [runners.kubernetes.affinity.nodeAffinity]
            [runners.kubernetes.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution]
              [[runners.kubernetes.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms]]
                [[runners.kubernetes.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchExpressions]]
                  key = "dagger/enabled"
                  operator = "In"
                  values = ["true"]
```

```yaml title="runner.yml"
# runner.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitlab-runner
  namespace: gitlab-runner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gitlab-runner
  template:
    metadata:
      labels:
        app: gitlab-runner
    spec:
      serviceAccountName: gitlab-runner
      containers:
        - name: gitlab-runner
          image: gitlab/gitlab-runner:latest
          args:
            - run
          volumeMounts:
            - name: config
              mountPath: /etc/gitlab-runner/config.toml
              subPath: config.toml
            - name: dagger-socket
              mountPath: /var/run/dagger
      volumes:
        - name: config
          configMap:
            name: gitlab-runner-config
        - name: dagger-socket
          hostPath:
            path: /var/run/dagger
            type: DirectoryOrCreate
```


## Resources

If you have any questions about additional ways to use GitLab with Dagger, join our [Discord](https://discord.gg/dagger-io) and ask your questions in our [GitLab channel](https://discord.com/channels/707636530424053791/1122940615806685296).

## About GitLab

[GitLab](https://gitlab.com/) is a popular Web-based platform used for version control and collaboration. It allows developers to store and manage their code in repositories, track changes over time, and collaborate with other developers on projects.
</file>

<file path="docs/dagger.io/ide-integration.md">
---
slug: /api/ide-integration
---

# IDE Integration

Dagger uses GraphQL as its low-level language-agnostic API query language, and each Dagger SDK generates native code-bindings for all dependencies from this API. This gives you all the benefits of type-checking, code completion and other IDE features for your favorite language when developing Dagger Functions.

The `dagger develop` command bootstraps a Dagger module template in the selected programming language and sets up or updates all the resources needed to develop a module. After running this command, follow the steps below to have your IDE recognize the Dagger module.

### Go
To get your IDE to recognize a Dagger Go module, [configure your `go.work` file](./custom-functions.md#go-workspaces) to include the path to your module.

When you generate a Dagger module for the first time, Dagger will create a `go.mod` file just for the module. If the module exists in a sub-directory of an outer Go project (such as `dagger/`, the default), this might confuse the IDE.

If there is already a `go.mod` in the parent directory that you would like to use instead, delete the newly-generated `go.mod` and `go.sum` and Dagger will use the parent one instead. This is not advisable if your Dagger module has certain dependencies that are not relevant to your core product, since most consumers will prefer a narrower set of dependencies.

To keep the generated `go.mod` file and edit your Dagger module in the same IDE session as your project code, one option is to set up a [Go workspace](https://go.dev/doc/tutorial/workspaces):

```shell
# in the root of your repository
go work init
go work use ./
go work use ./path/to/mod
```

After restarting the IDE, you should be able to use IDE features such as go-to-definition, even when editing `dagger/main.go` from the project root.

In most cases, `go.work` should not be committed to the repository, so it is advisable to add it to the project's `.gitignore` file:

```shell
echo go.work >> .gitignore
echo go.work.sum >> .gitignore
```

> **Tip:**
> You can also choose to omit the local Go files generated by `dagger develop` from your version control system, as they aren't required to run the module.

When working with multiple inter-dependent Dagger modules, it can be challenging to navigate between Dagger Functions defined in different modules. Source-maps offer a solution to this problem, by allowing developers to directly click through or navigate to type declarations in their IDE.

The Go SDK automatically attaches source-maps to the module code. These are a record of the filename, the line number, and the column number that a type declaration corresponds to in the source code. The SDK analyzes the source code, and outputs these (as line comments of the form `./path/to/filename:line`) when creating the type definitions during module initialization.

Most popular IDEs support source-maps, either natively or with an external plugin, such as the [Open file plugin](https://marketplace.visualstudio.com/items?itemName=Fr43nk.seito-openfile) for Visual Studio Code.

### Python
To get your IDE to recognize a Dagger Python module and any added Dagger module dependencies in the generated client, all dependencies must be installed in an activated [virtual environment](https://packaging.python.org/en/latest/tutorials/installing-packages/#creating-virtual-environments) (for example, in `.venv`, next to `pyproject.toml`). This can either be done manually or transparently by a package manager.

For example, to open the Visual Studio Code from the terminal, with functioning autocompletions (assuming the [Python extensions](https://github.com/microsoft/vscode-python) are installed):

```shell
dagger develop
uv run code .
```

For NeoVim, just replace `code` with `vim`, or any other terminal based editor.

> **Note:**
> The `uv run` command works with a `uv.lock` file. If you need to use a `requirements.lock` or other, continue reading for more examples.

## Project environment

The following examples show how to get a working [project environment](https://docs.astral.sh/uv/concepts/projects/#project-environments) (`.venv`) from the terminal, just make sure you run `dagger develop` first, to get the necessary `sdk` directory locally.

### uv

When using a `uv.lock`:

```shell
uv sync
```

If you have an older Dagger module, without a `uv.lock` file, you can create it with:

```shell
uv add --editable ./sdk
rm requirements.lock
````

> **Note:**
> uv with `uv.lock` needs to have the SDK library (`dagger-io`, generated in `./sdk`) defined as a production dependency, unlike the other installation methods where it should be a development dependency.

### uv pip

When using a `requirements.lock` file, uv provides a ["pip interface"](https://docs.astral.sh/uv/pip/) that replaces multiple tools, while still being much faster. It can manage the Python installation, virtual environment creation, and dependency installation.

For example:

```shell
uv venv
uv pip install -r requirements.lock -e ./sdk -e .
```

> **Note:**
> By default, `uv venv` will create an environment with Python 3.12. To pin Python 3.11 both locally and in Dagger, run this first:
>
> ```shell
> echo 3.11 > .python-version
> ```

### pip

Make sure you have the same Python version that Dagger uses first. By default it's Python 3.12, unless it's overridden in `pyproject.toml` or in `.python-version`.

```shell
python -m venv .venv
source .venv/bin/activate
python -m pip install -r requirements.lock -e ./sdk -e .
```

### hatch

First, update `pyproject.toml` to [allow direct references](https://hatch.pypa.io/latest/config/metadata/#allowing-direct-references), and add the [local](https://hatch.pypa.io/latest/config/dependency/#local) SDK library (`dagger-io`) as a  `dev` dependency:

```toml
[project]
name = "main"
version = "0.0.0"

[tool.hatch.metadata]
allow-direct-references = true

[tool.hatch.envs.dev]
dependencies = [
  "dagger-io @ {root:uri}/sdk",
]
```

Then, open the editor with the `dev` [Hatch environment](https://hatch.pypa.io/latest/config/environment/overview/):

```shell
hatch run dev:vim .
```

> **Note:**
> Hatch manages the project environment in a central location by default instead of `.venv`.

### poetry

First, add `./sdk` as a development dependency:

```shell
poetry add --group=dev -e ./sdk
```

Then, open the editor with:

```shell
poetry run vim .
```

> **Note:**
> Poetry manages the project environment in a central location instead of `.venv`.

> **Tip:**
> If you place the virtual environment (`.venv`) inside the module, don't forget to add it to `.gitignore` and to `"exclude"` in `dagger.json` to avoid uploading those files to the runtime container unnecessarily.

### Visual Studio Code

The simplest way to get [autocompletions and IntelliSense](https://code.visualstudio.com/docs/python/editing#_autocomplete-and-intellisense) working is to open it with the directory that has `.venv` (next to `.pyproject.toml`), because the IDE picks it up automatically when it's in the root of the [workspace](https://code.visualstudio.com/docs/python/python-tutorial#_start-vs-code-in-a-workspace-folder):

```shell
code .
```

VS Code doesn't need to be opened from the terminal though. The same directory can be opened with VS Code's **File > Open Folder** from the graphical interface (GUI).

The virtual environment can also be created and managed from within the IDE instead of from the terminal, if it supports the chosen lock file format. To know more, see [Visual Studio Code Python Environments](https://code.visualstudio.com/docs/python/environments).

### PyCharm
PyCharm is a very popular IDE for Python projects. The simplest way for PyCharm to recognize the `dagger` package is to open the IDE in the directory that has `.venv` and `pyproject.toml`, via the terminal or the GUI.

It can also create and manage the project environment, but may not support `uv.lock` yet, so best run `uv` from the terminal instead.

### TypeScript

When opening the generated `dagger/src/index.ts` in an IDE, most IDEs will automatically recognize the `@dagger.io/dagger` package, so long as the `tsconfig.json` file has a path configured on it.

For Dagger modules initialized using `dagger init`, the default template is already configured this way:

```json
    "experimentalDecorators": true,
    "paths": {
      "@dagger.io/dagger": ["./sdk"]
    }
```

This configuration doesn't require separate installation of the dependency to enable type-hinting or other IDE features.

When working with multiple inter-dependent Dagger modules, it can be challenging to navigate between Dagger Functions defined in different modules. Source-maps offer a solution to this problem, by allowing developers to directly click through or navigate to type declarations in their IDE.

The TypeScript SDK automatically attaches source-maps to the module code. These are a record of the filename, the line number, and the column number that a type declaration corresponds to in the source code. The SDK analyzes the source code, and outputs these (as line comments of the form `./path/to/filename:line`) when creating the type definitions during module initialization.

Most popular IDEs support source-maps, either natively or with an external plugin, such as the [Open file plugin](https://marketplace.visualstudio.com/items?itemName=Fr43nk.seito-openfile) for Visual Studio Code.

### PHP
To get your IDE to recognize a Dagger PHP module, ensure your `composer.json` has a path configured to the generated `dagger/dagger` package. This is located in the `sdk` directory of your module.

For Dagger modules initialized using `dagger init`, the default template is already configured this way:

```json
"repositories": [
  {
    "type": "path",
    "url": "./sdk"
  }
],
"require": {
  "dagger/dagger": "*@dev"
},
```

To get autocompletions and IntelliSense, the setup will vary depending on your IDE.

### PhpStorm
PhpStorm is a very popular IDE for PHP projects. The simplest way for PhpStorm to recognize the `dagger` package is to open the IDE in the directory containing your module's `composer.json` file. You may also open it from any parent directory, such as a project containing the dagger module.

### Visual Studio Code

The simplest way to get autocompletions and IntelliSense working is to install the [Intelephense](https://marketplace.visualstudio.com/items?itemName=bmewburn.vscode-intelephense-client) or the [Phpactor](https://marketplace.visualstudio.com/items?itemName=phpactor.vscode-phpactor) extension.

Then open Visual Studio Code in the directory containing your module's `composer.json` file. You may also open it from any parent directory, such as a project containing the dagger module.

### Emacs
The simplest way to get to get autocompletions and IntelliSense working is to use the built-in LSP client [Eglot](https://github.com/joaotavora/eglot).

A PHP LSP (Language Server Protocol) will need to be installed separately such as [Phpactor](https://github.com/phpactor/phpactor).

You may also want to install [PHP Mode](https://github.com/emacs-php/php-mode) for syntax highlighting and indentation.

Assuming you've installed the above, adding the following snippet to your `init.el` file will automatically enable autocompletion and Intellisense when opening PHP files.

```lisp
(require 'eglot)
(with-eval-after-load 'eglot
  (add-to-list 'eglot-server-programs
               '(php-mode . "phpactor"))
  (add-hook 'php-mode-hook 'eglot-ensure))
```

Then open Emacs in any file within the scope of your module's `composer.json` file.

### Java
When you generate a Dagger Java module, a `pom.xml` file is created in the module directory. For example, for a module name
`your-module`, the example source code is created under `src/main/java/io/dagger/modules/yourmodule/YourModule.java`.

The Dagger dependency code, especially the code generated based on the Dagger engine version and the different module
dependencies you can have, is available under `target/generated-sources`.

This creates a standard Maven project structure, and you can import the project into your favorite IDE as a Maven project.
The IDE will automatically recognize the Dagger dependencies and provide code completion and other features out of the box.

> **Tip:**
> The `target` directory is added to the `.gitignore` file by default, so you don't need to worry about committing the generated code.
> This code is only to help the IDE provide code completion and other features. This code is not required to run the module.
> It can be refreshed by running `dagger develop` again.

Most popular IDEs, like IntelliJ Idea or Visual Studio Code, support Maven projects, and will provide out-of-the-box code completion.

> **Note:**
> The provided `pom.xml` configures the `maven-compiler-plugin` to run the Dagger-specific annotation processor and the
> `maven-shade-plugin` to set the main class for the module and `build-helper-maven-plugin` to add generated Java files
> as sources to help for code completion. These configurations are necessary for the module to function correctly and
> should not be modified (although you can freely update other parts of the `pom.xml` file).
</file>

<file path="docs/dagger.io/index.md">
---
slug: /api
---

# Dagger API

The Dagger API is an unified interface for composing Dagger pipelines. It provides a set of core functions and core types for creating and managing application delivery pipelines, including types for containers, files, directories, network services, secrets, and more. You can call these functions from `dagger`, the Dagger CLI, or from custom functions written in any supported programming language.

## Functions

To create a pipeline with the Dagger API, you call multiple functions, combining them together in sequence to form a pipeline. Here's an example:

### System shell
```shell
dagger <<EOF
container |
  from alpine |
  file /etc/os-release |
  contents
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
container |
  from alpine |
  file /etc/os-release |
  contents
```

### Dagger CLI
```shell
dagger core container \
  from --address=alpine \
  file --path=/etc/os-release \
  contents
```

This example calls multiple functions from the Dagger API in sequence to create a pipeline that builds an image from an `alpine` container and returns the contents of the `/etc/os-release` file to the caller.

## Types

Like regular functions, Dagger functions can accept arguments. You can see this in the previous example: the `from` function accepts a container image address (`--address`) as argument, the file function accepts a filesystem location (`--path`) as argument, and so on.

In addition to supporting basic types (string, boolean, integer, array...), the Dagger API also provides [powerful core types](./types.md) for working with pipelines, such as `Container`, `GitRepository`, `Directory`, `Service`, `Secret`. You can use these core types as arguments to Dagger functions.

Here's an example:

### System shell
```shell
dagger <<EOF
container |
  with-directory /src https://github.com/dagger/dagger |
  directory /src |
  entries
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
container |
  with-directory /src https://github.com/dagger/dagger |
  directory /src |
  entries
```

### Dagger CLI
```shell
dagger core container \
  with-directory --path=/src --directory=https://github.com/dagger/dagger \
  directory --path=/src \
  entries
```

This example creates a scratch container, adds Dagger's GitHub repository to it, and returns the directory contents. In this case, the address of the GitHub repository is passed to the function as a `Directory` argument. The `Directory` loaded in this manner is not merely a string, but it is the actual filesystem state of the target directory, managed by the Dagger Engine and handled in code just like any another variable.

## Chaining

Each of Dagger's core types comes with functions of its own, which can be used to interact with the corresponding object.

When calling a Dagger function that returns a core type, the Dagger API lets you follow up by calling one of that type's functions, which itself can return another type, and so on. This is called "function chaining", and is a core feature of Dagger.

For example, if a Dagger function returns a `Directory`, the caller can continue the chain by calling a function from the `Directory` type to export it to the local filesystem, modify it, mount it into a container, and so on.

Although you may not have realized it, you've already seen function chaining in action. Both the previous examples chain functions together into pipelines. Here is one more example to illustrate the concept:

### System shell
```shell
dagger <<EOF
container |
  from golang:latest |
  with-directory /src https://github.com/dagger/dagger#main |
  with-workdir /src/cmd/dagger |
  with-exec -- go build -o dagger . |
  file ./dagger |
  export ./dagger.bin
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
container |
  from golang:latest |
  with-directory /src https://github.com/dagger/dagger#main |
  with-workdir /src/cmd/dagger |
  with-exec -- go build -o dagger . |
  file ./dagger |
  export ./dagger.bin
```

### Dagger CLI
```shell
dagger core container from --address="golang:latest" \
  with-directory --path="/src" --directory="https://github.com/dagger/dagger#main" \
  with-workdir --path="/src/cmd/dagger" \
  with-exec --args="go","build","-o","dagger","." \
  file --path="./dagger" \
  export --path="./dagger.bin"
```

This example chains multiple function calls into a pipeline that builds the Dagger CLI from source and exports it to the Dagger host:
- `from` returns a `golang` container image as a `Container` type
- `with-directory` adds the Dagger open source repository to the container image filesystem
- `with-workdir` sets the working directory to the Dagger repository
- `with-exec` compiles the Dagger CLI
- `file` returns the built binary as a `File` type
- `export` exports the binary artifact to the Dagger host as `./dagger.bin`

Functions can be chained with the CLI, or programmatically in a [custom Dagger function](./custom-functions.md) using a Dagger SDK. The following are equivalent:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

func (m *MyModule) Publish(ctx context.Context) (string, error) {
	return dag.Container().
		From("alpine:latest").
		WithEntrypoint([]string{"cat", "/etc/os-release"}).
		Publish(ctx, "ttl.sh/my-alpine")
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def publish(self) -> str:
        """Publish the container"""
        return await (
            dag.container()
            .from_("alpine:latest")
            .with_entrypoint(["cat", "/etc/os-release"])
            .publish("ttl.sh/my-alpine")
        )

```

### TypeScript

```typescript
import { dag, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  @func()
  async publish(): Promise<string> {
    return await dag
      .container()
      .from("alpine:latest")
      .withEntrypoint(["cat", "/etc/os-release"])
      .publish("ttl.sh/my-alpine")
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    #[DaggerFunction]
    public function publish(): string
    {
        return dag()
            ->container()
            ->from('alpine:latest')
            ->withEntrypoint(['cat', '/etc/os-release'])
            ->publish('ttl.sh/my-alpine');
    }
}

```

### System shell
```shell
dagger <<EOF
container |
  from alpine:latest |
  with-entrypoint  cat /etc/os-release |
  publish ttl.sh/my-alpine
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
container |
  from alpine:latest |
  with-entrypoint  cat /etc/os-release |
  publish ttl.sh/my-alpine
```

### Dagger CLI
```shell
dagger core container from --address="alpine:latest" \
  with-entrypoint --args="cat","/etc/os-release" \
  publish --address="ttl.sh/my-alpine"
```

As these example illustrate, function chaining is extremely powerful. See [more examples](./chaining.md) of it in action.

## Modules

Dagger [modules](../features/modules.md) are collections of Dagger functions, packaged together for easy sharing and consumption. They are portable and reusable across languages.

## Calling the Dagger API

You can call the Dagger API from code using a custom Dagger function created with a type-safe Dagger SDK, or from the command line using the Dagger CLI or the Dagger Shell.

The different ways to call the Dagger API are:

- From the Dagger CLI
  - [`dagger`, `dagger core` and `dagger call`](./clients-cli.md)
- From a custom application created with a Dagger SDK
  - [`dagger run`](./clients-sdk.md#custom-applications)
- From a [language-native GraphQL client](./clients-http.md#language-native-http-clients)
- From a command-line HTTP or GraphQL client
  - [`curl`](./clients-http.md#command-line-http-clients)
  - [`dagger query`](./clients-http.md#dagger-cli)

> **Note:**
> The Dagger CLI can call any Dagger function, either from your local filesystem or from a remote Git repository. They can be used interactively, from a shell script, or from a CI configuration. Apart from the Dagger CLI, no other infrastructure or tooling is required to call Dagger functions.

## Extending the Dagger API

The Dagger API is extensible and shareable by design. You can extend the API with [custom functions](./custom-functions.md), which are loaded via Dagger [modules](../features/modules.md). You are encouraged to write your own Dagger modules and share them with others.

Dagger also lets you import and reuse modules developed by your team, your organization or the broader Dagger community. The [Daggerverse](https://daggerverse.dev) is a free service run by Dagger, which indexes all publicly available Dagger modules and Dagger functions, and lets you easily search and consume them.

When a Dagger module is loaded, the Dagger API is [dynamically extended](./internals.md#api-extension-with-dagger-functions) with new Dagger functions served by that module. So, after loading a Dagger module, an API client can now call all of the original core functions _plus_ the new Dagger functions provided by that module.
</file>

<file path="docs/dagger.io/interfaces.md">
---
slug: /api/interfaces
---

# Interfaces

> **Important:**
> The information on this page is only applicable to Go and TypeScript SDKs. Interfaces are not currently supported in the Python SDK.

## Declaration

### Go

The Go SDK supports interfaces, which allow you to define Go-style interface
definitions so that your module can accept arbitrary values from other modules
without being tightly coupled to the concrete type underneath.

To use an interface, define a Go interface that embeds `DaggerObject` and use
it in a function argument:

Here is an example of the definition of an interface `Fooer` with a single function `foo`:

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

type Fooer interface {
	dagger.Object
	Foo(ctx context.Context, msg string) (string, error)
}

// Takes a Fooer interface as argument
func (m *MyModule) UseFooer(ctx context.Context, f Fooer) (string, error) {
	return f.Foo(ctx, "hello from module")
}

```

Functions defined in interface definitions must match the client-side API
signature style. If they return a scalar value or an array, they must accept a
`context.Context` argument and return an `error` return value. If they return a
chainable object value, they must not return an `error` value, and they do not need
to include a `context.Context` argument.

Note that you must also provide argument names, since they directly translate
to the GraphQL field argument names.

### TypeScript

The TypeScript SDK supports interfaces, which allow you to define a set of functions
that an object must implement to be considered a valid instance of that interface.
That way, your module can accept arbitrary values from other modules without being
tightly coupled to the concrete type underneath.

To use an interface, use the TypeScript `interface` keyword and use it
as a function argument:

Here is an example of the definition of an interface `Fooer` with a single function `foo`:

```ts
import { func, object } from "@dagger.io/dagger"

export interface Fooer {
  foo(msg: string): Promise<string>
}

@object()
class MyModule {
  /**
   * Takes a Fooer interface as argument
   */
  @func()
  async useFooer(f: Fooer): Promise<string> {
    return await f.foo("hello from module")
  }
}

```

Functions defined in interface definitions must match the client-side API
signature style:
- Always define `async` functions in interfaces (wrap the return type in a `Promise<T>`).
- Declare it as a method signature (e.g., `foo(): Promise<string>`) or a property signature (e.g., `foo: () => Promise<string>`).
- Parameters must be properly named since they directly translate to the GraphQL field argument names.

## Implementation

Here is an example of a module `Example` that implements the `Fooer` interface:

### Go

```go
package main

import (
	"context"
	"fmt"
)

type Example struct{}

func (m *Example) Foo(ctx context.Context, msg string) (string, error) {
	return fmt.Sprintf("foo: %s", msg), nil
}

```

### TypeScript

```ts
import { func, object } from "@dagger.io/dagger"

@object()
class Example {
  @func()
  async foo(msg: string): Promise<string> {
    return `foo: ${msg}`
  }
}

```

## Usage

Any object module that implements the interface method can be passed as an argument to the function
that uses the interface.

Dagger automatically detects if an object coming from the module itself or one of its dependencies implements
an interface defined in the module or its dependencies.
If so, it will add new conversion functions to the object that implement the interface
so it can be passed as argument.

Here is an example of a module that uses the `Example` module defined above and
passes it as argument to the `foo` function of the `MyModule` object:

### Go

```go
package main

import (
	"context"
)

type Usage struct{}

func (m *Usage) CallFooer(ctx context.Context) (string, error) {
	// Call the function that uses the interface, passing the Example object
	// Dagger automatically detects that Example implements Fooer and adds the AsFooer function
	return dag.MyModule().UseFooer(ctx, dag.Example().AsFooer())
}

```

### TypeScript

```ts
import { func, object } from "@dagger.io/dagger"

@object()
class Usage {
  @func()
  async callFooer(): Promise<string> {
    // Call the function that uses the interface, passing the Example object
    // Dagger automatically detects that Example implements Fooer and adds the asFooer function
    return await dag.myModule().useFooer(dag.example().asFooer())
  }
}
</file>

<file path="docs/dagger.io/internals.md">
---
slug: /api/internals
---

# API Internals

If you're interested in how Dagger uses GraphQL, this page provides additional information on the Dagger API's internals.

## Queries as pipelines

Consider the following GraphQL query:

```graphql
query {
  container {
    from(address: "alpine:latest") {
      withExec(args: ["apk", "info"]) {
        stdout
      }
    }
  }
}
```

This query represents a very simple Dagger pipeline. In plain English, it instructs Dagger to "download the latest `alpine` container image, run the command `apk info` in that image, and print the results of the command to the standard output device". It returns a list of the packages installed in the image:

A GraphQL schema works by defining one or more object types, and then fields (and field arguments) on those object types. The fields of an object type can themselves be objects, allowing for different entities to be logically connected with each other. API users can then perform GraphQL queries to return all or some fields of an object.

With Dagger's GraphQL API, while all of the above is true, each field in a query also resolves to a build operation. To understand this, let's dissect the query above:

1. Consider the first level of the previous query:

    ```graphql
    query {
      container {
        from(address: "alpine:latest") {
          ...
        }
      }
    }
    ```

    This query requests the `from` field of Dagger's [`Container`](https://docs.dagger.io/api/reference/#definition-Container) object type, passing it the address of a container image as argument. To resolve this, Dagger will initialize a container from the image published at the given address and return a [`Container`](https://docs.dagger.io/api/reference/#definition-Container) object representing the new container image.

1. Now, consider the next level of the query:

    ```graphql
    query {
      container {
        from(address: "alpine:latest") {
          withExec(args: ["apk", "info"]) {
            ...
          }
        }
      }
    }
    ```

    Here, the query requests the `withExec` field of the [`Container`](https://docs.dagger.io/api/reference/#definition-Container) object returned in the previous step, passing it the command to be executed as an array of arguments. To resolve this, Dagger will  define the command for execution in the container image and return a [`Container`](https://docs.dagger.io/api/reference/#definition-Container) object containing the execution results.

1. Finally, consider the innermost level of the query:

    ```graphql
    query {
      container {
        from(address: "alpine:latest") {
          withExec(args: ["apk", "info"]) {
            stdout
          }
        }
      }
    }
    ```

    Here, the query requests the `stdout` field of the [`Container`](https://docs.dagger.io/api/reference/#definition-Container) object returned in the previous step. To resolve this, Dagger will return a `String` containing the result of the last executed command.

## State representation

In a GraphQL schema, every object exposes an `id` field. This ID serves to uniquely identify and retrieve an object, and is also used by GraphQL's caching mechanism.

In the Dagger GraphQL API too, objects expose an ID but here, the ID represents the object's state at a given time. Objects like [`Container`](https://docs.dagger.io/api/reference/#definition-Container) and [`Directory`](https://docs.dagger.io/api/reference/#definition-Directory) should be thought of as collections of state, which are updated by subsequent field resolutions and whose ID represents their state at the instant of field resolution.

To illustrate this, consider the following query:

```graphql
query {
  host {
    directory(path: ".") {
      id
    }
  }
}
```

The return value of the previous query is an ID representing the state of the current directory on the host

By using object IDs to represesent object state, Dagger's GraphQL API enables some very powerful features. For example, you can save this state and reference it elsewhere (even in a different Dagger Function). You can then continue updating the state from the point you left off, or use it an input to another query.

To make this clearer, consider the following query:

```graphql
query {
  container {
    from(address: "alpine:latest") {
      withExec(args: ["touch", "/tmp/myfile"]) {
        id
      }
    }
  }
}
```

This query instructs Dagger to:

- initialize a container from the `alpine:latest` image (this resolves the `from` field)
- create an empty file at `/tmp/myfile` using the `touch` command (this resolves the `withExec` field)
- return an identifier representing the state of the container filesystem (this resolves the final `id` field)

The output of this query is an identifier representing the state of the container filesystem and Dagger's execution plan.

Now, execute a second query as follows, replacing the placeholder with the contents of the `id` field from the previous query:

```graphql
query {
  container(id: "YOUR-ID-HERE") {
    withExec(args: ["ls", "/tmp"]) {
      stdout
    }
  }
}
```

This second query instructs Dagger to:

- initialize a container using the filesystem state provided in the ID;
- run the `ls` command to list the files in the `/tmp/` directory (this resolves the `withExec` field);
- return the output of the command (this resolves the final `stdout` field).

This second query will return a listing for the `/tmp` directory of the container built by the first query.

As this example demonstrates, Dagger object IDs hold the state of their corresponding object. This state can be transferred from one query to another, or from one Dagger Function to another.

## Lazy evaluation

GraphQL query resolution is triggered only when a leaf value (scalar) is requested. Dagger uses this feature of GraphQL to evaluate pipelines "lazily".

An example will make this clearer. First, navigate to [Webhook.site](https://webhook.site), a free online tool that lets you receive and log incoming HTTP requests. Obtain and copy your unique webhook URL, as shown below:

![Webhook.site URL](/img/current_docs/api/webhook-1.png)

Then, execute the following query, replacing the placeholder with your unique webhook URL:

```graphql
query {
  container {
    from(address: "alpine:latest") {
      withExec(args: ["apk", "add", "curl"]) {
        withExec(args: ["curl", "YOUR-WEBHOOK-URL"]) {
          id
        }
      }
    }
  }
}
```

This query instructs Dagger to:

- initialize a container from the `alpine:latest` image (this resolves the `from` field);
- add the `curl` command-line tool to the image via `apk add` (this resolves the `withExec` field);
- send an HTTP request to your webhook URL using `curl` (this resolves the second `withExec` field);
- return an ID representing the container state (this resolves the final `id` field).

The query returns a base64-encoded block, as explained in the previous section.

However, check the Webhook.site dashboard and you will notice that no HTTP request was sent when this query was executed. The reason is laziness: the query requests only an ID and, since resolving this does not require the commands to be executed, Dagger does not do so. It merely returns the container state and execution plan without actually executing the plan or running the `curl` command.

Now, update and execute the query again as follows:

```graphql
query {
  container {
    from(address: "alpine:latest") {
      withExec(args: ["apk", "add", "curl"]) {
        withExec(args: ["curl", "YOUR-WEBHOOK-URL-HERE"]) {
          stdout
        }
      }
    }
  }
}
```

This time, Dagger both prepares and executes the plan, because that is the only way to resolve the `stdout` field. Check the Webhook.site dashboard and the HTTP request sent by the `curl` command will be visible in the request log, as shown below:

![Webhook.site request](/img/current_docs/api/webhook-2.png)

This example demonstrates the lazy evaluation model in action. The client requests data, and the Dagger GraphQL API returns that data. If executing a command is necessary to return that data, it will do so; if not, it will not.

> **Important:**
> Lazy evaluation is a key advantage of GraphQL. It allows users to write queries (or code, if using an SDK) in a procedural manner, and resolves those queries only when necessary to return data to the user. Dagger leverages this lazy evaluation model to optimize and parallelize query execution for maximum speed and performance.

## Dynamic API extension

```mermaid
graph LR;

subgraph host["Client Host"]
    cli["Dagger CLI"]
end

subgraph runner["Dagger Engine"]
    direction LR
    gql["GraphQL Server\n(per-session)"]

    subgraph core["Core"]
        ctr["Container"]
        dir["Directory"]
        file["File"]
        etc["(etc.)"]
    end

    subgraph ModA["Module A"]
        direction LR
        FnA1["func Build(someArg string) *Container"]
        FnA2["func Test() error"]
        FnA3["func Deploy(someArg bool, someOtherArg *Secret) (string, error)"]
    end

    subgraph ModB["Module B"]
        direction LR
        FnB1["def foo(some_arg: int) -> dag.Container:"]
        FnB2["def bar(some_dir: dag.Directory) -> str:"]
    end
end

gql <-..-> core

gql  <-..-> ModA

gql  <-..-> ModB

cli <-..-> gql
```

1. You execute a Dagger CLI command like `dagger call` against a Dagger module. The CLI either connects to an existing engine or provisions one on-the-fly. Once connected, it opens a new session with the Dagger Engine.

   - Each session is associated with its own GraphQL server instance running inside the Dagger Engine. This GraphQL server initially only has the core Dagger API available, which provides basic functionality like running containers, interacting with files and directories, etc.
   - The core API is highly optimized: each request is turned into a [Directed Acyclic Graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) of low-level operations required to compute the result. It uses caching and other optimizations to compute these results as efficiently as possible.

1. The core API also provides functionality for loading Dagger modules. When a module is loaded into the session, the GraphQL API is dynamically extended with new APIs served by that module. So, after loading a module, the CLI client can now call all of the original core APIs _plus_ the new APIs provided by that module.

   - Dagger modules are just source code that is configured to be loaded with a Dagger SDK. When the module is loaded, the source code is pulled into the Dagger Engine (if not already cached) and interfaced with the session via the SDK so that its APIs are parsed and prepared for execution. Once loaded, if an API provided by the module is called, the module will be executed inside a container in the Dagger Engine to obtain the result.
   - Dagger modules are themselves also Dagger clients connected back to the same session they were loaded into. They can call core APIs in addition to other modules on which they have declared a dependency.

1. The Dagger CLI command you executed loads the specified Dagger module and calls the requested API served by that module. It then uses the returned result in the most appropriate way depending on the CLI command being used (print a textual representation, download an asset, open an interactive shell, proxy network ports, etc.).
</file>

<file path="docs/dagger.io/llm.md">
---
slug: /api/llm
---

# LLM Integration

Dagger's `LLM` core type includes API methods to attach objects to a Large Language Model (LLM), send prompts, and receive responses.

## Prompts

Use the `LLM.withPrompt()` API method to append prompts to the LLM context:

### System shell
```shell
dagger <<EOF
llm |
  with-prompt "What tools do you have available?"
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
llm |
  with-prompt "What tools do you have available?"
```

For longer or more complex prompts, use the `LLM.withPromptFile()` API method to read the prompt from a text file:

### System shell
```shell
dagger <<EOF
llm |
  with-prompt-file ./prompt.txt
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
llm |
  with-prompt-file ./prompt.txt
```


## Responses and Variables

Use the `LLM.lastReply()` API method to obtain the last reply from the LLM

Dagger supports the use of variables in prompts. This allows you to interpolate results of other operations into an LLM prompt:

### System shell
```shell
dagger <<EOF
source=\$(container |
  from alpine |
  with-directory /src https://github.com/dagger/dagger |
  directory /src)
environment=\$(env |
  with-directory-input 'source' \$source 'a directory with source code')
llm |
  with-env \$environment |
  with-prompt "The directory also has some tools available." |
  with-prompt "Use the tools in the directory to read the first paragraph of the README.md file in the directory." |
  with-prompt "Reply with only the selected text." |
  last-reply
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
source=$(container |
  from alpine |
  with-directory /src https://github.com/dagger/dagger |
  directory /src)
environment=$(env |
  with-directory-input 'source' $source 'a directory with source code')
llm |
  with-env $environment |
  with-prompt "The directory also has some tools available." |
  with-prompt "Use the tools in the directory to read the first paragraph of the README.md file in the directory." |
  with-prompt "Reply with only the selected text." |
  last-reply
```

> **Tip:**
> To get the complete message history, use the `LLM.History()` API method.

## Environments

Dagger [modules](../features/modules.md) are collections of Dagger Functions. When you give a Dagger module to the `LLM` core type, every Dagger Function is turned into a tool that the LLM can call.

Environments configure any number of inputs and outputs for the LLM. For example, an environment might provide a `Directory`, a `Container`, a custom module, and a `string` variable. The LLM can use the scalars and the functions of these objects to complete the assigned task.

The documentation for the modules are provided to the LLM, so make sure to provide helpful documentation in your Dagger Functions. The LLM should be able to figure out how to use the tools on its own. Don't worry about describing the objects too much in your prompts because it will be redundant with this automatic documentation.

Consider the following Dagger Function:

### Go
```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type CodingAgent struct{}

// Write code to a file
func (m *CodingAgent) WriteCode(ctx context.Context, prompt string) (*dagger.Directory, error) {
	// Create a new directory for the toy workspace
	workspace := dag.Directory()

	// Create an environment with the toy workspace module
	env := dag.Env().
		WithInput("workspace", workspace, "A directory with source code").
		WithOutput("workspace", "The directory with the updated source code")

	// Create an LLM with the environment
	llm := dag.Llm().WithEnv(env)

	// Add prompts to the LLM
	llm = llm.
		WithPrompt("You are a helpful coding assistant.").
		WithPrompt("Use the tools available to you to write code that fulfills the user's request.").
		WithPrompt(prompt)

	// Get the last reply from the LLM
	reply := llm.LastReply()

	// Get the workspace directory from the reply
	return reply.Output("workspace").AsDirectory(), nil
}

```

### Python
```python
import dagger
from dagger import dag, function, object_type


@object_type
class CodingAgent:
    @function
    async def write_code(self, prompt: str) -> dagger.Directory:
        """Write code to a file"""
        # Create a new directory for the toy workspace
        workspace = dag.directory()

        # Create an environment with the toy workspace module
        env = (
            dag.env()
            .with_input("workspace", workspace, "A directory with source code")
            .with_output("workspace", "The directory with the updated source code")
        )

        # Create an LLM with the environment
        llm = dag.llm().with_env(env)

        # Add prompts to the LLM
        llm = (
            llm.with_prompt("You are a helpful coding assistant.")
            .with_prompt(
                "Use the tools available to you to write code that fulfills the user's request."
            )
            .with_prompt(prompt)
        )

        # Get the last reply from the LLM
        reply = llm.last_reply()

        # Get the workspace directory from the reply
        return reply.output("workspace").as_directory()

```

### TypeScript
```typescript
import { dag, Directory, func, object } from "@dagger.io/dagger"

@object()
class CodingAgent {
  /**
   * Write code to a file
   */
  @func()
  async writeCode(prompt: string): Promise<Directory> {
    // Create a new directory for the toy workspace
    const workspace = dag.directory()

    // Create an environment with the toy workspace module
    const env = dag
      .env()
      .withInput("workspace", workspace, "A directory with source code")
      .withOutput("workspace", "The directory with the updated source code")

    // Create an LLM with the environment
    let llm = dag.llm().withEnv(env)

    // Add prompts to the LLM
    llm = llm
      .withPrompt("You are a helpful coding assistant.")
      .withPrompt(
        "Use the tools available to you to write code that fulfills the user's request.",
      )
      .withPrompt(prompt)

    // Get the last reply from the LLM
    const reply = llm.lastReply()

    // Get the workspace directory from the reply
    return reply.output("workspace").asDirectory()
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Directory;

use function Dagger\dag;

#[DaggerObject]
class CodingAgent
{
    /**
     * Write code to a file
     */
    #[DaggerFunction]
    public function writeCode(string $prompt): Directory
    {
        // Create a new directory for the toy workspace
        $workspace = dag()->directory();

        // Create an environment with the toy workspace module
        $env = dag()
            ->env()
            ->withInput('workspace', $workspace, 'A directory with source code')
            ->withOutput('workspace', 'The directory with the updated source code');

        // Create an LLM with the environment
        $llm = dag()->llm()->withEnv($env);

        // Add prompts to the LLM
        $llm = $llm
            ->withPrompt('You are a helpful coding assistant.')
            ->withPrompt('Use the tools available to you to write code that fulfills the user\'s request.')
            ->withPrompt($prompt);

        // Get the last reply from the LLM
        $reply = $llm->lastReply();

        // Get the workspace directory from the reply
        return $reply->output('workspace')->asDirectory();
    }
}

```

### Java
```java
package io.dagger.modules.codingagent;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Directory;
import io.dagger.client.Env;
import io.dagger.client.Llm;
import io.dagger.client.LlmReply;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

@Module
@Object
public class CodingAgent {

  /**
   * Write code to a file
   */
  @Function
  public Directory writeCode(String prompt) throws Exception {
    try (Client client = Dagger.connect()) {
      // Create a new directory for the toy workspace
      Directory workspace = client.directory();

      // Create an environment with the toy workspace module
      Env env = client
          .env()
          .withInput("workspace", workspace, "A directory with source code")
          .withOutput("workspace", "The directory with the updated source code");

      // Create an LLM with the environment
      Llm llm = client.llm().withEnv(env);

      // Add prompts to the LLM
      llm = llm
          .withPrompt("You are a helpful coding assistant.")
          .withPrompt(
              "Use the tools available to you to write code that fulfills the user's request.")
          .withPrompt(prompt);

      // Get the last reply from the LLM
      LlmReply reply = llm.lastReply();

      // Get the workspace directory from the reply
      return reply.output("workspace").asDirectory();
    }
  }
}

```

Here, an instance of the `ToyWorkspace` module is attached as an input to the `Env` environment. The `ToyWorkspace` module contains a number of Dagger Functions for developing code: `Read()`, `Write()`, and `Build()`. When this environment is attached to an `LLM`, the LLM can call any of these Dagger Functions to change the state of the `ToyWorkspace` and complete the assigned task.

In the `Env`, a `ToyWorkspace` instance called `after` is specified as a desired output of the LLM. This means that the LLM should return the `ToyWorkspace` module instance as a result of completing its task. The resulting `ToyWorkspace` object is then available for further processing or for use in other Dagger Functions.
</file>

<file path="docs/dagger.io/module-dependencies.md">
---
slug: /api/module-dependencies
---

# Module Dependencies

## Installation

You can call Dagger Functions from any other Dagger module in your own Dagger module simply by adding it as a module dependency with `dagger install`, as in the following example:

```shell
dagger install github.com/shykes/daggerverse/hello@v0.3.0
```

This module will be added to your `dagger.json`:

```json
...
"dependencies": [
  {
    "name": "hello",
    "source": "github.com/shykes/daggerverse/hello@54d86c6002d954167796e41886a47c47d95a626d"
  }
]
```

When you add a dependency to your module with `dagger install`, the dependent module will be added to the code-generation routines and can be accessed from your own module's code.

The entrypoint to accessing dependent modules from your own module's code is `dag`, the Dagger client, which is pre-initialized. It contains all the core types (like `Container`, `Directory`, etc.), as well as bindings to any dependencies your module has declared.

Here is an example of accessing the installed `hello` module from your own module's code:

### Go

```go
func (m *MyModule) Greeting(ctx context.Context) (string, error) {
  return dag.Hello().Hello(ctx)
}
```

### Python

```python
@function
async def greeting(self) -> str:
  return await dag.hello().hello()
```

### TypeScript

```typescript
@func()
async greeting(): Promise<string> {
  return await dag.hello().hello()
}
```

### PHP

```php
#[DaggerFunction]
public function greeting(): string
{
    return dag()->hello()->hello();
}
```

### Java

```java
@Function
public String greeting() throws ExecutionException, DaggerQueryException, InterruptedException {
    return dag().hello().hello();
}
```

Here is a more complex example. It is a Dagger Function that utilizes a module from the Daggerverse to build a Go project, then chains a Dagger API method to open an interactive terminal session in the build directory.

First, install the module:

```shell
dagger install github.com/kpenfound/dagger-modules/golang@v0.2.0
```

Next, create a new Dagger Function:

### Go
```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Build a Go project and open a terminal in the build directory
func (m *MyModule) Example(
	ctx context.Context,
	// Source directory
	buildSrc *dagger.Directory,
	// Build arguments
	buildArgs []string,
) *dagger.Container {
	// Call the Golang module's build function
	buildDir := dag.Golang().Build(buildSrc, buildArgs)

	// Open a terminal in the build directory
	return buildDir.Terminal(ctx)
}

```

### Python
```python
from typing import List

import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def example(
        self,
        build_src: dagger.Directory,
        build_args: List[str],
    ) -> dagger.Container:
        """Build a Go project and open a terminal in the build directory"""
        # Call the Golang module's build function
        build_dir = dag.golang().build(build_src, build_args)

        # Open a terminal in the build directory
        return await build_dir.terminal()

```

### TypeScript
```typescript
import { dag, Directory, Container, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Build a Go project and open a terminal in the build directory
   */
  @func()
  async example(
    buildSrc: Directory,
    buildArgs: string[],
  ): Promise<Container> {
    // Call the Golang module's build function
    const buildDir = dag.golang().build(buildSrc, buildArgs)

    // Open a terminal in the build directory
    return await buildDir.terminal()
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Container;
use Dagger\Client\Directory;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Build a Go project and open a terminal in the build directory
     *
     * @param Directory $buildSrc Source directory
     * @param string[] $buildArgs Build arguments
     */
    #[DaggerFunction]
    public function example(
        Directory $buildSrc,
        array $buildArgs
    ): Container {
        // Call the Golang module's build function
        $buildDir = dag()->golang()->build($buildSrc, $buildArgs);

        // Open a terminal in the build directory
        return $buildDir->terminal();
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.client.Directory;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Build a Go project and open a terminal in the build directory
   */
  @Function
  public Container example(
      @Description("Source directory") Directory buildSrc,
      @Description("Build arguments") List<String> buildArgs) throws Exception {
    try (Client client = Dagger.connect()) {
      // Call the Golang module's build function
      Directory buildDir = client.golang().build(buildSrc, buildArgs);

      // Open a terminal in the build directory
      return buildDir.terminal();
    }
  }
}

```

This Dagger Function accepts two arguments - the source directory and a list of build arguments - and does the following:

- It invokes the Golang module via the `dag` Dagger client.
- It calls a Dagger Function from the module to build the source code and return a just-in-time directory with the compiled binary.
- It chains a core Dagger API method to open an interactive terminal session in the returned directory.

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'example https://github.com/golang/example#master:/hello .'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
example https://github.com/golang/example#master:/hello .
```

### Dagger CLI
```shell
dagger call example --build-src=https://github.com/golang/example#master:/hello --build-args=.
```

You can also use local modules as dependencies. However, they must be stored in a sub-directory of your module. For example:

```shell
dagger install ./path/to/module
```

> **Note:**
> Installing a module using a local path (relative or absolute) is only possible if your module is within the repository root (for Git repositories) or the directory containing the `dagger.json` file (for all other cases).

## Uninstallation

To remove a dependency from your Dagger module, use the `dagger uninstall` command. The `dagger uninstall` command can be passed either a remote repository reference or a local module name.

The commands below are equivalent:

```shell
dagger uninstall hello
dagger uninstall github.com/shykes/daggerverse/hello
```

## Update

To update a dependency in your Dagger module to the latest version (or the version specified), use the `dagger update` command. The target module must be local.

The commands below are equivalent:

```shell
dagger update hello
dagger update github.com/shykes/daggerverse/hello
```

> **Note:**
> Given a dependency like `github.com/path/name@branch/tag`:
> - `dagger update github.com/path/name` updates the dependency to the latest commit of the branch/tag.
> - `dagger update github.com/path/name@version` updates the dependency to the latest commit for the `version` branch/tag.
</file>

<file path="docs/dagger.io/module-structure.md">
---
slug: /api/module-structure
---

# Module Structure

<!-- Content from ../partials/_dagger_module_init.mdx would be inserted here if available -->
<!-- Assuming removal as per confirmed rules -->

## File layout

### Multiple files

### Go

You can split your Dagger module into multiple files, not just `main.go`. To do this, you can just create another file beside `main.go` (for example, `utils.go`):

```
.
‚îÇ‚îÄ‚îÄ ...
‚îÇ‚îÄ‚îÄ main.go
‚îÇ‚îÄ‚îÄ utils.go
‚îî‚îÄ‚îÄ dagger.json
```

This file should be inside the same package as `main.go`, and as such, can access any private variables/functions/types inside the package.

Additionally, you can also split your Dagger module into Go subpackages (for example, `utils`):

```
.
‚îÇ‚îÄ‚îÄ ...
‚îÇ‚îÄ‚îÄ main.go
|‚îÄ‚îÄ utils
‚îÇ   ‚îî‚îÄ‚îÄ utils.go
‚îî‚îÄ‚îÄ dagger.json
```

Because this is a separate package, you can only use the variables/functions/types that are exported from this package in `main.go` (you can't access types from `main.go` in the `utils` package).

> **Note:**
> Only types and functions in the top-level package are part of the public-facing API for the module.

You can access other Dagger types from a sub-package by importing the generated sub-package `dagger/<module>/internal/dagger`:

```go
// utils/utils.go

import "dagger/<module>/internal/dagger"

func DoThing(client *dagger.Client) *dagger.Directory {
    // we need to pass *dagger.Client in here, since we don't have access to `dag`
	...
}
```

### Python
The Dagger module's code in Python can be split into multiple files by making a [package](https://docs.python.org/3/tutorial/modules.html#packages) and ensuring the *main object* is imported in `__init__.py`. All the other object types should already be imported from there.

For example given this directory structure:

```
.
‚îú‚îÄ‚îÄ dagger.json
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ my_module
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lint.py
```

The `__init__.py` file should import the main object from `main.py`:

```python
# src/my_module/__init__.py
"""My very own Dagger module"""
from .main import MyModule as MyModule
```

And the `main.py` file should import the other objects from their respective files:

```python
# src/my_module/main.py
import dagger

from .test import Test  # in src/my_module/test.py
from .lint import Lint  # in src/my_module/lint.py

@dagger.object_type
class MyModule:
    @dagger.function
    def test(self) -> Test:
        return Test()

    @dagger.function
    def lint(self) -> Lint:
        return Lint()
```

> **Important:**
> Dagger expects that a Python Dagger module is structured like a [library](https://docs.astral.sh/uv/concepts/projects/#libraries), so that the SDK is able to load the code with an import, but it's up to the Python [build system](https://docs.astral.sh/uv/concepts/projects/#build-systems) to know where files are located in order to build and install the Python package correctly.
>
> This affords a lot of flexibility in how Dagger Python modules can be structured, and which tools are supported.

The default project template follows known conventions for structuring a Python library (src layout, package name matching project name), which allows Python [build backends](https://www.python.org/dev/peps/pep-0517/) to automatically recognize where the files are.

However, it's possible to change the project's name and file structure with a bit of extra configuration, as long as the build backend correctly builds the code in a way that allows the SDK to import it after installation (i.e., must be installed in `site-packages`).

> **Note:**
> Build backends are independent from installers (or *build frontends*, see [PEP 517](https://www.python.org/dev/peps/pep-0517/)), even though some installers may provide both (as separate packages). For example, [hatch](https://pypi.org/project/hatch/) vs [hatchling](https://pypi.org/project/hatchling/), [poetry](https://pypi.org/project/poetry/) vs [poetry-core](https://pypi.org/project/poetry-core/).

> **Tip:**
> `dagger init` won't override existing `pyproject.toml` and `.py` files, so it's possible to use an external process to generate a different template, before calling `dagger init`.

> **Tip:** SINGLE FILE MODULE
> Here is an example of moving all the code into a single `main.py` [module](https://docs.python.org/3/tutorial/modules.html), resulting in the following structure:
>
> ```
> .
> ‚îú‚îÄ‚îÄ dagger.json
> ‚îú‚îÄ‚îÄ main.py
> ‚îú‚îÄ‚îÄ pyproject.toml
> ‚îî‚îÄ‚îÄ uv.lock
> ```
>
> And corresponding `pyproject.toml` configuration:
>
> ### hatchling
>
> ```toml
> [build-system]
> requires = ["hatchling>=0.15.0"]
> build-backend = "hatchling.build"
>
> [tool.hatch.build.targets.wheel]
> packages = ["main.py"]
> ```
>
> ### poetry-core
>
> ```toml
> [build-system]
> requires = ["poetry-core>=1.0.0"]
> build-backend = "poetry.core.masonry.api"
> ```
>
> ### setuptools
>
> ```toml
> [build-system]
> requires = ["setuptools", "wheel"]
> build-backend = "setuptools.build_meta"
>
> [tool.setuptools]
> py-modules = ["main"]
> ```

> **Tip:** CUSTOMIZE IMPORT PACKAGE
> The Python SDK looks for the *main object* of the Dagger module in the Python [import package that is named after the distribution package name](https://packaging.python.org/en/latest/discussions/distribution-package-vs-import-package/#how-do-distribution-package-names-and-import-package-names-compare) (in particular, using underscores `_` as a word separator).
>
> If they are different, you must explicitly tell the Python SDK where the main object needs to be imported from, using the following entry point configuration in `pyproject.toml`:
>
> ```toml
> [project.entry-points."dagger.mod"]
> main_object = "<import package>:<main object>"
> ```
>
> For example, for a Dagger module named `my-module`:
>
> - Main object: `MyModule` (required to be the name in `dagger.json`, in PascalCase)
> - Default distribution package: `my-module` (in `pyproject.toml`; can be changed)
> - Default import package: `src/my_module` (normalized after distribution package name; can be changed)
>
> Then, the default `main_object` entry point that the Python SDK looks for is `my_module:MyModule`, with a fallback to `main:MyModule` for backwards compatibility.
>
> Thus, if you have the following configuration:
>
> ```toml
> [project.entry-points."dagger.mod"]
> main_object = "my_module.main:MyModule"
> ```
>
> Then the import in `__init__.py` is no longer needed since Dagger knows to import from `my_module.main` directly.

### TypeScript

Due to TypeScript limitations, it is not possible to split your main class module (`index.ts`) into multiple files. However, it is possible to create sub-classes in different files and access them from your main class module:

```typescript
// src/index.ts
import { func, object } from "@dagger.io/dagger"

import { Test } from "./test" // in src/test.ts
import { Lint } from "./lint" // in src/lint.ts

@object()
class MyModule {
  @func()
  test(): Test {
    return new Test()
  }

  @func()
  lint(): Lint {
    return new Lint()
  }
}
```

### PHP

Only functions from your main class (`MyModule.php`) can _initially_ be called by Dagger. However, it is possible to create other classes and access them from your main class:

```php
// src/MyModule.php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\{DaggerObject, DaggerFunction};
use DaggerModule\Test; // in src/Test.php
use DaggerModule\Lint; // in src/Lint.php

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    #[DaggerFunction]
    public function test(): Test
    {
        return new Test();
    }

    #[DaggerFunction]
    public function lint(): Lint
    {
        return new Lint();
    }
}
```

### Java
The Dagger module's code in Java can be split into multiple classes, in multiple files. A few constraints apply:

- The main Dagger object must be represented by a class using the same name as the module, in PascalCase. For instance if the module name is `my-module`, the main object's class must be named `MyModule`.
- The exposed objects must be annotated with `@Object` and the exposed functions with `@Function`.

> **Note:**
> The package in which the main object exists is not important. By convention, the name is based on the module name, but any name may be used.

The description of the module can be set in a `package-info.java` file, with the `@Module` annotation:

```java
/** My Dagger module */
@Module
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
```

> **Note:**
> By default, the description will be read from the JavaDoc documentation of the package, class or function. To define a different
> description, use the `description` field in each `@Module`, `@Object` or `@Function` annotation.
>
> ```java
> /**
>  * Returns the build container
>  *
>  * @param name The name of the container
>  * @return The container
>  */
> @Function(description = "Build container")
> public Container build(String name) {
>     //...
> }
> ```
>
> ```console
> $ dagger functions
>
> Name    Description
> build   Build container
> ```

### Monorepos

A monorepo typically contains multiple independent projects, each of which has different test, build and deployment requirements. Managing these requirements in a single CI workflow or YAML file can be incredibly complex and time-consuming.

Dagger modules provide a framework that you can use to break up this complexity and cleanly separate CI responsibilities in a monorepo without losing reusability or performance. There are two possible patterns you can follow:

1. One top-level Dagger module per project, with sub-modules for the various CI workflows in that project. This pattern is suitable when there are significant differences between the projects in the monorepo (e.g. a monorepo with SDKs, CLIs, web applications, docs, all of which have different CI requirements).

```mermaid
graph TD
    A[Top-level orchestrator module] --> B[Webapp frontend module]
    A[Top-level orchestrator module] --> C[Webapp backend module]
    A[Top-level orchestrator module] --> D[Utilities module]
    A[Top-level orchestrator module] --> E[Docs module]
```

Benefits of this pattern include:

- Easier debugging: Sub-modules provide a way to separate, and therefore easily debug, the business logic for different CI tasks.
- Code reuse: There may be opportunities for sub-modules in different projects to import each other to reuse existing functionality.
- Improved performance: The top-level module of a project can orchestrate the sub-modules using the language‚Äôs native concurrency features.

2. A single, shared CI / automation module which all projects use and contribute to. This pattern is suitable when there are significant commonalities between the projects in the monorepo (e.g. a monorepo with only micro-services or only front-end applications).

```mermaid
graph TD
    A[Shared module] --> B[Microservice 1]
    A[Shared module] --> C[Microservice 2]
    A[Shared module] --> D[Microservice 3]
```

Benefits of this pattern include:

- Code reuse: This reduces code duplication and ensures a consistent CI environment for all projects. For example, the shared module could create a common build environment and leverage this for multiple projects in the monorepo.
- Reduced onboarding friction: There is no need to create a new CI module when adding a new project or component. New projects can get started faster with their CI implementation.
- Best practices: All projects benefit from the best practices implemented in the shared module.
- Knowledge sharing: By contributing to a shared CI module, project teams can learn from each other's CI strategies.

## Runtime container

Dagger modules run in a runtime container that's bootstrapped by the Dagger Engine, with the necessary environment to run the Dagger module's code.

### Go

The runtime container is currently hardcoded to run in Go 1.21  (although this may be configurable in future).

### Python

The runtime container is based on the [python:3.12-slim](https://hub.docker.com/_/python/tags?name=3.12-slim) base image by default, but it can be overridden by setting [`requires-python`](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#python-requires), or pinned with a `.python-version` file next to your `pyproject.toml`:

```shell
echo "3.11" > .python-version
```

This will instruct Dagger to use the `python:3.11-slim` base image instead.

Pinning the interpreter version can be useful to prevent an automatic upgrade from a future version of Dagger, or to select a newer version.

> **Tip:**
>
> For more advanced needs, a different base image can be used by adding the following to your `pyproject.toml`:
>
> ```toml
> [tool.dagger]
> base-image = "acme/python:3.11"
> ```
>
> This can be useful to add a few requirements to the module's execution environment such as system packages like `git`, or to add necessary environment variables, for example. However, don't deviate from the default base image too much or it may break in a future version of Dagger.
>
> ‚ö†Ô∏è **Override at own risk!**

### TypeScript

The runtime container is currently hardcoded to run in Node.js 22.11.0, but it can be overridden by [setting an alternative base image](../configuration/modules.md#alternative-base-images).

[Bun](https://bun.sh/) is experimentally supported and [work is in progress](https://github.com/dagger/dagger/issues/4368) to support [Deno](https://deno.com/).

The TypeScript SDK is installed automatically, including dependencies, with a version that's tied to the currently running Dagger Engine container:

```shell
# executed by the runtime container
yarn install
npm pkg set "dependencies[@dagger.io/dagger]=./sdk"
```

The SDK files are mounted under `/sdk` in the Dagger Engine runtime container.

This is why the initial `package.json` doesn't include any dependencies except a local link to the `sdk` generated directory.

```json
{
  "dependencies": {
    "typescript": "^5.3.2"
    "@dagger.io/dagger": "./sdk"
  }
}
```

[^1]:

### PHP

The runtime container is currently hardcoded to run in [php:8.3-cli-alpine](https://hub.docker.com/_/php/tags?name=8.3-cli-alpine) (although this may be configurable in future).

### Java

Two containers are used by the runtime, one to build the module into a JAR file, one to run it.

They are currently hardcoded to run in [maven:3.9.9-eclipse-temurin-17](https://hub.docker.com/_/maven/tags?name=3.9.9-eclipse-temurin-17) and [eclipse-temurin:23-jre-noble](https://hub.docker.com/_/eclipse-temurin/tags?name=23-jre-noble) respectively (although this may be configurable in future).

## Language-native packaging

The structure of a Dagger module mimics that of each language's conventional packaging mechanisms and tools.

### Go

Dagger modules written for use with the Go SDK are automatically created as [Go modules](https://go.dev/ref/mod). At module creation time, a `go.mod` and `go.sum` file will automatically be created  that import the necessary dependencies. Dependencies can be installed and managed just as for any standard Go environment.

After using new dependencies in your code, update your `go.mod`/`go.sum` with the newly imported dependencies by using [`go mod tidy`](https://go.dev/ref/mod#go-mod-tidy).

#### Go workspaces

Since it's common to have a sub-directory inside your main project containing your Dagger module code, you can manage your modules using [Go workspaces](https://go.dev/ref/mod#workspaces).

When a new Dagger module is created, Dagger attempts to add it to a root `go.work` if it exists. If not, it can be added manually later with `go work use ./path/to/mymodule`.

```go
// go.work
go 1.21.7

use (
	./path/to/mymodule
)
```

#### Go vendor

[Go vendor](https://go.dev/ref/mod#go-mod-vendor) directories are not currently supported. See [https://github.com/dagger/dagger/issues/6076](https://github.com/dagger/dagger/issues/6076) for more information.

### Python

Dagger modules in Python are built to be installed, like libraries. At module creation time, a `pyproject.toml` and `uv.lock` file will automatically be created that depend on the locally generated client library (in `./sdk`). This dependency is configured to be editable so that changes in the code don't require a re-install.

With the `uv.lock` file, Dagger uses uv's [project management](https://docs.astral.sh/uv/concepts/projects/) capabilites, but it can be opted out by removing this file, in which case Dagger falls back to [the pip interface](https://docs.astral.sh/uv/pip/) instead.

> **Info:**
> Dagger also supports pinning dependencies with a `pip-tools` compatible `requirements.lock` file in order to support the use of other project managers like Poetry or Hatch locally (when developing).
>
> In this case, Dagger installs dependencies with:
>
> ```shell
> # executed by the runtime
> uv pip install -r requirements.lock -e ./sdk -e .
> ```
>
> Notice that the `./sdk` and `.` packages don't need to be in the `requirements.lock` file, only the third party dependencies.
>
> This means module developers can use any tool they want to manage their virtual environment and install dependencies, but if third-party dependencies aren't pinned in a `requirements.lock` file, the developers may get different versions between the Dagger execution environment and their own local environment.
>
> For example, Poetry has its own `poetry.lock` which Dagger doesn't recognize, but it can be exported as a `requirements.lock` file with:
>
> ```shell
> poetry export --without main -o requirements.lock
> ```
>
> It will have to be manually kept in sync, though.

### TypeScript
Dagger modules in Typescript are built to be installed, like libraries. The runtime container installs the module code with:

```shell
# executed by the runtime container
yarn install --production
```

This means that so long as the project has a `package.json` file, it can be used as a library and it can be managed using any Node.js package manager such as [`npm`](https://www.npmjs.com/), [`pnpm`](https://pnpm.io/) or [`yarn`](https://yarnpkg.com/).

Only production dependencies are installed, not packages defined in the `devDependencies` field.

### PHP
Dagger modules in PHP are built to be installed, like libraries. The runtime container installs the module code with:

```shell
# executed by the runtime container
composer install
```

This means that so long as the project has a `composer.json` file, it can be used as a library.

### Java
Dagger modules in Java are built as JAR files, using Maven. The runtime container builds the module code with:

```shell
# executed by the runtime build container
mvn clean package
```

The generated JAR file is then executed by the runtime container with:

```shell
# executed by the runtime run container
java -jar module.jar
```

> **Important:**
> This means you have to keep the `pom.xml` file and especially the `maven-compiler-plugin` and the `maven-shade-plugin` configurations.

> **Note:**
> Other build tools like Gradle might be supported in the future.
</file>

<file path="docs/dagger.io/module-tests.md">
---
slug: /api/module-tests
---

# Module Tests

Like any other piece of software, Dagger Functions and modules should be thoroughly tested. This section documents proven patterns and best practices for effectively testing reusable modules.

The following examples rely on a module called `greeter` that provides a function to greet a person:


### Go

```go
package main

import (
	"fmt"
)

type Greeter struct{}

// Returns a greeting message
func (m *Greeter) Hello(name string) string {
	return fmt.Sprintf("Hello, %s!", name)
}

```

### Python

```python
import dagger
from dagger import function, object_type


@object_type
class Greeter:
    @function
    def hello(self, name: str) -> str:
        """Returns a greeting message"""
        return f"Hello, {name}!"

```

### TypeScript

```typescript
import { func, object } from "@dagger.io/dagger"

@object()
export class Greeter {
  /**
   * Returns a greeting message
   */
  @func()
  hello(name: string): string {
    return `Hello, ${name}!`
  }
}

```

## Test module

Well-written tests often provide the best documentation for your software, and this holds true for Dagger modules as well. It's considered a best practice to keep your tests close to the module's code so they can serve as both verification and reference. Additionally, tests that rely on the module's public API act as functional examples, clearly illustrating how to use the module.

Following these principles leads to writing tests for your Dagger modules using Dagger modules themselves. In practice, this means creating a test module in the same directory as your main module and writing your tests as Dagger Functions, as shown below:

### Go

```bash
mkdir tests
cd tests
dagger init --name tests --sdk go --source .
```

Then add the following to `main.go`:

```go
func (m *Tests) Hello(ctx context.Context) error {
	greeting, err := dag.Greeter().Hello(ctx, "World")
	if err != nil {
		return err
	}

	if greeting != "Hello, World!" {
		return errors.New("unexpected greeting")
	}

	return nil
}
```

### Python

```bash
mkdir tests
cd tests
dagger init --name tests --sdk python --source .
```

Then add the following to `src/tests/main.py`:

```python
@object_type
class Tests:
    @function
    async def hello(self):
        greeting = await dag.greeter().hello("World")

        if greeting != "Hello, World!":
            raise Exception("unexpected greeting")
```

### TypeScript

```bash
mkdir tests
cd tests
dagger init --name tests --sdk typescript --source .
```

Then add the following to `src/index.ts`:

```typescript
@object()
export class Tests {
  @func()
  hello(): Promise<void> {
    return dag
      .greeter()
      .hello("World")
      .then((value: string) => {
        if (value != "Hello, World!") {
          throw new Error("unexpected greeting");
        }

        return;
      });
  }
}
```

> **Tip:**
> `tests` is a logical name to use for the test module, but this is not mandatory. Some people call it `dev` to indicate it contains other, development related functions, not just tests.

## Testable examples

In the Daggerverse, [example modules](https://docs.dagger.io/api/daggerverse#examples) are special modules designed to showcase your own modules, offering better demonstrations than the automatically generated ones.

You can combine example modules with the test module pattern to turn those examples into executable tests. Often, this approach provides enough coverage to eliminate the need for a separate test module.

### Go

```bash
mkdir -p examples/go
cd examples/go
dagger init --name examples/go --sdk go --source .
```

Then add the following to `main.go`:

```go
func (m *Examples) GreeterHello(ctx context.Context) error {
	greeting, err := dag.Greeter().Hello(ctx, "World")
	if err != nil {
		return err
	}

	// Do something with the greeting
	_ = greeting

	return nil
}
```

### Python

```bash
mkdir -p examples/python
cd examples/python
dagger init --name examples/python --sdk python --source .
```

Then add the following to `src/examples/main.py`:

```python
@object_type
class Examples:
    @function
    async def greeter_hello(self):
        greeting = await dag.greeter().hello("World")

       	# Do something with the greeting
```

### TypeScript

```bash
mkdir -p examples/typescript
cd examples/typescript
dagger init --name examples/typescript --sdk typescript --source .
```

Then add the following to `src/index.ts`:

```typescript
@object()
export class Examples {
  @func()
  greeterHello(): Promise<void> {
    return dag
      .greeter()
      .hello("World")
      .then((_: string) => {
        // Do something with the greeting

        return;
      });
  }
}
```

If you require more in-depth testing, you can still create a dedicated test module as demonstrated earlier.

> **Tip:**
> Make sure to check out the documentation o [example function naming](https://docs.dagger.io/api/daggerverse#examples).

## Test function signature

Since test functions are ordinary Dagger functions, you can return any value that's allowed. While this approach works fine when running a test with `dagger call`, there are scenarios where a single return value isn't sufficient. For example, you might need to handle multiple output objects, wait for asynchronous operations, manage errors, or (in some languages) provide additional context.

Another challenge arises when you have multiple test functions with parameters, as you must remember to call each test function with the correct arguments.

In such cases, it can be helpful to standardize your test function signature. Consider generating inputs from within the function, synchronizing any asynchronous tasks there, and returning a single value or an error. This approach keeps your tests consistent and easier to maintain.

### Go

```go
func (m *Tests) YourTest(ctx context.Context) error {
	// Your test here

	if false { // Your error condition here
		return errors.New("test failed")
	}

	return nil
}
```

### Python

```python
@object_type
class Tests:
    @function
    async def your_test(self):
        # Your test here

        if false: # Your error condition here
            raise Exception("test failed")
```

### TypeScript

```typescript
@object()
export class Tests {
  @func()
  hello(): Promise<void> {
    return dag
      .yourModule()
      .yourFunction()
      .then(() => {
        if (false) { // Your error condition here
          throw new Error("test failed");
        }

        return;
      });
  }
}
```

In some situations, you may need to provide specific values to your test modules, such as when authenticating against an external service. In these cases, you can rely on [module constructors](./constructors.md) to inject any required inputs.

## "All" function pattern

Regardless of whether you employ the test or the example module pattern, you probably want the ability to run all tests at once (for example, in CI or just to verify everything works locally), while reserving the capability to run individual tests for debugging purposes.

This is where the `all` function comes into the picture. It's basically a single function that executes all your tests or examples.

Depending on the SDK you use, this may be as simple as calling each test function after the other:

### Go

```go
func (m *Tests) All(ctx context.Context) error {
	var err error

	err = m.FirstTest(ctx)
	if err != nil {
		return err
	}

	err = m.SecondTest(ctx)
	if err != nil {
		return err
	}

	return nil
}
```

### Python

```python
@object_type
class Tests:
    @function
    async def all(self):
        await self.hello()
        await self.custom_greeting()
```

### TypeScript

```typescript
@object()
export class Tests {
  @func()
  async all(): Promise<void> {
    await this.hello();
    await this.customGreeting();
  }
}
```

Alternatively, if the SDK/language you use supports this, you can run tests in parallel:

### Go

```go
import "github.com/sourcegraph/conc/pool"

type Tests struct{}

func (m *Tests) All(ctx context.Context) error {
	p := pool.New().WithErrors().WithContext(ctx)

	p.Go(m.Hello)
	p.Go(m.CustomGreeting)

	return p.Wait()
}
```

### Python

```python
import anyio

@object_type
class Tests:
    @function
    async def all(self):
        async with anyio.create_task_group() as tg:
            tg.start_soon(self.first_test)
            tg.start_soon(self.second_test)
```

### TypeScript

```typescript
@object()
export class Tests {
  @func()
  async all(): Promise<void> {
    await Promise.all([this.firstTest(), this.secondTest()]);
  }
}
```

You can now run all tests for the module using `dagger call -m tests all`.

> **Tip:**
> Adopting a standard test function signature greatly simplifies both kinds of `all` functions.
</file>

<file path="docs/dagger.io/packages.md">
---
slug: /api/packages
---

# Third-Party Packages

Dagger Functions are just regular code, written in your usual programming language. One of the key advantages of this approach is that it opens up access to your language's existing ecosystem of packages or modules. You can easily import these packages/modules in your Dagger module via your language's package manager.

### Go

To add a Go module, add it to your `go.mod` file using `go get`. For example:

```shell
go get github.com/spf13/cobra
```

### Python

To add a Python package, add it to your `pyproject.toml` file using your chosen package manager. For example:

#### uv

```sh
uv add requests
```

#### poetry

```sh
poetry add requests
```

#### uv pip

Add the dependency manually to [`pyproject.toml`](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#dependencies-and-requirements):

```toml
[project]
dependencies = [
    "requirements>=2.32.3",
]
```

Then install into your virtual environment:

```sh
uv pip install -e ./sdk -e .
```

> **Note:**
> There's no need to activate the virtual environment before `uv pip install`, but it does need to exist.

#### pip

Add the dependency manually to [`pyproject.toml`](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#dependencies-and-requirements):

```toml
[project]
dependencies = [
    "requirements>=2.32.3",
]
```

Then install into your virtual environment:

```sh
python -m pip install -e ./sdk -e .
```

> **Tip:**
> If you haven't setup your local environment yet, see [IDE Integration](./ide-integration.md).

> **Note:**
> Third-party dependencies are managed in the same way as any normal Python project. The only limitation is in "pinning" the dependencies. Currently, Dagger can install directly from a `uv.lock` file, or a [pip-tools compatible](https://docs.astral.sh/uv/pip/compile/#locking-requirements) `requirements.lock` file (notice `.lock` extension, not `.txt`). See [Language-native packaging](./custom-functions.md#language-native-packaging) for more information.

### TypeScript

To add a TypeScript package, add it to the `package.json` file using your favorite package manager. For example:

```shell
npm install pm2
```

Pinning a specific dependency version or adding local dependencies are supported, in the same way as any Node.js project.

### PHP

To add a PHP package, add it to the `composer.json` file, the same way as any PHP project. For example:

```shell
composer require phpunit/phpunit
```

> **Note:**
> Dagger modules installed as packages via Composer are not registered with Dagger.
>
> You can access its code, like any other PHP package, but this is not the indended use-case of a Dagger module.
> This may lead to unexpected behaviour.
>
> Use Composer for standard third-party packages.
>
> Use Dagger to [install Dagger modules](./module-dependencies.md)

### Java

To add a Java package, add it to your `pom.xml` file using Maven. For example:

```xml
<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-simple</artifactId>
    <scope>runtime</scope>
    <version>2.0.16</version>
</dependency>
</file>

<file path="docs/dagger.io/playground.md">
---
slug: /api/playground
---

# API Playground

The API Playground was an in-browser tool for testing, running and sharing Dagger API queries. It has since been decommissioned.

The recommended approach is to use the [`dagger query`](./clients-http.md#dagger-cli) sub-command, which provides an easy way to send raw GraphQL queries to the Dagger API from the command line.
</file>

<file path="docs/dagger.io/remote-modules.md">
---
slug: /api/remote-modules
---

# Using Modules from Remote Repositories

Dagger supports the use of HTTP and SSH protocols for accessing remote repositories as Dagger [modules](../features/modules.md), compatible with all major Git hosting platforms such as GitHub, GitLab, BitBucket, Azure DevOps, Codeberg, and Sourcehut. Dagger supports authentication via both HTTPS (using Git credential managers) and SSH (using a unified authentication approach).

Dagger supports various reference schemes for Dagger modules, as below:

| Protocol | Scheme            | Authentication | Example |
|----------|-------------------|----------------|---------|
| HTTP(S)  | Go-like ref style | Git credential manager | `github.com/username/repo[/subdir][@version]`  |
| HTTP(S)  | Git HTTP style    | Git credential manager | `https://github.com/username/repo.git[/subdir][@version]` |
| SSH      | SCP-like          | SSH keys | `git@github.com:username/repo.git[/subdir][@version]`     |
| SSH      | Explicit SSH      | SSH keys | `ssh://git@github.com/username/repo.git[/subdir][@version]` |

Dagger provides additional flexibility in referencing modules through the following options:

- The `.git` extension is optional for HTTP refs or explicit SSH refs, except for [GitLab, when referencing modules stored on a private repo or private subgroup](https://gitlab.com/gitlab-org/gitlab-foss/-/blob/master/lib/gitlab/middleware/go.rb#L229-237).
- Monorepo support: Append `/subpath` to access a specific subdirectory within the repository.
- Version specification: Add `@version` to target a particular version of the module. This can be a tag, branch name, or full commit hash. If omitted, the default branch is used.

Here is an example of using a Go builder Dagger module from a public repository over HTTPS:

```shell
dagger -m github.com/kpenfound/dagger-modules/golang@v0.2.0 call \
  build --source=https://github.com/dagger/dagger --args=./cmd/dagger \
  export --path=./build
```

Here is the same example using SSH authentication. Note that this requires [SSH authentication to be properly configured](#configuring-ssh-authentication) on your Dagger host).

```shell
dagger -m git@github.com:kpenfound/dagger-modules/golang@v0.2.0 call \
  build --source=https://github.com/dagger/dagger --args=./cmd/dagger \
  export --path=./build
```

## Authentication methods

Dagger supports both HTTPS and SSH authentication for accessing remote repositories.

### HTTPS authentication

For HTTPS authentication, Dagger uses your system's configured Git credential manager. This means if you're already authenticated with your Git provider, Dagger will automatically use these credentials when needed.

The following credential helpers are supported:
- [Git Credential Manager](https://github.com/git-ecosystem/git-credential-manager)
- macOS Keychain
- Windows Credential Manager
- Custom credential helpers configured in your `.gitconfig`

To verify if your credentials are properly configured, try cloning a private repository (replace the placeholders below with valid values):

```shell
git clone https://github.com/USER/PRIVATE_REPOSITORY.git
```

If this works, Dagger will be able to use the same credentials to access your private repositories.

#### Credential manager configuration

- GitHub: Use [`gh auth login`](https://docs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git#github-cli) or [configure credentials via Git Credential Manager](https://docs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git#git-credential-manager)
- GitLab: Use [`glab auth login`](https://gitlab.com/gitlab-org/cli/-/blob/main/docs/source/auth/login.md) or [configure credentials via Git Credential Manager](https://github.com/git-ecosystem/git-credential-manager)
- Azure DevOps: Use [Git Credential Manager](https://learn.microsoft.com/en-us/azure/devops/repos/git/set-up-credential-managers)
- BitBucket: Configure credentials using the [Git credential system](https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage) (the widely-adopted implementation is [Git Credential Manager](https://github.com/git-ecosystem/git-credential-manager))

### SSH authentication

Dagger mounts the socket specified by your host's `SSH_AUTH_SOCK` environment variable to the Dagger Engine. This is essential for SSH refs, as most Git servers use your SSH key for authentication and tracking purposes, even when cloning public repositories.

This means that you must ensure that the `SSH_AUTH_SOCK` environment variable is properly set in your environment when using SSH refs with Dagger.

[Read detailed instructions on setting up SSH authentication](https://docs.github.com/en/authentication/connecting-to-github-with-ssh), including how to generate SSH keys, start the SSH agent, and add your keys.

## Best practices

For quick and easy referencing:
- Copy the repository ref from your preferred Git server's UI.
- To specify a particular version or commit, append `#version` (for directory arguments) or `@version` (for modules).
- To target a specific directory within the repository, use the format `#version:subpath` (for directory arguments) or add a `/subpath` (for modules). Remember that the version is mandatory when specifying a subpath.
- For private repositories:
  - HTTPS: Ensure your Git credentials are properly configured using your provider's recommended method.
  - SSH: Make sure your SSH keys are properly set up and added to the SSH agent.


## Known limitations and workarounds

This section outlines current limitations and provides workarounds for common issues. We're actively working on improvements for these areas.

### Windows is not supported

Currently, SSH refs are fully supported on UNIX-based systems (Linux and macOS). Windows support is under development. Track progress and contribute to the discussion in our [GitHub issue for Windows support](https://github.com/dagger/dagger/issues/8313).

### Multiple SSH keys may cause SSH forwarding to fail

SSH forwarding may fail when multiple keys are loaded in your SSH agent. This is under active investigation in our [GitHub issue](https://github.com/dagger/dagger/issues/8288). Until this is resolved, the following workaround may be used:

1. Clear all loaded keys: `ssh-add -D`
2. Add back only the required key: `ssh-add /path/to/key`
</file>

<file path="docs/dagger.io/return-values.md">
---
slug: /api/return-values
---

# Return Values

In addition to returning basic types (string, boolean, ...), Dagger Functions can also return any of Dagger's core types, such as `Directory`, `Container`, `Service`, `Secret`, and many more.

This opens powerful applications to Dagger Functions. For example, a Dagger Function that builds binaries could take a directory with the source code as argument and return another directory (a "just-in-time" directory) containing just binaries or a container image (a "just-in-time" container) with the binaries included.

> **Note:**
> If a function doesn't have a return type annotation, it'll be translated to the [dagger.Void][void-type] type in the API.
>
> [void-type]: https://docs.dagger.io/api/reference/#definition-Void

## String return values

Here is an example of a Dagger Function that returns operating system information for the container as a string:

### Go
```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns OS information
func (m *MyModule) OsInfo(ctx context.Context, ctr *dagger.Container) (string, error) {
	return ctr.WithExec([]string{"uname", "-a"}).Stdout(ctx)
}

```

### Python
```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def os_info(self, ctr: dagger.Container) -> str:
        """Returns OS information"""
        return await ctr.with_exec(["uname", "-a"]).stdout()

```

### TypeScript
```typescript
import { dag, Container, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns OS information
   */
  @func()
  async osInfo(ctr: Container): Promise<string> {
    return await ctr.withExec(["uname", "-a"]).stdout()
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Container;

#[DaggerObject]
class MyModule
{
    /**
     * Returns OS information
     */
    #[DaggerFunction]
    public function osInfo(Container $ctr): string
    {
        return $ctr->withExec(['uname', '-a'])->stdout();
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Container;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns OS information
   */
  @Function
  public String osInfo(Container ctr) throws Exception {
    return ctr.withExec(List.of("uname", "-a")).stdout().get();
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'os-info ubuntu:latest'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
os-info ubuntu:latest
```

### Dagger CLI
```shell
dagger call os-info --ctr=ubuntu:latest
```

The result will look like this:

```shell
Linux dagger 6.1.0-22-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21) x86_64 x86_64 x86_64 GNU/Linux
```

## Integer return values

Here is an example of a Dagger Function that returns the sum of two integers:

### Go
```go
package main

type MyModule struct{}

// Returns the sum of two integers
func (m *MyModule) AddInteger(a, b int) int {
	return a + b
}

```

### Python
```python
import dagger
from dagger import function, object_type


@object_type
class MyModule:
    @function
    def add_integer(self, a: int, b: int) -> int:
        """Returns the sum of two integers"""
        return a + b

```

### TypeScript
```typescript
import { func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns the sum of two integers
   */
  @func()
  addInteger(a: number, b: number): number {
    return a + b
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

#[DaggerObject]
class MyModule
{
    /**
     * Returns the sum of two integers
     */
    #[DaggerFunction]
    public function addInteger(int $a, int $b): int
    {
        return $a + $b;
    }
}

```

### Java
> **Note:**
> You can either use the primitive `int` type or the boxed `java.lang.Integer` type.

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

@Module
@Object
public class MyModule {

  /**
   * Returns the sum of two integers
   */
  @Function
  public int addInteger(int a, int b) {
    return a + b;
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'add-integer 1 2'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
add-integer 1 2
```

### Dagger CLI
```shell
dagger call add-integer --a=1 --b=2
```

The result will look like this:

```shell
3
```

## Floating-point number return values

Here is an example of a Dagger Function that returns the sum of two floating-point numbers:

### Go
```go
package main

type MyModule struct{}

// Returns the sum of two floats
func (m *MyModule) AddFloat(a, b float64) float64 {
	return a + b
}

```

### Python
```python
import dagger
from dagger import function, object_type


@object_type
class MyModule:
    @function
    def add_float(self, a: float, b: float) -> float:
        """Returns the sum of two floats"""
        return a + b

```

### TypeScript

> **Note:**
> There's no `float` type keyword in TypeScript because the type keyword `number` already supports floating point numbers.
>
> To declare a `float` return type on the function signature, import `float` from `@dagger.io/dagger` and use it as return type.
> The imported `float` type is a `number` underneath, so you can return it as you would return a regular type `number`.

```typescript
import { func, object, float } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns the sum of two floats
   */
  @func()
  addFloat(a: float, b: float): float {
    return a + b
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;

#[DaggerObject]
class MyModule
{
    /**
     * Returns the sum of two floats
     */
    #[DaggerFunction]
    public function addFloat(float $a, float $b): float
    {
        return $a + $b;
    }
}

```

### Java
> **Note:**
> You can either use the primitive `float` type or the boxed `java.lang.Float` type.

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

@Module
@Object
public class MyModule {

  /**
   * Returns the sum of two floats
   */
  @Function
  public float addFloat(float a, float b) {
    return a + b;
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'add-float 1.4 2.7'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
add-float 1.4 2.7
```

### Dagger CLI
```shell
dagger call add-float --a=1.4 --b=2.7
```

The result will look like this:

```shell
4.1
```

## Directory return values

Directory return values might be produced by a Dagger Function that:

- Builds language-specific binaries
- Downloads source code from git or another remote source
- Processes source code (for example, generating documentation or linting code)
- Downloads or processes datasets
- Downloads machine learning models

Here is an example of a Go builder Dagger Function that accepts a remote Git address as a `Directory` argument, builds a Go binary from the source code in that repository, and returns the build directory containing the compiled binary:

### Go
```go
package main

import (
	"fmt"

	"dagger.io/dagger"
)

type MyModule struct{}

// Build the application and return the build directory
func (m *MyModule) GoBuilder(
	// Source directory
	src *dagger.Directory,
	// Architecture to build for
	arch string,
	// OS to build for
	os string,
) *dagger.Directory {
	return dag.Container().
		From("golang:latest").
		WithEnvVariable("GOOS", os).
		WithEnvVariable("GOARCH", arch).
		WithDirectory("/src", src).
		WithWorkdir("/src").
		WithExec([]string{"go", "build", "-o", fmt.Sprintf("build/%s-%s", os, arch)}).
		Directory("build")
}

```

### Python
```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def go_builder(
        self,
        src: dagger.Directory,
        arch: str,
        os: str,
    ) -> dagger.Directory:
        """Build the application and return the build directory"""
        return await (
            dag.container()
            .from_("golang:latest")
            .with_env_variable("GOOS", os)
            .with_env_variable("GOARCH", arch)
            .with_directory("/src", src)
            .with_workdir("/src")
            .with_exec(["go", "build", "-o", f"build/{os}-{arch}"])
            .directory("build")
        )

```

### TypeScript
```typescript
import { dag, Directory, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Build the application and return the build directory
   */
  @func()
  goBuilder(src: Directory, arch: string, os: string): Directory {
    return dag
      .container()
      .from("golang:latest")
      .withEnvVariable("GOOS", os)
      .withEnvVariable("GOARCH", arch)
      .withDirectory("/src", src)
      .withWorkdir("/src")
      .withExec(["go", "build", "-o", `build/${os}-${arch}`])
      .directory("build")
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Directory;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Build the application and return the build directory
     */
    #[DaggerFunction]
    public function goBuilder(
        Directory $src,
        string $arch,
        string $os
    ): Directory {
        return dag()
            ->container()
            ->from('golang:latest')
            ->withEnvVariable('GOOS', $os)
            ->withEnvVariable('GOARCH', $arch)
            ->withDirectory('/src', $src)
            ->withWorkdir('/src')
            ->withExec(['go', 'build', '-o', "build/{$os}-{$arch}"])
            ->directory('build');
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Directory;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Build the application and return the build directory
   */
  @Function
  public Directory goBuilder(
      @Description("Source directory") Directory src,
      @Description("Architecture to build for") String arch,
      @Description("OS to build for") String os) throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("golang:latest")
          .withEnvVariable("GOOS", os)
          .withEnvVariable("GOARCH", arch)
          .withDirectory("/src", src)
          .withWorkdir("/src")
          .withExec(List.of("go", "build", "-o", String.format("build/%s-%s", os, arch)))
          .directory("build");
    }
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'go-builder https://github.com/golang/example#master:/hello amd64 linux'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
go-builder https://github.com/golang/example#master:/hello amd64 linux
```

### Dagger CLI
```shell
dagger call go-builder --src=https://github.com/golang/example#master:/hello --arch=amd64 --os=linux
```

Once the command completes, you should see this output:

```shell
_type: Directory
entries:
    - hello
```

This means that the build succeeded, and a `Directory` type representing the build directory was returned. This `Directory` is called a "just-in-time" directory: a dynamically-produced artifact of a Dagger pipeline.

## File return values

Similar to just-in-time directories, Dagger Functions can produce just-in-time files by returning the `File` type.

Just-in-time files might be produced by a Dagger Function that:

- Builds language-specific binaries
- Combines multiple input files into a single output file, such as a composite video or a compressed archive

Here is an example of a Dagger Function that accepts a filesystem path or remote Git address as a `Directory` argument and  returns a ZIP archive of that directory:

### Go
```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a ZIP archive of the directory
func (m *MyModule) Archiver(ctx context.Context, src *dagger.Directory) *dagger.File {
	return dag.Container().
		From("alpine:latest").
		WithExec([]string{"apk", "add", "zip"}).
		WithDirectory("/src", src).
		WithWorkdir("/src").
		WithExec([]string{"zip", "-r", "out.zip", "."}).
		File("out.zip")
}

```

### Python
```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def archiver(self, src: dagger.Directory) -> dagger.File:
        """Returns a ZIP archive of the directory"""
        return await (
            dag.container()
            .from_("alpine:latest")
            .with_exec(["apk", "add", "zip"])
            .with_directory("/src", src)
            .with_workdir("/src")
            .with_exec(["zip", "-r", "out.zip", "."])
            .file("out.zip")
        )

```

### TypeScript
```typescript
import { dag, Directory, File, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a ZIP archive of the directory
   */
  @func()
  archiver(src: Directory): File {
    return dag
      .container()
      .from("alpine:latest")
      .withExec(["apk", "add", "zip"])
      .withDirectory("/src", src)
      .withWorkdir("/src")
      .withExec(["zip", "-r", "out.zip", "."])
      .file("out.zip")
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Directory;
use Dagger\Client\File;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a ZIP archive of the directory
     */
    #[DaggerFunction]
    public function archiver(Directory $src): File
    {
        return dag()
            ->container()
            ->from('alpine:latest')
            ->withExec(['apk', 'add', 'zip'])
            ->withDirectory('/src', $src)
            ->withWorkdir('/src')
            ->withExec(['zip', '-r', 'out.zip', '.'])
            ->file('out.zip');
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Directory;
import io.dagger.client.File;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a ZIP archive of the directory
   */
  @Function
  public File archiver(@Description("Source directory") Directory src) throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("alpine:latest")
          .withExec(List.of("apk", "add", "zip"))
          .withDirectory("/src", src)
          .withWorkdir("/src")
          .withExec(List.of("zip", "-r", "out.zip", "."))
          .file("out.zip");
    }
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'archiver https://github.com/dagger/dagger#main:./docs/current_docs/quickstart'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
archiver https://github.com/dagger/dagger#main:./docs/current_docs/quickstart
```

### Dagger CLI
```shell
dagger call archiver --src=https://github.com/dagger/dagger#main:./docs/current_docs/quickstart
```

Once the command completes, you should see this output:

```shell
_type: File
name: out.zip
size: 13744
```

This means that the build succeeded, and a `File` type representing the ZIP archive was returned.

## Container return values

Similar to directories and files, just-in-time containers are produced by calling a Dagger Function that returns the `Container` type. This type provides a complete API for building, running and distributing containers.

Just-in-time containers might be produced by a Dagger Function that:

- Builds a container
- Minifies a container
- Downloads a container image from a running registry
- Exports a container from Docker or other container runtimes
- Snapshots the state of a running container

You can think of a just-in-time container, and the `Container` type that represents it, as a build stage in Dockerfile. Each operation produces a new immutable state, which can be further processed, or exported as an OCI image. Dagger Functions can accept, return and pass containers between themselves, just like regular variables.

Here's an example of a Dagger Function that returns a base `alpine` container image with a list of additional specified packages:

### Go
```go
package main

import (
	"dagger.io/dagger"
)

type MyModule struct{}

// Returns an Alpine container with the specified packages installed
func (m *MyModule) AlpineBuilder(
	// Packages to install
	packages []string,
) *dagger.Container {
	return dag.Container().
		From("alpine:latest").
		WithExec(append([]string{"apk", "add"}, packages...))
}

```

### Python
```python
from typing import List

import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def alpine_builder(self, packages: List[str]) -> dagger.Container:
        """Returns an Alpine container with the specified packages installed"""
        return await (
            dag.container()
            .from_("alpine:latest")
            .with_exec(["apk", "add", *packages])
        )

```

### TypeScript
```typescript
import { dag, Container, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns an Alpine container with the specified packages installed
   */
  @func()
  alpineBuilder(packages: string[]): Container {
    return dag
      .container()
      .from("alpine:latest")
      .withExec(["apk", "add", ...packages])
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Container;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns an Alpine container with the specified packages installed
     *
     * @param string[] $packages Packages to install
     */
    #[DaggerFunction]
    public function alpineBuilder(array $packages): Container
    {
        return dag()
            ->container()
            ->from('alpine:latest')
            ->withExec(['apk', 'add', ...$packages]);
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

import java.util.ArrayList;
import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns an Alpine container with the specified packages installed
   */
  @Function
  public Container alpineBuilder(@Description("Packages to install") List<String> packages) throws Exception {
    try (Client client = Dagger.connect()) {
      List<String> args = new ArrayList<>();
      args.add("apk");
      args.add("add");
      args.addAll(packages);
      return client
          .container()
          .from("alpine:latest")
          .withExec(args);
    }
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'alpine-builder curl,openssh'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
alpine-builder curl,openssh
```

### Dagger CLI
```shell
dagger call alpine-builder --packages=curl,openssh
```

Once the command completes, you should see this output:

```shell
_type: Container
defaultArgs:
    - /bin/sh
entrypoint: []
mounts: []
platform: linux/amd64
user: ""
workdir: ""
```

This means that the build succeeded, and a `Container` type representing the built container image was returned.

> **Note:**
> When calling Dagger Functions that produce a just-in-time artifact, you can use the Dagger CLI to add more functions to the pipeline for further processing - for example, inspecting the contents of a directory artifact, exporting a file artifact to the local filesystem, publishing a container artifact to a registry, and so on. This is called ["function chaining"](./index.md#chaining), and it is one of Dagger's most powerful features.

## Chaining

So long as a Dagger Function returns an object that can be JSON-serialized, its state will be preserved and passed to the next function in the chain. This makes it possible to write custom Dagger Functions that support function chaining in the same style as the Dagger API.

Here is an example module with support for function chaining:

### Go
```go
package main

import (
	"fmt"
)

type MyModule struct {
	Greeting string
	Name     string
}

func New(
	// +optional
	// +default="Hello"
	greeting string,
	// +optional
	// +default="World"
	name string,
) *MyModule {
	return &MyModule{
		Greeting: greeting,
		Name:     name,
	}
}

// Return the greeting message
func (m *MyModule) Message() string {
	return fmt.Sprintf("%s, %s!", m.Greeting, m.Name)
}

// Update the greeting message
func (m *MyModule) WithGreeting(greeting string) *MyModule {
	m.Greeting = greeting
	return m
}

// Update the name
func (m *MyModule) WithName(name string) *MyModule {
	m.Name = name
	return m
}

```

### Python
```python
from typing import Annotated

import dagger
from dagger import Doc, field, function, object_type


@object_type
class MyModule:
    greeting: Annotated[str, Doc("The greeting to use")] = field(default="Hello")
    name: Annotated[str, Doc("Who to greet")] = field(default="World")

    @function
    def message(self) -> str:
        """Return the greeting message"""
        return f"{self.greeting}, {self.name}!"

    @function
    def with_greeting(self, greeting: str) -> "MyModule":
        """Update the greeting message"""
        self.greeting = greeting
        return self

    @function
    def with_name(self, name: str) -> "MyModule":
        """Update the name"""
        self.name = name
        return self

```

### TypeScript
```typescript
import { field, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * The greeting to use
   *
   * @default "Hello"
   */
  @field()
  greeting = "Hello"

  /**
   * Who to greet
   *
   * @default "World"
   */
  @field()
  name = "World"

  /**
   * Return the greeting message
   */
  @func()
  message(): string {
    return `${this.greeting}, ${this.name}!`
  }

  /**
   * Update the greeting message
   */
  @func()
  withGreeting(greeting: string): this {
    this.greeting = greeting
    return this
  }

  /**
   * Update the name
   */
  @func()
  withName(name: string): this {
    this.name = name
    return this
  }
}

```

### PHP
```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Attribute\DaggerField;

#[DaggerObject]
class MyModule
{
    /**
     * The greeting to use
     */
    #[DaggerField(default: 'Hello')]
    public string $greeting;

    /**
     * Who to greet
     */
    #[DaggerField(default: 'World')]
    public string $name;

    /**
     * Return the greeting message
     */
    #[DaggerFunction]
    public function message(): string
    {
        return sprintf('%s, %s!', $this->greeting, $this->name);
    }

    /**
     * Update the greeting message
     */
    #[DaggerFunction]
    public function withGreeting(string $greeting): self
    {
        $this->greeting = $greeting;
        return $this;
    }

    /**
     * Update the name
     */
    #[DaggerFunction]
    public function withName(string $name): self
    {
        $this->name = $name;
        return $this;
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;
import io.dagger.module.annotation.Default;

@Module
@Object
public class MyModule {

  @Description("The greeting to use")
  @Default("\"Hello\"")
  public String greeting;

  @Description("Who to greet")
  @Default("\"World\"")
  public String name;

  public MyModule() {}

  public MyModule(String greeting, String name) {
    this.greeting = greeting;
    this.name = name;
  }

  /**
   * Return the greeting message
   */
  @Function
  public String message() {
    return String.format("%s, %s!", this.greeting, this.name);
  }

  /**
   * Update the greeting message
   */
  @Function
  public MyModule withGreeting(String greeting) {
    this.greeting = greeting;
    return this;
  }

  /**
   * Update the name
   */
  @Function
  public MyModule withName(String name) {
    this.name = name;
    return this;
  }
}

```

And here is an example call for this module:

### System shell
```shell
dagger -c 'with-name Monde | with-greeting Bonjour | message'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
with-name Monde | with-greeting Bonjour | message
```

### Dagger CLI
```shell
dagger call with-name --name=Monde with-greeting --greeting=Bonjour message
```

The result will be:

```shell
Bonjour, Monde!
</file>

<file path="docs/dagger.io/secrets.md">
---
slug: /api/secrets
---

# Secrets

Dagger has first-class support for "secrets", such as passwords, API keys, SSH keys and so on. These secrets can be securely used in Dagger functions without exposing them in plaintext logs, writing them into the filesystem of containers you're building, or inserting them into the cache.

Here is an example, which uses a secret in a Dagger function chain:

```shell
export API_TOKEN="guessme"
```

### System shell
```shell
dagger <<'EOF'
container |
  from alpine:latest |
  with-secret-variable MY_SECRET env://API_TOKEN |
  with-exec -- sh -c 'echo this is the secret: $MY_SECRET' |
  stdout
EOF
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
container |
  from alpine:latest |
  with-secret-variable MY_SECRET env://API_TOKEN |
  with-exec -- sh -c 'echo this is the secret: $MY_SECRET' |
  stdout
```

### Dagger CLI
```shell
dagger core container \
  from --address=alpine:latest \
  with-secret-variable --name="MY_SECRET" --secret="env://API_TOKEN" \
  with-exec --args="sh","-c",'echo this is the secret: $MY_SECRET' \
  stdout
```

### Go
```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Use a secret in a container
func (m *MyModule) UseSecret(ctx context.Context, secret *dagger.Secret) (string, error) {
	return dag.Container().
		From("alpine:latest").
		WithSecretVariable("MY_SECRET", secret).
		WithExec([]string{"sh", "-c", "echo this is the secret: $MY_SECRET"}).
		Stdout(ctx)
}

```

### Python
```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def use_secret(self, secret: dagger.Secret) -> str:
        """Use a secret in a container"""
        return await (
            dag.container()
            .from_("alpine:latest")
            .with_secret_variable("MY_SECRET", secret)
            .with_exec(["sh", "-c", "echo this is the secret: $MY_SECRET"])
            .stdout()
        )

```

### TypeScript
```typescript
import { dag, Secret, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Use a secret in a container
   */
  @func()
  async useSecret(secret: Secret): Promise<string> {
    return await dag
      .container()
      .from("alpine:latest")
      .withSecretVariable("MY_SECRET", secret)
      .withExec(["sh", "-c", "echo this is the secret: $MY_SECRET"])
      .stdout()
  }
}

```

[Secret arguments](./arguments.md#secret-arguments) can be sourced from multiple providers: the host environment, the host filesystem, the result of host command execution, and external secret managers [1Password](https://1password.com/) and [Vault](https://www.hashicorp.com/products/vault).

## Security considerations

- Dagger automatically scrubs secrets from its various logs and output streams. This ensures that sensitive data does not leak - for example, in the event of a crash.
- Secret plaintext should be handled securely within your Dagger pipeline. For example, you should not write secret plaintext to a file, as it could then be stored in the Dagger cache.
</file>

<file path="docs/dagger.io/services.md">
---
slug: /api/services
title: "Services"
---

# Services

Dagger Functions support service containers, enabling users to spin up additional long-running services (as containers) and communicate with those services from Dagger Functions.

This makes it possible to:
- Instantiate and return services from a Dagger Function, and then:
  - Use those services in other Dagger Functions (container-to-container networking)
  - Use those services from the calling host (container-to-host networking)
- Expose host services for use in a Dagger Function (host-to-container networking).

Some common scenarios for using services with Dagger Functions are:

- Running a database service for local storage or testing
- Running end-to-end integration tests against a service
- Running sidecar services

## Service containers

Services instantiated by a Dagger Function run in service containers, which have the following characteristics:

- Each service container has a canonical, content-addressed hostname and an optional set of exposed ports.
- Service containers are started just-in-time, de-duplicated, and stopped when no longer needed.
- Service containers are health checked prior to running clients.

## Bind services in functions

A Dagger Function can create and return a service, which can then be used from another Dagger Function or from the calling host. Services in Dagger Functions are returned using the `Service` core type.

Here is an example of a Dagger Function that returns an HTTP service. This service is used by another Dagger Function, which creates a service binding using the alias `www` and then accesses the HTTP service using this alias.

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns an HTTP service
func (m *MyModule) HttpService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("nginx:1.25-alpine").
		WithNewFile("/usr/share/nginx/html/index.html", dagger.ContainerWithNewFileOpts{
			Contents: "Hello, world!",
		}).
		WithExposedPort(80).
		AsService()
}

// Accesses the HTTP service
func (m *MyModule) Get(ctx context.Context) (string, error) {
	// bind HTTP service to container
	// access HTTP service using service binding
	// return response
	return dag.Container().
		From("alpine:latest").
		WithExec([]string{"apk", "add", "curl"}).
		WithServiceBinding("www", m.HttpService(ctx)).
		WithExec([]string{"curl", "http://www:80"}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def http_service(self) -> dagger.Service:
        """Returns an HTTP service"""
        return (
            dag.container()
            .from_("nginx:1.25-alpine")
            .with_new_file(
                "/usr/share/nginx/html/index.html", contents="Hello, world!"
            )
            .with_exposed_port(80)
            .as_service()
        )

    @function
    async def get(self) -> str:
        """Accesses the HTTP service"""
        # bind HTTP service to container
        # access HTTP service using service binding
        # return response
        return await (
            dag.container()
            .from_("alpine:latest")
            .with_exec(["apk", "add", "curl"])
            .with_service_binding("www", self.http_service())
            .with_exec(["curl", "http://www:80"])
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, Container, Directory, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns an HTTP service
   */
  @func()
  httpService(): Service {
    return dag
      .container()
      .from("nginx:1.25-alpine")
      .withNewFile("/usr/share/nginx/html/index.html", {
        contents: "Hello, world!",
      })
      .withExposedPort(80)
      .asService()
  }

  /**
   * Accesses the HTTP service
   */
  @func()
  async get(): Promise<string> {
    // bind HTTP service to container
    // access HTTP service using service binding
    // return response
    return await dag
      .container()
      .from("alpine:latest")
      .withExec(["apk", "add", "curl"])
      .withServiceBinding("www", this.httpService())
      .withExec(["curl", "http://www:80"])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns an HTTP service
     */
    #[DaggerFunction]
    public function httpService(): Service
    {
        return dag()
            ->container()
            ->from('nginx:1.25-alpine')
            ->withNewFile('/usr/share/nginx/html/index.html', contents: 'Hello, world!')
            ->withExposedPort(80)
            ->asService();
    }

    /**
     * Accesses the HTTP service
     */
    #[DaggerFunction]
    public function get(): string
    {
        // bind HTTP service to container
        // access HTTP service using service binding
        // return response
        return dag()
            ->container()
            ->from('alpine:latest')
            ->withExec(['apk', 'add', 'curl'])
            ->withServiceBinding('www', $this->httpService())
            ->withExec(['curl', 'http://www:80'])
            ->stdout();
    }
}

```

### Java
```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns an HTTP service
   */
  @Function
  public Service httpService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("nginx:1.25-alpine")
          .withNewFile(
              "/usr/share/nginx/html/index.html",
              new Container.WithNewFileArguments().withContents("Hello, world!"))
          .withExposedPort(80)
          .asService();
    }
  }

  /**
   * Accesses the HTTP service
   */
  @Function
  public String get() throws Exception {
    try (Client client = Dagger.connect()) {
      // bind HTTP service to container
      // access HTTP service using service binding
      // return response
      return client
          .container()
          .from("alpine:latest")
          .withExec(List.of("apk", "add", "curl"))
          .withServiceBinding("www", this.httpService())
          .withExec(List.of("curl", "http://www:80"))
          .stdout()
          .get();
    }
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c get
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
get
```

### Dagger CLI
```shell
dagger call get
```

The result will be:

```shell
Hello, world!
```

## Expose services returned by functions to the host

Services returned by Dagger Functions can also be exposed directly to the host. This enables clients on the host to communicate with services running in Dagger.

One use case is for testing, where you need to be able to spin up ephemeral databases against which to run tests. You might also use this to access a web UI in a browser on your desktop.

Here is another example call for the Dagger Function shown previously, this time exposing the HTTP service on the host

### System shell
```shell
dagger -c 'http-service | up'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
http-service | up
```

### Dagger CLI
```shell
dagger call http-service up
```

By default, each service port maps to the same port on the host - in this case, port 8080. The service can then be accessed by clients on the host. Here's an example:

```shell
curl localhost:8080
```

The result will be:

```shell
Hello, world!
```

To specify a different mapping, use the additional `--ports` argument with a list of host/service port mappings. Here's an example, which exposes the service on host port 9000:

### System shell
```shell
dagger -c 'http-service | up --ports 9000:8080'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
http-service | up --ports 9000:8080
```

### Dagger CLI
```shell
dagger call http-service up --ports 9000:8080
```

> **Note:**
> To bind ports randomly, use the `--random` argument.

## Expose host services to functions

Dagger Functions can also receive host services as function arguments of type `Service`, in the form `tcp://<host>:<port>`. This enables client containers in Dagger Functions to communicate with services running on the host.

> **Note:**
> This implies that a service is already listening on a port on the host, out-of-band of Dagger.

Here is an example of how a container running in a Dagger Function can access and query a MariaDB database service (bound using the alias `db`) running on the host.

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a list of users from the database
func (m *MyModule) UserList(ctx context.Context, svc *dagger.Service) (string, error) {
	return dag.Container().
		From("mariadb:10.11.2").
		WithServiceBinding("db", svc).
		WithEnvVariable("MARIADB_HOST", "db").
		WithEnvVariable("MARIADB_PASSWORD", "secret").
		WithExec([]string{"mariadb", "-u", "root", "-e", "SELECT Host, User FROM mysql.user;"}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def user_list(self, svc: dagger.Service) -> str:
        """Returns a list of users from the database"""
        return await (
            dag.container()
            .from_("mariadb:10.11.2")
            .with_service_binding("db", svc)
            .with_env_variable("MARIADB_HOST", "db")
            .with_env_variable("MARIADB_PASSWORD", "secret")
            .with_exec(
                ["mariadb", "-u", "root", "-e", "SELECT Host, User FROM mysql.user;"]
            )
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a list of users from the database
   */
  @func()
  async userList(svc: Service): Promise<string> {
    return await dag
      .container()
      .from("mariadb:10.11.2")
      .withServiceBinding("db", svc)
      .withEnvVariable("MARIADB_HOST", "db")
      .withEnvVariable("MARIADB_PASSWORD", "secret")
      .withExec([
        "mariadb",
        "-u",
        "root",
        "-e",
        "SELECT Host, User FROM mysql.user;",
      ])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a list of users from the database
     */
    #[DaggerFunction]
    public function userList(Service $svc): string
    {
        return dag()
            ->container()
            ->from('mariadb:10.11.2')
            ->withServiceBinding('db', $svc)
            ->withEnvVariable('MARIADB_HOST', 'db')
            ->withEnvVariable('MARIADB_PASSWORD', 'secret')
            ->withExec(['mariadb', '-u', 'root', '-e', 'SELECT Host, User FROM mysql.user;'])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a list of users from the database
   */
  @Function
  public String userList(@Description("Database service") Service svc) throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("mariadb:10.11.2")
          .withServiceBinding("db", svc)
          .withEnvVariable("MARIADB_HOST", "db")
          .withEnvVariable("MARIADB_PASSWORD", "secret")
          .withExec(
              List.of("mariadb", "-u", "root", "-e", "SELECT Host, User FROM mysql.user;"))
          .stdout()
          .get();
    }
  }
}

```

Before calling this Dagger Function, use the following command to start a MariaDB database service on the host:

```shell
docker run --rm --detach -p 3306:3306 --name my-mariadb --env MARIADB_ROOT_PASSWORD=secret  mariadb:10.11.2
```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'user-list tcp://localhost:3306'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
user-list tcp://localhost:3306
```

### Dagger CLI
```shell
dagger call user-list --svc=tcp://localhost:3306
```

The result will be:

```shell
Host    User
%       root
localhost       mariadb.sys
localhost       root
```

## Create interdependent services

Global hostnames can be assigned to services. This feature is especially valuable for complex networking configurations, such as circular dependencies between services, by allowing services to reference each other by predefined hostnames, without requiring an explicit service binding.

Custom hostnames follow a structured format (`<host>.<module id>.<session id>.dagger.local`), ensuring unique identifiers across modules and sessions.

For example, you can now run two services that depend on each other, each using a hostname to refer to the other by name:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Create two interdependent services
func (m *MyModule) Services(ctx context.Context) *dagger.Container {
	// create service A
	serviceA := dag.Container().
		From("python:3.11-slim").
		WithExec([]string{"pip", "install", "flask"}).
		WithNewFile("/srv/app.py", dagger.ContainerWithNewFileOpts{
			Contents: `
from flask import Flask, request
import requests

app = Flask(__name__)

@app.route('/')
def index():
    # Make a request to service B
    response = requests.get('http://svcb:8081')
    return f'Service A received response from Service B: {response.text}'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
`,
		}).
		WithExposedPort(8080).
		WithExec([]string{"python", "/srv/app.py"})

	// create service B
	serviceB := dag.Container().
		From("python:3.11-slim").
		WithExec([]string{"pip", "install", "flask"}).
		WithNewFile("/srv/app.py", dagger.ContainerWithNewFileOpts{
			Contents: `
from flask import Flask, request
import requests

app = Flask(__name__)

@app.route('/')
def index():
    # Make a request to service A
    response = requests.get('http://svca:8080')
    return f'Service B received response from Service A: {response.text}'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8081)
`,
		}).
		WithExposedPort(8081).
		WithExec([]string{"python", "/srv/app.py"})

	// create client container with service bindings
	return dag.Container().
		From("alpine:latest").
		WithExec([]string{"apk", "add", "curl"}).
		WithServiceBinding("svca", serviceA.AsService()).
		WithServiceBinding("svcb", serviceB.AsService()).
		WithExec([]string{"curl", "http://svca:8080"})
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def services(self) -> dagger.Container:
        """Create two interdependent services"""
        # create service A
        service_a = (
            dag.container()
            .from_("python:3.11-slim")
            .with_exec(["pip", "install", "flask"])
            .with_new_file(
                "/srv/app.py",
                contents="""
from flask import Flask, request
import requests

app = Flask(__name__)

@app.route('/')
def index():
    # Make a request to service B
    response = requests.get('http://svcb:8081')
    return f'Service A received response from Service B: {response.text}'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
""",
            )
            .with_exposed_port(8080)
            .with_exec(["python", "/srv/app.py"])
        )

        # create service B
        service_b = (
            dag.container()
            .from_("python:3.11-slim")
            .with_exec(["pip", "install", "flask"])
            .with_new_file(
                "/srv/app.py",
                contents="""
from flask import Flask, request
import requests

app = Flask(__name__)

@app.route('/')
def index():
    # Make a request to service A
    response = requests.get('http://svca:8080')
    return f'Service B received response from Service A: {response.text}'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8081)
""",
            )
            .with_exposed_port(8081)
            .with_exec(["python", "/srv/app.py"])
        )

        # create client container with service bindings
        return await (
            dag.container()
            .from_("alpine:latest")
            .with_exec(["apk", "add", "curl"])
            .with_service_binding("svca", service_a.as_service())
            .with_service_binding("svcb", service_b.as_service())
            .with_exec(["curl", "http://svca:8080"])
        )

```

### TypeScript

```typescript
import { dag, Container, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Create two interdependent services
   */
  @func()
  async services(): Promise<Container> {
    // create service A
    const serviceA = dag
      .container()
      .from("python:3.11-slim")
      .withExec(["pip", "install", "flask"])
      .withNewFile("/srv/app.py", {
        contents: `
from flask import Flask, request
import requests

app = Flask(__name__)

@app.route('/')
def index():
    # Make a request to service B
    response = requests.get('http://svcb:8081')
    return f'Service A received response from Service B: {response.text}'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
`,
      })
      .withExposedPort(8080)
      .withExec(["python", "/srv/app.py"])

    // create service B
    const serviceB = dag
      .container()
      .from("python:3.11-slim")
      .withExec(["pip", "install", "flask"])
      .withNewFile("/srv/app.py", {
        contents: `
from flask import Flask, request
import requests

app = Flask(__name__)

@app.route('/')
def index():
    # Make a request to service A
    response = requests.get('http://svca:8080')
    return f'Service B received response from Service A: {response.text}'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8081)
`,
      })
      .withExposedPort(8081)
      .withExec(["python", "/srv/app.py"])

    // create client container with service bindings
    return await dag
      .container()
      .from("alpine:latest")
      .withExec(["apk", "add", "curl"])
      .withServiceBinding("svca", serviceA.asService())
      .withServiceBinding("svcb", serviceB.asService())
      .withExec(["curl", "http://svca:8080"])
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Container;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Create two interdependent services
     */
    #[DaggerFunction]
    public function services(): Container
    {
        // create service A
        $serviceA = dag()
            ->container()
            ->from('python:3.11-slim')
            ->withExec(['pip', 'install', 'flask'])
            ->withNewFile('/srv/app.py', contents: <<<PYTHON
from flask import Flask, request
import requests

app = Flask(__name__)

@app.route('/')
def index():
    # Make a request to service B
    response = requests.get('http://svcb:8081')
    return f'Service A received response from Service B: {response.text}'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
PYTHON)
            ->withExposedPort(8080)
            ->withExec(['python', '/srv/app.py']);

        // create service B
        $serviceB = dag()
            ->container()
            ->from('python:3.11-slim')
            ->withExec(['pip', 'install', 'flask'])
            ->withNewFile('/srv/app.py', contents: <<<PYTHON
from flask import Flask, request
import requests

app = Flask(__name__)

@app.route('/')
def index():
    # Make a request to service A
    response = requests.get('http://svca:8080')
    return f'Service B received response from Service A: {response.text}'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8081)
PYTHON)
            ->withExposedPort(8081)
            ->withExec(['python', '/srv/app.py']);

        // create client container with service bindings
        return dag()
            ->container()
            ->from('alpine:latest')
            ->withExec(['apk', 'add', 'curl'])
            ->withServiceBinding('svca', $serviceA->asService())
            ->withServiceBinding('svcb', $serviceB->asService())
            ->withExec(['curl', 'http://svca:8080']);
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Create two interdependent services
   */
  @Function
  public Container services() throws Exception {
    try (Client client = Dagger.connect()) {
      // create service A
      Container serviceA = client
          .container()
          .from("python:3.11-slim")
          .withExec(List.of("pip", "install", "flask"))
          .withNewFile(
              "/srv/app.py",
              new Container.WithNewFileArguments().withContents("""
                  from flask import Flask, request
                  import requests

                  app = Flask(__name__)

                  @app.route('/')
                  def index():
                      # Make a request to service B
                      response = requests.get('http://svcb:8081')
                      return f'Service A received response from Service B: {response.text}'

                  if __name__ == '__main__':
                      app.run(host='0.0.0.0', port=8080)
                  """))
          .withExposedPort(8080)
          .withExec(List.of("python", "/srv/app.py"));

      // create service B
      Container serviceB = client
          .container()
          .from("python:3.11-slim")
          .withExec(List.of("pip", "install", "flask"))
          .withNewFile(
              "/srv/app.py",
              new Container.WithNewFileArguments().withContents("""
                  from flask import Flask, request
                  import requests

                  app = Flask(__name__)

                  @app.route('/')
                  def index():
                      # Make a request to service A
                      response = requests.get('http://svca:8080')
                      return f'Service B received response from Service A: {response.text}'

                  if __name__ == '__main__':
                      app.run(host='0.0.0.0', port=8081)
                  """))
          .withExposedPort(8081)
          .withExec(List.of("python", "/srv/app.py"));

      // create client container with service bindings
      return client
          .container()
          .from("alpine:latest")
          .withExec(List.of("apk", "add", "curl"))
          .withServiceBinding("svca", serviceA.asService())
          .withServiceBinding("svcb", serviceB.asService())
          .withExec(List.of("curl", "http://svca:8080"));
    }
  }
}

```

In this example, service A and service B are set up with custom hostnames `svca` and `svcb`, allowing each service to communicate with the other by hostname. This capability provides enhanced flexibility for managing service dependencies and interconnections within modular workflows, making it easier to handle complex setups in Dagger.

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c 'services | up --ports 8080:80'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
services | up --ports 8080:80
```

### Dagger CLI
```shell
dagger call services up --ports 8080:80
```


## Persist service state

Dagger cancels each service run after a 10 second grace period to avoid frequent restarts. To avoid relying on the grace period, use a cache volume to persist a service's data, as in the following example:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a Redis service
func (m *MyModule) RedisService(ctx context.Context) *dagger.Service {
	// create cache volume for redis data
	redisData := dag.CacheVolume("redis-data")

	// create redis container
	// mount cache volume to /data
	// expose redis port 6379
	// start service
	return dag.Container().
		From("redis:7.2-alpine").
		WithMountedCache("/data", redisData).
		WithExposedPort(6379).
		AsService()
}

// Sets a key in the Redis service
func (m *MyModule) Set(ctx context.Context, key string, value string) (string, error) {
	// bind redis service to container
	// execute redis-cli command
	// return response
	return dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", m.RedisService(ctx)).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "set", key, value}).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "save"}).
		Stdout(ctx)
}

// Gets a key from the Redis service
func (m *MyModule) Get(ctx context.Context, key string) (string, error) {
	// bind redis service to container
	// execute redis-cli command
	// return response
	return dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", m.RedisService(ctx)).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "get", key}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def redis_service(self) -> dagger.Service:
        """Returns a Redis service"""
        # create cache volume for redis data
        redis_data = dag.cache_volume("redis-data")

        # create redis container
        # mount cache volume to /data
        # expose redis port 6379
        # start service
        return (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_mounted_cache("/data", redis_data)
            .with_exposed_port(6379)
            .as_service()
        )

    @function
    async def set(self, key: str, value: str) -> str:
        """Sets a key in the Redis service"""
        # bind redis service to container
        # execute redis-cli command
        # return response
        return await (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", self.redis_service())
            .with_exec(["redis-cli", "-h", "redis-srv", "set", key, value])
            .with_exec(["redis-cli", "-h", "redis-srv", "save"])
            .stdout()
        )

    @function
    async def get(self, key: str) -> str:
        """Gets a key from the Redis service"""
        # bind redis service to container
        # execute redis-cli command
        # return response
        return await (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", self.redis_service())
            .with_exec(["redis-cli", "-h", "redis-srv", "get", key])
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, CacheVolume, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a Redis service
   */
  @func()
  redisService(): Service {
    // create cache volume for redis data
    const redisData: CacheVolume = dag.cacheVolume("redis-data")

    // create redis container
    // mount cache volume to /data
    // expose redis port 6379
    // start service
    return dag
      .container()
      .from("redis:7.2-alpine")
      .withMountedCache("/data", redisData)
      .withExposedPort(6379)
      .asService()
  }

  /**
   * Sets a key in the Redis service
   */
  @func()
  async set(key: string, value: string): Promise<string> {
    // bind redis service to container
    // execute redis-cli command
    // return response
    return await dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", this.redisService())
      .withExec(["redis-cli", "-h", "redis-srv", "set", key, value])
      .withExec(["redis-cli", "-h", "redis-srv", "save"])
      .stdout()
  }

  /**
   * Gets a key from the Redis service
   */
  @func()
  async get(key: string): Promise<string> {
    // bind redis service to container
    // execute redis-cli command
    // return response
    return await dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", this.redisService())
      .withExec(["redis-cli", "-h", "redis-srv", "get", key])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a Redis service
     */
    #[DaggerFunction]
    public function redisService(): Service
    {
        // create cache volume for redis data
        $redisData = dag()->cacheVolume('redis-data');

        // create redis container
        // mount cache volume to /data
        // expose redis port 6379
        // start service
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withMountedCache('/data', $redisData)
            ->withExposedPort(6379)
            ->asService();
    }

    /**
     * Sets a key in the Redis service
     */
    #[DaggerFunction]
    public function set(string $key, string $value): string
    {
        // bind redis service to container
        // execute redis-cli command
        // return response
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $this->redisService())
            ->withExec(['redis-cli', '-h', 'redis-srv', 'set', $key, $value])
            ->withExec(['redis-cli', '-h', 'redis-srv', 'save'])
            ->stdout();
    }

    /**
     * Gets a key from the Redis service
     */
    #[DaggerFunction]
    public function get(string $key): string
    {
        // bind redis service to container
        // execute redis-cli command
        // return response
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $this->redisService())
            ->withExec(['redis-cli', '-h', 'redis-srv', 'get', $key])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.CacheVolume;
import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a Redis service
   */
  @Function
  public Service redisService() throws Exception {
    try (Client client = Dagger.connect()) {
      // create cache volume for redis data
      CacheVolume redisData = client.cacheVolume("redis-data");

      // create redis container
      // mount cache volume to /data
      // expose redis port 6379
      // start service
      return client
          .container()
          .from("redis:7.2-alpine")
          .withMountedCache("/data", redisData)
          .withExposedPort(6379)
          .asService();
    }
  }

  /**
   * Sets a key in the Redis service
   */
  @Function
  public String set(String key, String value) throws Exception {
    try (Client client = Dagger.connect()) {
      // bind redis service to container
      // execute redis-cli command
      // return response
      return client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", this.redisService())
          .withExec(List.of("redis-cli", "-h", "redis-srv", "set", key, value))
          .withExec(List.of("redis-cli", "-h", "redis-srv", "save"))
          .stdout()
          .get();
    }
  }

  /**
   * Gets a key from the Redis service
   */
  @Function
  public String get(String key) throws Exception {
    try (Client client = Dagger.connect()) {
      // bind redis service to container
      // execute redis-cli command
      // return response
      return client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", this.redisService())
          .withExec(List.of("redis-cli", "-h", "redis-srv", "get", key))
          .stdout()
          .get();
    }
  }
}

```


This example uses Redis's `SAVE` command to save the service's data to a cache volume. When a new instance of the service is created, it uses the same cache volume to recreate the original state.

Here is an example of using these Dagger Functions:

### System shell
```shell
dagger -c 'set foo 123'
dagger -c 'get foo'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
set foo 123
get foo
```

### Dagger CLI
```shell
dagger call set --key=foo --value=123
dagger call get --key=foo
```

The result will be:

```shell
123
```

## Start and stop services

Services are designed to be expressed as a Directed Acyclic Graph (DAG) with explicit bindings allowing services to be started lazily, just like every other DAG node. But sometimes, you may need to explicitly manage the lifecycle in a Dagger Function.

For example, this may be needed if the application in the service has certain behavior on shutdown (such as flushing data) that needs careful coordination with the rest of your logic.

The following example explicitly starts the Redis service and stops it at the end, ensuring the 10 second grace period doesn't get in the way, without the need for a persistent cache volume:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a Redis service
func (m *MyModule) RedisService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("redis:7.2-alpine").
		WithExposedPort(6379).
		AsService()
}

// Sets and gets a key in the Redis service
func (m *MyModule) SetGet(ctx context.Context, key string, value string) (string, error) {
	// start redis service
	redisSrv, err := m.RedisService(ctx).Start(ctx)
	if err != nil {
		return "", err
	}

	// create redis client container
	// bind redis service
	// execute redis-cli command
	redisCLI := dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", redisSrv).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "set", key, value}).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "get", key})

	// get result
	val, err := redisCLI.Stdout(ctx)
	if err != nil {
		return "", err
	}

	// stop redis service
	_, err = redisSrv.Stop(ctx)
	if err != nil {
		return "", err
	}

	// return result
	return val, nil
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def redis_service(self) -> dagger.Service:
        """Returns a Redis service"""
        return (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_exposed_port(6379)
            .as_service()
        )

    @function
    async def set_get(self, key: str, value: str) -> str:
        """Sets and gets a key in the Redis service"""
        # start redis service
        redis_srv = await self.redis_service().start()

        # create redis client container
        # bind redis service
        # execute redis-cli command
        redis_cli = (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", redis_srv)
            .with_exec(["redis-cli", "-h", "redis-srv", "set", key, value])
            .with_exec(["redis-cli", "-h", "redis-srv", "get", key])
        )

        # get result
        val = await redis_cli.stdout()

        # stop redis service
        await redis_srv.stop()

        # return result
        return val

```

### TypeScript

```typescript
import { dag, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a Redis service
   */
  @func()
  redisService(): Service {
    return dag
      .container()
      .from("redis:7.2-alpine")
      .withExposedPort(6379)
      .asService()
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @func()
  async setGet(key: string, value: string): Promise<string> {
    // start redis service
    const redisSrv = await this.redisService().start()

    // create redis client container
    // bind redis service
    // execute redis-cli command
    const redisCLI: Container = dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", redisSrv)
      .withExec(["redis-cli", "-h", "redis-srv", "set", key, value])
      .withExec(["redis-cli", "-h", "redis-srv", "get", key])

    // get result
    const val = await redisCLI.stdout()

    // stop redis service
    await redisSrv.stop()

    // return result
    return val
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a Redis service
     */
    #[DaggerFunction]
    public function redisService(): Service
    {
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withExposedPort(6379)
            ->asService();
    }

    /**
     * Sets and gets a key in the Redis service
     */
    #[DaggerFunction]
    public function setGet(string $key, string $value): string
    {
        // start redis service
        $redisSrv = $this->redisService()->start();

        // create redis client container
        // bind redis service
        // execute redis-cli command
        $redisCLI = dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $redisSrv)
            ->withExec(['redis-cli', '-h', 'redis-srv', 'set', $key, $value])
            ->withExec(['redis-cli', '-h', 'redis-srv', 'get', $key]);

        // get result
        $val = $redisCLI->stdout();

        // stop redis service
        $redisSrv->stop();

        // return result
        return $val;
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a Redis service
   */
  @Function
  public Service redisService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("redis:7.2-alpine")
          .withExposedPort(6379)
          .asService();
    }
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @Function
  public String setGet(String key, String value) throws Exception {
    try (Client client = Dagger.connect()) {
      // start redis service
      Service redisSrv = this.redisService().start().get();

      // create redis client container
      // bind redis service
      // execute redis-cli command
      Container redisCLI = client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", redisSrv)
          .withExec(List.of("redis-cli", "-h", "redis-srv", "set", key, value))
          .withExec(List.of("redis-cli", "-h", "redis-srv", "get", key));

      // get result
      String val = redisCLI.stdout().get();

      // stop redis service
      redisSrv.stop().get();

      // return result
      return val;
    }
  }
}

```

## Example: MariaDB database service for application tests

The following example demonstrates how services can be used in Dagger Functions, by creating a Dagger Function for application unit/integration testing against a bound MariaDB database service.

The application used in this example is [Drupal](https://www.drupal.org/), a popular open-source PHP CMS. Drupal includes a large number of unit tests, including tests which require an active database connection. All Drupal 10.x tests are written and executed using the [PHPUnit](https://phpunit.de/) testing framework. Read more about [running PHPUnit tests in Drupal](https://phpunit.de/).

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a MariaDB service
func (m *MyModule) MariaDBService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("mariadb:10.11.2").
		WithEnvVariable("MARIADB_ROOT_PASSWORD", "secret").
		WithEnvVariable("MARIADB_DATABASE", "drupal").
		WithExposedPort(3306).
		AsService()
}

// Tests a Drupal application using a MariaDB service
func (m *MyModule) Test(ctx context.Context) (string, error) {
	// get drupal source code
	drupalDir := dag.Git("https://git.drupalcode.org/project/drupal.git").
		Branch("10.1.x").
		Tree()

	// get php container
	// mount drupal source code
	// mount composer cache
	php := dag.Container().
		From("php:8.2-cli").
		WithDirectory("/opt/drupal", drupalDir).
		WithWorkdir("/opt/drupal/web").
		WithMountedCache("/root/.composer/cache", dag.CacheVolume("composer-cache"))

	// install php dependencies
	// install drupal dependencies
	php = php.
		WithExec([]string{"apt-get", "update"}).
		WithExec([]string{"apt-get", "install", "-y", "git", "libsqlite3-dev", "libxml2-dev", "zip"}).
		WithExec([]string{"docker-php-ext-install", "gd", "pdo_mysql", "pdo_sqlite", "xml"}).
		WithExec([]string{"pecl", "install", "xdebug"}).
		WithExec([]string{"docker-php-ext-enable", "xdebug"}).
		WithExec([]string{"php", "-r", "copy('https://getcomposer.org/installer', 'composer-setup.php');"}).
		WithExec([]string{"php", "composer-setup.php"}).
		WithExec([]string{"php", "-r", "unlink('composer-setup.php');"}).
		WithExec([]string{"mv", "composer.phar", "/usr/local/bin/composer"}).
		WithExec([]string{"composer", "install"})

	// bind mariadb service
	// set database url env var
	// execute tests
	// return test output
	return php.
		WithServiceBinding("db", m.MariaDBService(ctx)).
		WithEnvVariable("SIMPLETEST_DB", "mysql://root:secret@db/drupal").
		WithExec([]string{"../../vendor/bin/phpunit", "-c", "core/phpunit.xml.dist", "core/modules/user/tests/src/Kernel"}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def mariadb_service(self) -> dagger.Service:
        """Returns a MariaDB service"""
        return (
            dag.container()
            .from_("mariadb:10.11.2")
            .with_env_variable("MARIADB_ROOT_PASSWORD", "secret")
            .with_env_variable("MARIADB_DATABASE", "drupal")
            .with_exposed_port(3306)
            .as_service()
        )

    @function
    async def test(self) -> str:
        """Tests a Drupal application using a MariaDB service"""
        # get drupal source code
        drupal_dir = dag.git("https://git.drupalcode.org/project/drupal.git").branch(
            "10.1.x"
        ).tree()

        # get php container
        # mount drupal source code
        # mount composer cache
        php = (
            dag.container()
            .from_("php:8.2-cli")
            .with_directory("/opt/drupal", drupal_dir)
            .with_workdir("/opt/drupal/web")
            .with_mounted_cache(
                "/root/.composer/cache", dag.cache_volume("composer-cache")
            )
        )

        # install php dependencies
        # install drupal dependencies
        php = (
            php.with_exec(["apt-get", "update"])
            .with_exec(
                ["apt-get", "install", "-y", "git", "libsqlite3-dev", "libxml2-dev", "zip"]
            )
            .with_exec(["docker-php-ext-install", "gd", "pdo_mysql", "pdo_sqlite", "xml"])
            .with_exec(["pecl", "install", "xdebug"])
            .with_exec(["docker-php-ext-enable", "xdebug"])
            .with_exec(["php", "-r", "copy('https://getcomposer.org/installer', 'composer-setup.php');"])
            .with_exec(["php", "composer-setup.php"])
            .with_exec(["php", "-r", "unlink('composer-setup.php');"])
            .with_exec(["mv", "composer.phar", "/usr/local/bin/composer"])
            .with_exec(["composer", "install"])
        )

        # bind mariadb service
        # set database url env var
        # execute tests
        # return test output
        return await (
            php.with_service_binding("db", self.mariadb_service())
            .with_env_variable("SIMPLETEST_DB", "mysql://root:secret@db/drupal")
            .with_exec(
                [
                    "../../vendor/bin/phpunit",
                    "-c",
                    "core/phpunit.xml.dist",
                    "core/modules/user/tests/src/Kernel",
                ]
            )
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, Directory, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a MariaDB service
   */
  @func()
  mariadbService(): Service {
    return dag
      .container()
      .from("mariadb:10.11.2")
      .withEnvVariable("MARIADB_ROOT_PASSWORD", "secret")
      .withEnvVariable("MARIADB_DATABASE", "drupal")
      .withExposedPort(3306)
      .asService()
  }

  /**
   * Tests a Drupal application using a MariaDB service
   */
  @func()
  async test(): Promise<string> {
    // get drupal source code
    const drupalDir: Directory = dag
      .git("https://git.drupalcode.org/project/drupal.git")
      .branch("10.1.x")
      .tree()

    // get php container
    // mount drupal source code
    // mount composer cache
    let php: Container = dag
      .container()
      .from("php:8.2-cli")
      .withDirectory("/opt/drupal", drupalDir)
      .withWorkdir("/opt/drupal/web")
      .withMountedCache("/root/.composer/cache", dag.cacheVolume("composer-cache"))

    // install php dependencies
    // install drupal dependencies
    php = php
      .withExec(["apt-get", "update"])
      .withExec([
        "apt-get",
        "install",
        "-y",
        "git",
        "libsqlite3-dev",
        "libxml2-dev",
        "zip",
      ])
      .withExec([
        "docker-php-ext-install",
        "gd",
        "pdo_mysql",
        "pdo_sqlite",
        "xml",
      ])
      .withExec(["pecl", "install", "xdebug"])
      .withExec(["docker-php-ext-enable", "xdebug"])
      .withExec([
        "php",
        "-r",
        "copy('https://getcomposer.org/installer', 'composer-setup.php');",
      ])
      .withExec(["php", "composer-setup.php"])
      .withExec(["php", "-r", "unlink('composer-setup.php');"])
      .withExec(["mv", "composer.phar", "/usr/local/bin/composer"])
      .withExec(["composer", "install"])

    // bind mariadb service
    // set database url env var
    // execute tests
    // return test output
    return await php
      .withServiceBinding("db", this.mariadbService())
      .withEnvVariable("SIMPLETEST_DB", "mysql://root:secret@db/drupal")
      .withExec([
        "../../vendor/bin/phpunit",
        "-c",
        "core/phpunit.xml.dist",
        "core/modules/user/tests/src/Kernel",
      ])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a MariaDB service
     */
    #[DaggerFunction]
    public function mariadbService(): Service
    {
        return dag()
            ->container()
            ->from('mariadb:10.11.2')
            ->withEnvVariable('MARIADB_ROOT_PASSWORD', 'secret')
            ->withEnvVariable('MARIADB_DATABASE', 'drupal')
            ->withExposedPort(3306)
            ->asService();
    }

    /**
     * Tests a Drupal application using a MariaDB service
     */
    #[DaggerFunction]
    public function test(): string
    {
        // get drupal source code
        $drupalDir = dag()
            ->git('https://git.drupalcode.org/project/drupal.git')
            ->branch('10.1.x')
            ->tree();

        // get php container
        // mount drupal source code
        // mount composer cache
        $php = dag()
            ->container()
            ->from('php:8.2-cli')
            ->withDirectory('/opt/drupal', $drupalDir)
            ->withWorkdir('/opt/drupal/web')
            ->withMountedCache('/root/.composer/cache', dag()->cacheVolume('composer-cache'));

        // install php dependencies
        // install drupal dependencies
        $php = $php
            ->withExec(['apt-get', 'update'])
            ->withExec(['apt-get', 'install', '-y', 'git', 'libsqlite3-dev', 'libxml2-dev', 'zip'])
            ->withExec(['docker-php-ext-install', 'gd', 'pdo_mysql', 'pdo_sqlite', 'xml'])
            ->withExec(['pecl', 'install', 'xdebug'])
            ->withExec(['docker-php-ext-enable', 'xdebug'])
            ->withExec(['php', '-r', "copy('https://getcomposer.org/installer', 'composer-setup.php');"])
            ->withExec(['php', 'composer-setup.php'])
            ->withExec(['php', '-r', "unlink('composer-setup.php');"])
            ->withExec(['mv', 'composer.phar', '/usr/local/bin/composer'])
            ->withExec(['composer', 'install']);

        // bind mariadb service
        // set database url env var
        // execute tests
        // return test output
        return $php
            ->withServiceBinding('db', $this->mariadbService())
            ->withEnvVariable('SIMPLETEST_DB', 'mysql://root:secret@db/drupal')
            ->withExec(['../../vendor/bin/phpunit', '-c', 'core/phpunit.xml.dist', 'core/modules/user/tests/src/Kernel'])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.client.Directory;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a MariaDB service
   */
  @Function
  public Service mariadbService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("mariadb:10.11.2")
          .withEnvVariable("MARIADB_ROOT_PASSWORD", "secret")
          .withEnvVariable("MARIADB_DATABASE", "drupal")
          .withExposedPort(3306)
          .asService();
    }
  }

  /**
   * Tests a Drupal application using a MariaDB service
   */
  @Function
  public String test() throws Exception {
    try (Client client = Dagger.connect()) {
      // get drupal source code
      Directory drupalDir = client
          .git("https://git.drupalcode.org/project/drupal.git")
          .branch("10.1.x")
          .tree();

      // get php container
      // mount drupal source code
      // mount composer cache
      Container php = client
          .container()
          .from("php:8.2-cli")
          .withDirectory("/opt/drupal", drupalDir)
          .withWorkdir("/opt/drupal/web")
          .withMountedCache("/root/.composer/cache", client.cacheVolume("composer-cache"));

      // install php dependencies
      // install drupal dependencies
      php = php
          .withExec(List.of("apt-get", "update"))
          .withExec(
              List.of(
                  "apt-get",
                  "install",
                  "-y",
                  "git",
                  "libsqlite3-dev",
                  "libxml2-dev",
                  "zip"))
          .withExec(
              List.of("docker-php-ext-install", "gd", "pdo_mysql", "pdo_sqlite", "xml"))
          .withExec(List.of("pecl", "install", "xdebug"))
          .withExec(List.of("docker-php-ext-enable", "xdebug"))
          .withExec(
              List.of(
                  "php",
                  "-r",
                  "copy('https://getcomposer.org/installer', 'composer-setup.php');"))
          .withExec(List.of("php", "composer-setup.php"))
          .withExec(List.of("php", "-r", "unlink('composer-setup.php');"))
          .withExec(List.of("mv", "composer.phar", "/usr/local/bin/composer"))
          .withExec(List.of("composer", "install"));

      // bind mariadb service
      // set database url env var
      // execute tests
      // return test output
      return php
          .withServiceBinding("db", this.mariadbService())
          .withEnvVariable("SIMPLETEST_DB", "mysql://root:secret@db/drupal")
          .withExec(
              List.of(
                  "../../vendor/bin/phpunit",
                  "-c",
                  "core/phpunit.xml.dist",
                  "core/modules/user/tests/src/Kernel"))
          .stdout()
          .get();
    }
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c test
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
test
```

### Dagger CLI
```shell
dagger call test
```

The result will be:

```shell
PHPUnit 9.6.17 by Sebastian Bergmann and contributors.
Runtime:       PHP 8.2.5
Configuration: /opt/drupal/web/core/phpunit.xml.dist
Testing
.....................S                                            22 / 22 (100%)
Time: 00:15.806, Memory: 315.00 MB
There was 1 skipped test:

1) Drupal\Tests\pgsql\Kernel\pgsql\KernelTestBaseTest::testSetUp

This test only runs for the database driver 'pgsql'. Current database driver is 'mysql'.
/opt/drupal/web/core/tests/Drupal/KernelTests/Core/Database/DriverSpecificKernelTestBase.php:44
/opt/drupal/vendor/phpunit/phpunit/src/Framework/TestResult.php:728

OK, but incomplete, skipped, or risky tests!
Tests: 22, Assertions: 72, Skipped: 1.
```

## Reference: How service binding works in Dagger Functions

If you're not interested in what's happening in the background, you can skip this section and just trust that services are running when they need to be. If you're interested in the theory, keep reading.

Consider this example:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a Redis service
func (m *MyModule) RedisService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("redis:7.2-alpine").
		WithExposedPort(6379).
		AsService()
}

// Pings the Redis service
func (m *MyModule) Ping(ctx context.Context) (string, error) {
	// bind redis service to container
	// execute redis-cli command
	// return response
	return dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", m.RedisService(ctx)).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "ping"}).
		Stdout(ctx)
}

```

Here's what happens on the last line:

1. The client requests the `ping` container's stdout, which requires the container to run.
1. Dagger sees that the `ping` container has a service binding, `redisSrv`.
1. Dagger starts the `redisSrv` container, which recurses into this same process.
1. Dagger waits for health checks to pass against `redisSrv`.
1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def redis_service(self) -> dagger.Service:
        """Returns a Redis service"""
        return (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_exposed_port(6379)
            .as_service()
        )

    @function
    async def ping(self) -> str:
        """Pings the Redis service"""
        # bind redis service to container
        # execute redis-cli command
        # return response
        return await (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", self.redis_service())
            .with_exec(["redis-cli", "-h", "redis-srv", "ping"])
            .stdout()
        )

```

Here's what happens on the last line:

1. The client requests the `ping` container's stdout, which requires the container to run.
1. Dagger sees that the `ping` container has a service binding, `redis_srv`.
1. Dagger starts the `redis_srv` container, which recurses into this same process.
1. Dagger waits for health checks to pass against `redis_srv`.
1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.

### TypeScript

```typescript
import { dag, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a Redis service
   */
  @func()
  redisService(): Service {
    return dag
      .container()
      .from("redis:7.2-alpine")
      .withExposedPort(6379)
      .asService()
  }

  /**
   * Pings the Redis service
   */
  @func()
  async ping(): Promise<string> {
    // bind redis service to container
    // execute redis-cli command
    // return response
    return await dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", this.redisService())
      .withExec(["redis-cli", "-h", "redis-srv", "ping"])
      .stdout()
  }
}

```

Here's what happens on the last line:

1. The client requests the `ping` container's stdout, which requires the container to run.
1. Dagger sees that the `ping` container has a service binding, `redisSrv`.
1. Dagger starts the `redisSrv` container, which recurses into this same process.
1. Dagger waits for health checks to pass against `redisSrv`.
1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a Redis service
     */
    #[DaggerFunction]
    public function redisService(): Service
    {
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withExposedPort(6379)
            ->asService();
    }

    /**
     * Pings the Redis service
     */
    #[DaggerFunction]
    public function ping(): string
    {
        // bind redis service to container
        // execute redis-cli command
        // return response
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $this->redisService())
            ->withExec(['redis-cli', '-h', 'redis-srv', 'ping'])
            ->stdout();
    }
}

```

Here's what happens on the last line:

1. The client requests the `ping` container's stdout, which requires the container to run.
1. Dagger sees that the `ping` container has a service binding, `$redisSrv`.
1. Dagger starts the `$redisSrv` container, which recurses into this same process.
1. Dagger waits for health checks to pass against `$redisSrv`.
1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a Redis service
   */
  @Function
  public Service redisService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("redis:7.2-alpine")
          .withExposedPort(6379)
          .asService();
    }
  }

  /**
   * Pings the Redis service
   */
  @Function
  public String ping() throws Exception {
    try (Client client = Dagger.connect()) {
      // bind redis service to container
      // execute redis-cli command
      // return response
      return client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", this.redisService())
          .withExec(List.of("redis-cli", "-h", "redis-srv", "ping"))
          .stdout()
          .get();
    }
  }
}

```

Here's what happens on the last line:

1. The client requests the `ping` container's stdout, which requires the container to run.
1. Dagger sees that the `ping` container has a service binding, `redisSrv`.
1. Dagger starts the `redisSrv` container, which recurses into this same process.
1. Dagger waits for health checks to pass against `redisSrv`.
1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.

> **Note:**
> Dagger cancels each service run after a 10 second grace period to avoid frequent restarts, unless the explicit `Start` and `Stop` APIs are used.

Services are based on containers, but they run a little differently. Whereas regular containers in Dagger are de-duplicated across the entire Dagger Engine, service containers are only de-duplicated within a Dagger client session. This means that if you run separate Dagger sessions that use the exact same services, they will each get their own "instance" of the service. This process is carefully tuned to preserve caching at each client call-site, while prohibiting "cross-talk" from one Dagger session's client to another Dagger session's service.

Content-addressed services are very convenient. You don't have to come up with names and maintain instances of services; just use them by value. You also don't have to manage the state of the service; you can just trust that it will be running when needed and stopped when not.

> **Tip:**
> If you need multiple instances of a service, just attach something unique to each one, such as an instance ID.

Here's a more detailed client-server example of running commands against a Redis service:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a Redis service
func (m *MyModule) RedisService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("redis:7.2-alpine").
		WithExposedPort(6379).
		AsService()
}

// Sets and gets a key in the Redis service
func (m *MyModule) SetGet(ctx context.Context, key string, value string) (string, error) {
	// bind redis service to container
	// execute redis-cli command
	// return response
	return dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", m.RedisService(ctx)).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "set", key, value}).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "get", key}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def redis_service(self) -> dagger.Service:
        """Returns a Redis service"""
        return (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_exposed_port(6379)
            .as_service()
        )

    @function
    async def set_get(self, key: str, value: str) -> str:
        """Sets and gets a key in the Redis service"""
        # bind redis service to container
        # execute redis-cli command
        # return response
        return await (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", self.redis_service())
            .with_exec(["redis-cli", "-h", "redis-srv", "set", key, value])
            .with_exec(["redis-cli", "-h", "redis-srv", "get", key])
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a Redis service
   */
  @func()
  redisService(): Service {
    return dag
      .container()
      .from("redis:7.2-alpine")
      .withExposedPort(6379)
      .asService()
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @func()
  async setGet(key: string, value: string): Promise<string> {
    // bind redis service to container
    // execute redis-cli command
    // return response
    return await dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", this.redisService())
      .withExec(["redis-cli", "-h", "redis-srv", "set", key, value])
      .withExec(["redis-cli", "-h", "redis-srv", "get", key])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a Redis service
     */
    #[DaggerFunction]
    public function redisService(): Service
    {
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withExposedPort(6379)
            ->asService();
    }

    /**
     * Sets and gets a key in the Redis service
     */
    #[DaggerFunction]
    public function setGet(string $key, string $value): string
    {
        // bind redis service to container
        // execute redis-cli command
        // return response
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $this->redisService())
            ->withExec(['redis-cli', '-h', 'redis-srv', 'set', $key, $value])
            ->withExec(['redis-cli', '-h', 'redis-srv', 'get', $key])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a Redis service
   */
  @Function
  public Service redisService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("redis:7.2-alpine")
          .withExposedPort(6379)
          .asService();
    }
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @Function
  public String setGet(String key, String value) throws Exception {
    try (Client client = Dagger.connect()) {
      // bind redis service to container
      // execute redis-cli command
      // return response
      return client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", this.redisService())
          .withExec(List.of("redis-cli", "-h", "redis-srv", "set", key, value))
          .withExec(List.of("redis-cli", "-h", "redis-srv", "get", key))
          .stdout()
          .get();
    }
  }
}

```

This example relies on the 10-second grace period, which you should try to avoid. Depending on the 10-second grace period is risky because there are many factors which could cause a 10-second delay between calls to Dagger, such as excessive CPU load, high network latency between the client and Dagger, or Dagger operations that require a variable amount of time to process.

It would be better to chain both commands together, which ensures that the service stays running for both, as in the revision below:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a Redis service
func (m *MyModule) RedisService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("redis:7.2-alpine").
		WithExposedPort(6379).
		AsService()
}

// Sets and gets a key in the Redis service
func (m *MyModule) SetGet(ctx context.Context, key string, value string) (string, error) {
	// bind redis service to container
	// execute redis-cli command
	// return response
	return dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", m.RedisService(ctx)).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "set", key, value}).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "get", key}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def redis_service(self) -> dagger.Service:
        """Returns a Redis service"""
        return (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_exposed_port(6379)
            .as_service()
        )

    @function
    async def set_get(self, key: str, value: str) -> str:
        """Sets and gets a key in the Redis service"""
        # bind redis service to container
        # execute redis-cli command
        # return response
        return await (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", self.redis_service())
            .with_exec(["redis-cli", "-h", "redis-srv", "set", key, value])
            .with_exec(["redis-cli", "-h", "redis-srv", "get", key])
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a Redis service
   */
  @func()
  redisService(): Service {
    return dag
      .container()
      .from("redis:7.2-alpine")
      .withExposedPort(6379)
      .asService()
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @func()
  async setGet(key: string, value: string): Promise<string> {
    // bind redis service to container
    // execute redis-cli command
    // return response
    return await dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", this.redisService())
      .withExec(["redis-cli", "-h", "redis-srv", "set", key, value])
      .withExec(["redis-cli", "-h", "redis-srv", "get", key])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a Redis service
     */
    #[DaggerFunction]
    public function redisService(): Service
    {
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withExposedPort(6379)
            ->asService();
    }

    /**
     * Sets and gets a key in the Redis service
     */
    #[DaggerFunction]
    public function setGet(string $key, string $value): string
    {
        // bind redis service to container
        // execute redis-cli command
        // return response
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $this->redisService())
            ->withExec(['redis-cli', '-h', 'redis-srv', 'set', $key, $value])
            ->withExec(['redis-cli', '-h', 'redis-srv', 'get', $key])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a Redis service
   */
  @Function
  public Service redisService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("redis:7.2-alpine")
          .withExposedPort(6379)
          .asService();
    }
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @Function
  public String setGet(String key, String value) throws Exception {
    try (Client client = Dagger.connect()) {
      // bind redis service to container
      // execute redis-cli command
      // return response
      return client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", this.redisService())
          .withExec(List.of("redis-cli", "-h", "redis-srv", "set", key, value))
          .withExec(List.of("redis-cli", "-h", "redis-srv", "get", key))
          .stdout()
          .get();
    }
  }
}		Stdout(ctx)
}

// Gets a key from the Redis service
func (m *MyModule) Get(ctx context.Context, key string) (string, error) {
	// bind redis service to container
	// execute redis-cli command
	// return response
	return dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", m.RedisService(ctx)).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "get", key}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def redis_service(self) -> dagger.Service:
        """Returns a Redis service"""
        # create cache volume for redis data
        redis_data = dag.cache_volume("redis-data")

        # create redis container
        # mount cache volume to /data
        # expose redis port 6379
        # start service
        return (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_mounted_cache("/data", redis_data)
            .with_exposed_port(6379)
            .as_service()
        )

    @function
    async def set(self, key: str, value: str) -> str:
        """Sets a key in the Redis service"""
        # bind redis service to container
        # execute redis-cli command
        # return response
        return await (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", self.redis_service())
            .with_exec(["redis-cli", "-h", "redis-srv", "set", key, value])
            .with_exec(["redis-cli", "-h", "redis-srv", "save"])
            .stdout()
        )

    @function
    async def get(self, key: str) -> str:
        """Gets a key from the Redis service"""
        # bind redis service to container
        # execute redis-cli command
        # return response
        return await (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", self.redis_service())
            .with_exec(["redis-cli", "-h", "redis-srv", "get", key])
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, CacheVolume, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a Redis service
   */
  @func()
  redisService(): Service {
    // create cache volume for redis data
    const redisData: CacheVolume = dag.cacheVolume("redis-data")

    // create redis container
    // mount cache volume to /data
    // expose redis port 6379
    // start service
    return dag
      .container()
      .from("redis:7.2-alpine")
      .withMountedCache("/data", redisData)
      .withExposedPort(6379)
      .asService()
  }

  /**
   * Sets a key in the Redis service
   */
  @func()
  async set(key: string, value: string): Promise<string> {
    // bind redis service to container
    // execute redis-cli command
    // return response
    return await dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", this.redisService())
      .withExec(["redis-cli", "-h", "redis-srv", "set", key, value])
      .withExec(["redis-cli", "-h", "redis-srv", "save"])
      .stdout()
  }

  /**
   * Gets a key from the Redis service
   */
  @func()
  async get(key: string): Promise<string> {
    // bind redis service to container
    // execute redis-cli command
    // return response
    return await dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", this.redisService())
      .withExec(["redis-cli", "-h", "redis-srv", "get", key])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a Redis service
     */
    #[DaggerFunction]
    public function redisService(): Service
    {
        // create cache volume for redis data
        $redisData = dag()->cacheVolume('redis-data');

        // create redis container
        // mount cache volume to /data
        // expose redis port 6379
        // start service
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withMountedCache('/data', $redisData)
            ->withExposedPort(6379)
            ->asService();
    }

    /**
     * Sets a key in the Redis service
     */
    #[DaggerFunction]
    public function set(string $key, string $value): string
    {
        // bind redis service to container
        // execute redis-cli command
        // return response
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $this->redisService())
            ->withExec(['redis-cli', '-h', 'redis-srv', 'set', $key, $value])
            ->withExec(['redis-cli', '-h', 'redis-srv', 'save'])
            ->stdout();
    }

    /**
     * Gets a key from the Redis service
     */
    #[DaggerFunction]
    public function get(string $key): string
    {
        // bind redis service to container
        // execute redis-cli command
        // return response
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $this->redisService())
            ->withExec(['redis-cli', '-h', 'redis-srv', 'get', $key])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.CacheVolume;
import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a Redis service
   */
  @Function
  public Service redisService() throws Exception {
    try (Client client = Dagger.connect()) {
      // create cache volume for redis data
      CacheVolume redisData = client.cacheVolume("redis-data");

      // create redis container
      // mount cache volume to /data
      // expose redis port 6379
      // start service
      return client
          .container()
          .from("redis:7.2-alpine")
          .withMountedCache("/data", redisData)
          .withExposedPort(6379)
          .asService();
    }
  }

  /**
   * Sets a key in the Redis service
   */
  @Function
  public String set(String key, String value) throws Exception {
    try (Client client = Dagger.connect()) {
      // bind redis service to container
      // execute redis-cli command
      // return response
      return client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", this.redisService())
          .withExec(List.of("redis-cli", "-h", "redis-srv", "set", key, value))
          .withExec(List.of("redis-cli", "-h", "redis-srv", "save"))
          .stdout()
          .get();
    }
  }

  /**
   * Gets a key from the Redis service
   */
  @Function
  public String get(String key) throws Exception {
    try (Client client = Dagger.connect()) {
      // bind redis service to container
      // execute redis-cli command
      // return response
      return client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", this.redisService())
          .withExec(List.of("redis-cli", "-h", "redis-srv", "get", key))
          .stdout()
          .get();
    }
  }
}

```


This example uses Redis's `SAVE` command to save the service's data to a cache volume. When a new instance of the service is created, it uses the same cache volume to recreate the original state.

Here is an example of using these Dagger Functions:

### System shell
```shell
dagger -c 'set foo 123'
dagger -c 'get foo'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
set foo 123
get foo
```

### Dagger CLI
```shell
dagger call set --key=foo --value=123
dagger call get --key=foo
```

The result will be:

```shell
123
```

## Start and stop services

Services are designed to be expressed as a Directed Acyclic Graph (DAG) with explicit bindings allowing services to be started lazily, just like every other DAG node. But sometimes, you may need to explicitly manage the lifecycle in a Dagger Function.

For example, this may be needed if the application in the service has certain behavior on shutdown (such as flushing data) that needs careful coordination with the rest of your logic.

The following example explicitly starts the Redis service and stops it at the end, ensuring the 10 second grace period doesn't get in the way, without the need for a persistent cache volume:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a Redis service
func (m *MyModule) RedisService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("redis:7.2-alpine").
		WithExposedPort(6379).
		AsService()
}

// Sets and gets a key in the Redis service
func (m *MyModule) SetGet(ctx context.Context, key string, value string) (string, error) {
	// start redis service
	redisSrv, err := m.RedisService(ctx).Start(ctx)
	if err != nil {
		return "", err
	}

	// create redis client container
	// bind redis service
	// execute redis-cli command
	redisCLI := dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", redisSrv).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "set", key, value}).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "get", key})

	// get result
	val, err := redisCLI.Stdout(ctx)
	if err != nil {
		return "", err
	}

	// stop redis service
	_, err = redisSrv.Stop(ctx)
	if err != nil {
		return "", err
	}

	// return result
	return val, nil
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def redis_service(self) -> dagger.Service:
        """Returns a Redis service"""
        return (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_exposed_port(6379)
            .as_service()
        )

    @function
    async def set_get(self, key: str, value: str) -> str:
        """Sets and gets a key in the Redis service"""
        # start redis service
        redis_srv = await self.redis_service().start()

        # create redis client container
        # bind redis service
        # execute redis-cli command
        redis_cli = (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", redis_srv)
            .with_exec(["redis-cli", "-h", "redis-srv", "set", key, value])
            .with_exec(["redis-cli", "-h", "redis-srv", "get", key])
        )

        # get result
        val = await redis_cli.stdout()

        # stop redis service
        await redis_srv.stop()

        # return result
        return val

```

### TypeScript

```typescript
import { dag, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a Redis service
   */
  @func()
  redisService(): Service {
    return dag
      .container()
      .from("redis:7.2-alpine")
      .withExposedPort(6379)
      .asService()
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @func()
  async setGet(key: string, value: string): Promise<string> {
    // start redis service
    const redisSrv = await this.redisService().start()

    // create redis client container
    // bind redis service
    // execute redis-cli command
    const redisCLI: Container = dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", redisSrv)
      .withExec(["redis-cli", "-h", "redis-srv", "set", key, value])
      .withExec(["redis-cli", "-h", "redis-srv", "get", key])

    // get result
    const val = await redisCLI.stdout()

    // stop redis service
    await redisSrv.stop()

    // return result
    return val
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a Redis service
     */
    #[DaggerFunction]
    public function redisService(): Service
    {
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withExposedPort(6379)
            ->asService();
    }

    /**
     * Sets and gets a key in the Redis service
     */
    #[DaggerFunction]
    public function setGet(string $key, string $value): string
    {
        // start redis service
        $redisSrv = $this->redisService()->start();

        // create redis client container
        // bind redis service
        // execute redis-cli command
        $redisCLI = dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $redisSrv)
            ->withExec(['redis-cli', '-h', 'redis-srv', 'set', $key, $value])
            ->withExec(['redis-cli', '-h', 'redis-srv', 'get', $key]);

        // get result
        $val = $redisCLI->stdout();

        // stop redis service
        $redisSrv->stop();

        // return result
        return $val;
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a Redis service
   */
  @Function
  public Service redisService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("redis:7.2-alpine")
          .withExposedPort(6379)
          .asService();
    }
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @Function
  public String setGet(String key, String value) throws Exception {
    try (Client client = Dagger.connect()) {
      // start redis service
      Service redisSrv = this.redisService().start().get();

      // create redis client container
      // bind redis service
      // execute redis-cli command
      Container redisCLI = client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", redisSrv)
          .withExec(List.of("redis-cli", "-h", "redis-srv", "set", key, value))
          .withExec(List.of("redis-cli", "-h", "redis-srv", "get", key));

      // get result
      String val = redisCLI.stdout().get();

      // stop redis service
      redisSrv.stop().get();

      // return result
      return val;
    }
  }
}

```

## Example: MariaDB database service for application tests

The following example demonstrates how services can be used in Dagger Functions, by creating a Dagger Function for application unit/integration testing against a bound MariaDB database service.

The application used in this example is [Drupal](https://www.drupal.org/), a popular open-source PHP CMS. Drupal includes a large number of unit tests, including tests which require an active database connection. All Drupal 10.x tests are written and executed using the [PHPUnit](https://phpunit.de/) testing framework. Read more about [running PHPUnit tests in Drupal](https://phpunit.de/).

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a MariaDB service
func (m *MyModule) MariaDBService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("mariadb:10.11.2").
		WithEnvVariable("MARIADB_ROOT_PASSWORD", "secret").
		WithEnvVariable("MARIADB_DATABASE", "drupal").
		WithExposedPort(3306).
		AsService()
}

// Tests a Drupal application using a MariaDB service
func (m *MyModule) Test(ctx context.Context) (string, error) {
	// get drupal source code
	drupalDir := dag.Git("https://git.drupalcode.org/project/drupal.git").
		Branch("10.1.x").
		Tree()

	// get php container
	// mount drupal source code
	// mount composer cache
	php := dag.Container().
		From("php:8.2-cli").
		WithDirectory("/opt/drupal", drupalDir).
		WithWorkdir("/opt/drupal/web").
		WithMountedCache("/root/.composer/cache", dag.CacheVolume("composer-cache"))

	// install php dependencies
	// install drupal dependencies
	php = php.
		WithExec([]string{"apt-get", "update"}).
		WithExec([]string{"apt-get", "install", "-y", "git", "libsqlite3-dev", "libxml2-dev", "zip"}).
		WithExec([]string{"docker-php-ext-install", "gd", "pdo_mysql", "pdo_sqlite", "xml"}).
		WithExec([]string{"pecl", "install", "xdebug"}).
		WithExec([]string{"docker-php-ext-enable", "xdebug"}).
		WithExec([]string{"php", "-r", "copy('https://getcomposer.org/installer', 'composer-setup.php');"}).
		WithExec([]string{"php", "composer-setup.php"}).
		WithExec([]string{"php", "-r", "unlink('composer-setup.php');"}).
		WithExec([]string{"mv", "composer.phar", "/usr/local/bin/composer"}).
		WithExec([]string{"composer", "install"})

	// bind mariadb service
	// set database url env var
	// execute tests
	// return test output
	return php.
		WithServiceBinding("db", m.MariaDBService(ctx)).
		WithEnvVariable("SIMPLETEST_DB", "mysql://root:secret@db/drupal").
		WithExec([]string{"../../vendor/bin/phpunit", "-c", "core/phpunit.xml.dist", "core/modules/user/tests/src/Kernel"}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def mariadb_service(self) -> dagger.Service:
        """Returns a MariaDB service"""
        return (
            dag.container()
            .from_("mariadb:10.11.2")
            .with_env_variable("MARIADB_ROOT_PASSWORD", "secret")
            .with_env_variable("MARIADB_DATABASE", "drupal")
            .with_exposed_port(3306)
            .as_service()
        )

    @function
    async def test(self) -> str:
        """Tests a Drupal application using a MariaDB service"""
        # get drupal source code
        drupal_dir = dag.git("https://git.drupalcode.org/project/drupal.git").branch(
            "10.1.x"
        ).tree()

        # get php container
        # mount drupal source code
        # mount composer cache
        php = (
            dag.container()
            .from_("php:8.2-cli")
            .with_directory("/opt/drupal", drupal_dir)
            .with_workdir("/opt/drupal/web")
            .with_mounted_cache(
                "/root/.composer/cache", dag.cache_volume("composer-cache")
            )
        )

        # install php dependencies
        # install drupal dependencies
        php = (
            php.with_exec(["apt-get", "update"])
            .with_exec(
                ["apt-get", "install", "-y", "git", "libsqlite3-dev", "libxml2-dev", "zip"]
            )
            .with_exec(["docker-php-ext-install", "gd", "pdo_mysql", "pdo_sqlite", "xml"])
            .with_exec(["pecl", "install", "xdebug"])
            .with_exec(["docker-php-ext-enable", "xdebug"])
            .with_exec(["php", "-r", "copy('https://getcomposer.org/installer', 'composer-setup.php');"])
            .with_exec(["php", "composer-setup.php"])
            .with_exec(["php", "-r", "unlink('composer-setup.php');"])
            .with_exec(["mv", "composer.phar", "/usr/local/bin/composer"])
            .with_exec(["composer", "install"])
        )

        # bind mariadb service
        # set database url env var
        # execute tests
        # return test output
        return await (
            php.with_service_binding("db", self.mariadb_service())
            .with_env_variable("SIMPLETEST_DB", "mysql://root:secret@db/drupal")
            .with_exec(
                [
                    "../../vendor/bin/phpunit",
                    "-c",
                    "core/phpunit.xml.dist",
                    "core/modules/user/tests/src/Kernel",
                ]
            )
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, Directory, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a MariaDB service
   */
  @func()
  mariadbService(): Service {
    return dag
      .container()
      .from("mariadb:10.11.2")
      .withEnvVariable("MARIADB_ROOT_PASSWORD", "secret")
      .withEnvVariable("MARIADB_DATABASE", "drupal")
      .withExposedPort(3306)
      .asService()
  }

  /**
   * Tests a Drupal application using a MariaDB service
   */
  @func()
  async test(): Promise<string> {
    // get drupal source code
    const drupalDir: Directory = dag
      .git("https://git.drupalcode.org/project/drupal.git")
      .branch("10.1.x")
      .tree()

    // get php container
    // mount drupal source code
    // mount composer cache
    let php: Container = dag
      .container()
      .from("php:8.2-cli")
      .withDirectory("/opt/drupal", drupalDir)
      .withWorkdir("/opt/drupal/web")
      .withMountedCache("/root/.composer/cache", dag.cacheVolume("composer-cache"))

    // install php dependencies
    // install drupal dependencies
    php = php
      .withExec(["apt-get", "update"])
      .withExec([
        "apt-get",
        "install",
        "-y",
        "git",
        "libsqlite3-dev",
        "libxml2-dev",
        "zip",
      ])
      .withExec([
        "docker-php-ext-install",
        "gd",
        "pdo_mysql",
        "pdo_sqlite",
        "xml",
      ])
      .withExec(["pecl", "install", "xdebug"])
      .withExec(["docker-php-ext-enable", "xdebug"])
      .withExec([
        "php",
        "-r",
        "copy('https://getcomposer.org/installer', 'composer-setup.php');",
      ])
      .withExec(["php", "composer-setup.php"])
      .withExec(["php", "-r", "unlink('composer-setup.php');"])
      .withExec(["mv", "composer.phar", "/usr/local/bin/composer"])
      .withExec(["composer", "install"])

    // bind mariadb service
    // set database url env var
    // execute tests
    // return test output
    return await php
      .withServiceBinding("db", this.mariadbService())
      .withEnvVariable("SIMPLETEST_DB", "mysql://root:secret@db/drupal")
      .withExec([
        "../../vendor/bin/phpunit",
        "-c",
        "core/phpunit.xml.dist",
        "core/modules/user/tests/src/Kernel",
      ])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a MariaDB service
     */
    #[DaggerFunction]
    public function mariadbService(): Service
    {
        return dag()
            ->container()
            ->from('mariadb:10.11.2')
            ->withEnvVariable('MARIADB_ROOT_PASSWORD', 'secret')
            ->withEnvVariable('MARIADB_DATABASE', 'drupal')
            ->withExposedPort(3306)
            ->asService();
    }

    /**
     * Tests a Drupal application using a MariaDB service
     */
    #[DaggerFunction]
    public function test(): string
    {
        // get drupal source code
        $drupalDir = dag()
            ->git('https://git.drupalcode.org/project/drupal.git')
            ->branch('10.1.x')
            ->tree();

        // get php container
        // mount drupal source code
        // mount composer cache
        $php = dag()
            ->container()
            ->from('php:8.2-cli')
            ->withDirectory('/opt/drupal', $drupalDir)
            ->withWorkdir('/opt/drupal/web')
            ->withMountedCache('/root/.composer/cache', dag()->cacheVolume('composer-cache'));

        // install php dependencies
        // install drupal dependencies
        $php = $php
            ->withExec(['apt-get', 'update'])
            ->withExec(['apt-get', 'install', '-y', 'git', 'libsqlite3-dev', 'libxml2-dev', 'zip'])
            ->withExec(['docker-php-ext-install', 'gd', 'pdo_mysql', 'pdo_sqlite', 'xml'])
            ->withExec(['pecl', 'install', 'xdebug'])
            ->withExec(['docker-php-ext-enable', 'xdebug'])
            ->withExec(['php', '-r', "copy('https://getcomposer.org/installer', 'composer-setup.php');"])
            ->withExec(['php', 'composer-setup.php'])
            ->withExec(['php', '-r', "unlink('composer-setup.php');"])
            ->withExec(['mv', 'composer.phar', '/usr/local/bin/composer'])
            ->withExec(['composer', 'install']);

        // bind mariadb service
        // set database url env var
        // execute tests
        // return test output
        return $php
            ->withServiceBinding('db', $this->mariadbService())
            ->withEnvVariable('SIMPLETEST_DB', 'mysql://root:secret@db/drupal')
            ->withExec(['../../vendor/bin/phpunit', '-c', 'core/phpunit.xml.dist', 'core/modules/user/tests/src/Kernel'])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Container;
import io.dagger.client.Dagger;
import io.dagger.client.Directory;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a MariaDB service
   */
  @Function
  public Service mariadbService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("mariadb:10.11.2")
          .withEnvVariable("MARIADB_ROOT_PASSWORD", "secret")
          .withEnvVariable("MARIADB_DATABASE", "drupal")
          .withExposedPort(3306)
          .asService();
    }
  }

  /**
   * Tests a Drupal application using a MariaDB service
   */
  @Function
  public String test() throws Exception {
    try (Client client = Dagger.connect()) {
      // get drupal source code
      Directory drupalDir = client
          .git("https://git.drupalcode.org/project/drupal.git")
          .branch("10.1.x")
          .tree();

      // get php container
      // mount drupal source code
      // mount composer cache
      Container php = client
          .container()
          .from("php:8.2-cli")
          .withDirectory("/opt/drupal", drupalDir)
          .withWorkdir("/opt/drupal/web")
          .withMountedCache("/root/.composer/cache", client.cacheVolume("composer-cache"));

      // install php dependencies
      // install drupal dependencies
      php = php
          .withExec(List.of("apt-get", "update"))
          .withExec(
              List.of(
                  "apt-get",
                  "install",
                  "-y",
                  "git",
                  "libsqlite3-dev",
                  "libxml2-dev",
                  "zip"))
          .withExec(
              List.of("docker-php-ext-install", "gd", "pdo_mysql", "pdo_sqlite", "xml"))
          .withExec(List.of("pecl", "install", "xdebug"))
          .withExec(List.of("docker-php-ext-enable", "xdebug"))
          .withExec(
              List.of(
                  "php",
                  "-r",
                  "copy('https://getcomposer.org/installer', 'composer-setup.php');"))
          .withExec(List.of("php", "composer-setup.php"))
          .withExec(List.of("php", "-r", "unlink('composer-setup.php');"))
          .withExec(List.of("mv", "composer.phar", "/usr/local/bin/composer"))
          .withExec(List.of("composer", "install"));

      // bind mariadb service
      // set database url env var
      // execute tests
      // return test output
      return php
          .withServiceBinding("db", this.mariadbService())
          .withEnvVariable("SIMPLETEST_DB", "mysql://root:secret@db/drupal")
          .withExec(
              List.of(
                  "../../vendor/bin/phpunit",
                  "-c",
                  "core/phpunit.xml.dist",
                  "core/modules/user/tests/src/Kernel"))
          .stdout()
          .get();
    }
  }
}

```

Here is an example call for this Dagger Function:

### System shell
```shell
dagger -c test
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
test
```

### Dagger CLI
```shell
dagger call test
```

The result will be:

```shell
PHPUnit 9.6.17 by Sebastian Bergmann and contributors.
Runtime:       PHP 8.2.5
Configuration: /opt/drupal/web/core/phpunit.xml.dist
Testing
.....................S                                            22 / 22 (100%)
Time: 00:15.806, Memory: 315.00 MB
There was 1 skipped test:

1) Drupal\Tests\pgsql\Kernel\pgsql\KernelTestBaseTest::testSetUp

This test only runs for the database driver 'pgsql'. Current database driver is 'mysql'.
/opt/drupal/web/core/tests/Drupal/KernelTests/Core/Database/DriverSpecificKernelTestBase.php:44
/opt/drupal/vendor/phpunit/phpunit/src/Framework/TestResult.php:728

OK, but incomplete, skipped, or risky tests!
Tests: 22, Assertions: 72, Skipped: 1.
```

## Reference: How service binding works in Dagger Functions

If you're not interested in what's happening in the background, you can skip this section and just trust that services are running when they need to be. If you're interested in the theory, keep reading.

Consider this example:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a Redis service
func (m *MyModule) RedisService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("redis:7.2-alpine").
		WithExposedPort(6379).
		AsService()
}

// Pings the Redis service
func (m *MyModule) Ping(ctx context.Context) (string, error) {
	// bind redis service to container
	// execute redis-cli command
	// return response
	return dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", m.RedisService(ctx)).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "ping"}).
		Stdout(ctx)
}

```

Here's what happens on the last line:

1. The client requests the `ping` container's stdout, which requires the container to run.
1. Dagger sees that the `ping` container has a service binding, `redisSrv`.
1. Dagger starts the `redisSrv` container, which recurses into this same process.
1. Dagger waits for health checks to pass against `redisSrv`.
1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def redis_service(self) -> dagger.Service:
        """Returns a Redis service"""
        return (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_exposed_port(6379)
            .as_service()
        )

    @function
    async def ping(self) -> str:
        """Pings the Redis service"""
        # bind redis service to container
        # execute redis-cli command
        # return response
        return await (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", self.redis_service())
            .with_exec(["redis-cli", "-h", "redis-srv", "ping"])
            .stdout()
        )

```

Here's what happens on the last line:

1. The client requests the `ping` container's stdout, which requires the container to run.
1. Dagger sees that the `ping` container has a service binding, `redis_srv`.
1. Dagger starts the `redis_srv` container, which recurses into this same process.
1. Dagger waits for health checks to pass against `redis_srv`.
1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.

### TypeScript

```typescript
import { dag, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a Redis service
   */
  @func()
  redisService(): Service {
    return dag
      .container()
      .from("redis:7.2-alpine")
      .withExposedPort(6379)
      .asService()
  }

  /**
   * Pings the Redis service
   */
  @func()
  async ping(): Promise<string> {
    // bind redis service to container
    // execute redis-cli command
    // return response
    return await dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", this.redisService())
      .withExec(["redis-cli", "-h", "redis-srv", "ping"])
      .stdout()
  }
}

```

Here's what happens on the last line:

1. The client requests the `ping` container's stdout, which requires the container to run.
1. Dagger sees that the `ping` container has a service binding, `redisSrv`.
1. Dagger starts the `redisSrv` container, which recurses into this same process.
1. Dagger waits for health checks to pass against `redisSrv`.
1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a Redis service
     */
    #[DaggerFunction]
    public function redisService(): Service
    {
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withExposedPort(6379)
            ->asService();
    }

    /**
     * Pings the Redis service
     */
    #[DaggerFunction]
    public function ping(): string
    {
        // bind redis service to container
        // execute redis-cli command
        // return response
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $this->redisService())
            ->withExec(['redis-cli', '-h', 'redis-srv', 'ping'])
            ->stdout();
    }
}

```

Here's what happens on the last line:

1. The client requests the `ping` container's stdout, which requires the container to run.
1. Dagger sees that the `ping` container has a service binding, `$redisSrv`.
1. Dagger starts the `$redisSrv` container, which recurses into this same process.
1. Dagger waits for health checks to pass against `$redisSrv`.
1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a Redis service
   */
  @Function
  public Service redisService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("redis:7.2-alpine")
          .withExposedPort(6379)
          .asService();
    }
  }

  /**
   * Pings the Redis service
   */
  @Function
  public String ping() throws Exception {
    try (Client client = Dagger.connect()) {
      // bind redis service to container
      // execute redis-cli command
      // return response
      return client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", this.redisService())
          .withExec(List.of("redis-cli", "-h", "redis-srv", "ping"))
          .stdout()
          .get();
    }
  }
}

```

Here's what happens on the last line:

1. The client requests the `ping` container's stdout, which requires the container to run.
1. Dagger sees that the `ping` container has a service binding, `redisSrv`.
1. Dagger starts the `redisSrv` container, which recurses into this same process.
1. Dagger waits for health checks to pass against `redisSrv`.
1. Dagger runs the `ping` container with the `redis-srv` alias magically added to `/etc/hosts`.

> **Note:**
> Dagger cancels each service run after a 10 second grace period to avoid frequent restarts, unless the explicit `Start` and `Stop` APIs are used.

Services are based on containers, but they run a little differently. Whereas regular containers in Dagger are de-duplicated across the entire Dagger Engine, service containers are only de-duplicated within a Dagger client session. This means that if you run separate Dagger sessions that use the exact same services, they will each get their own "instance" of the service. This process is carefully tuned to preserve caching at each client call-site, while prohibiting "cross-talk" from one Dagger session's client to another Dagger session's service.

Content-addressed services are very convenient. You don't have to come up with names and maintain instances of services; just use them by value. You also don't have to manage the state of the service; you can just trust that it will be running when needed and stopped when not.

> **Tip:**
> If you need multiple instances of a service, just attach something unique to each one, such as an instance ID.

Here's a more detailed client-server example of running commands against a Redis service:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a Redis service
func (m *MyModule) RedisService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("redis:7.2-alpine").
		WithExposedPort(6379).
		AsService()
}

// Sets and gets a key in the Redis service
func (m *MyModule) SetGet(ctx context.Context, key string, value string) (string, error) {
	// bind redis service to container
	// execute redis-cli command
	// return response
	return dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", m.RedisService(ctx)).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "set", key, value}).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "get", key}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def redis_service(self) -> dagger.Service:
        """Returns a Redis service"""
        return (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_exposed_port(6379)
            .as_service()
        )

    @function
    async def set_get(self, key: str, value: str) -> str:
        """Sets and gets a key in the Redis service"""
        # bind redis service to container
        # execute redis-cli command
        # return response
        return await (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", self.redis_service())
            .with_exec(["redis-cli", "-h", "redis-srv", "set", key, value])
            .with_exec(["redis-cli", "-h", "redis-srv", "get", key])
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a Redis service
   */
  @func()
  redisService(): Service {
    return dag
      .container()
      .from("redis:7.2-alpine")
      .withExposedPort(6379)
      .asService()
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @func()
  async setGet(key: string, value: string): Promise<string> {
    // bind redis service to container
    // execute redis-cli command
    // return response
    return await dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", this.redisService())
      .withExec(["redis-cli", "-h", "redis-srv", "set", key, value])
      .withExec(["redis-cli", "-h", "redis-srv", "get", key])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a Redis service
     */
    #[DaggerFunction]
    public function redisService(): Service
    {
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withExposedPort(6379)
            ->asService();
    }

    /**
     * Sets and gets a key in the Redis service
     */
    #[DaggerFunction]
    public function setGet(string $key, string $value): string
    {
        // bind redis service to container
        // execute redis-cli command
        // return response
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $this->redisService())
            ->withExec(['redis-cli', '-h', 'redis-srv', 'set', $key, $value])
            ->withExec(['redis-cli', '-h', 'redis-srv', 'get', $key])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a Redis service
   */
  @Function
  public Service redisService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("redis:7.2-alpine")
          .withExposedPort(6379)
          .asService();
    }
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @Function
  public String setGet(String key, String value) throws Exception {
    try (Client client = Dagger.connect()) {
      // bind redis service to container
      // execute redis-cli command
      // return response
      return client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", this.redisService())
          .withExec(List.of("redis-cli", "-h", "redis-srv", "set", key, value))
          .withExec(List.of("redis-cli", "-h", "redis-srv", "get", key))
          .stdout()
          .get();
    }
  }
}

```

This example relies on the 10-second grace period, which you should try to avoid. Depending on the 10-second grace period is risky because there are many factors which could cause a 10-second delay between calls to Dagger, such as excessive CPU load, high network latency between the client and Dagger, or Dagger operations that require a variable amount of time to process.

It would be better to chain both commands together, which ensures that the service stays running for both, as in the revision below:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Returns a Redis service
func (m *MyModule) RedisService(ctx context.Context) *dagger.Service {
	return dag.Container().
		From("redis:7.2-alpine").
		WithExposedPort(6379).
		AsService()
}

// Sets and gets a key in the Redis service
func (m *MyModule) SetGet(ctx context.Context, key string, value string) (string, error) {
	// bind redis service to container
	// execute redis-cli command
	// return response
	return dag.Container().
		From("redis:7.2-alpine").
		WithServiceBinding("redis-srv", m.RedisService(ctx)).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "set", key, value}).
		WithExec([]string{"redis-cli", "-h", "redis-srv", "get", key}).
		Stdout(ctx)
}

```

### Python

```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    def redis_service(self) -> dagger.Service:
        """Returns a Redis service"""
        return (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_exposed_port(6379)
            .as_service()
        )

    @function
    async def set_get(self, key: str, value: str) -> str:
        """Sets and gets a key in the Redis service"""
        # bind redis service to container
        # execute redis-cli command
        # return response
        return await (
            dag.container()
            .from_("redis:7.2-alpine")
            .with_service_binding("redis-srv", self.redis_service())
            .with_exec(["redis-cli", "-h", "redis-srv", "set", key, value])
            .with_exec(["redis-cli", "-h", "redis-srv", "get", key])
            .stdout()
        )

```

### TypeScript

```typescript
import { dag, Container, Service, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Returns a Redis service
   */
  @func()
  redisService(): Service {
    return dag
      .container()
      .from("redis:7.2-alpine")
      .withExposedPort(6379)
      .asService()
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @func()
  async setGet(key: string, value: string): Promise<string> {
    // bind redis service to container
    // execute redis-cli command
    // return response
    return await dag
      .container()
      .from("redis:7.2-alpine")
      .withServiceBinding("redis-srv", this.redisService())
      .withExec(["redis-cli", "-h", "redis-srv", "set", key, value])
      .withExec(["redis-cli", "-h", "redis-srv", "get", key])
      .stdout()
  }
}

```

### PHP

```php
<?php

declare(strict_types=1);

namespace DaggerModule;

use Dagger\Attribute\DaggerFunction;
use Dagger\Attribute\DaggerObject;
use Dagger\Client\Service;

use function Dagger\dag;

#[DaggerObject]
class MyModule
{
    /**
     * Returns a Redis service
     */
    #[DaggerFunction]
    public function redisService(): Service
    {
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withExposedPort(6379)
            ->asService();
    }

    /**
     * Sets and gets a key in the Redis service
     */
    #[DaggerFunction]
    public function setGet(string $key, string $value): string
    {
        // bind redis service to container
        // execute redis-cli command
        // return response
        return dag()
            ->container()
            ->from('redis:7.2-alpine')
            ->withServiceBinding('redis-srv', $this->redisService())
            ->withExec(['redis-cli', '-h', 'redis-srv', 'set', $key, $value])
            ->withExec(['redis-cli', '-h', 'redis-srv', 'get', $key])
            ->stdout();
    }
}

```

### Java

```java
package io.dagger.modules.mymodule;

import io.dagger.client.Client;
import io.dagger.client.Dagger;
import io.dagger.client.Service;
import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;

import java.util.List;

@Module
@Object
public class MyModule {

  /**
   * Returns a Redis service
   */
  @Function
  public Service redisService() throws Exception {
    try (Client client = Dagger.connect()) {
      return client
          .container()
          .from("redis:7.2-alpine")
          .withExposedPort(6379)
          .asService();
    }
  }

  /**
   * Sets and gets a key in the Redis service
   */
  @Function
  public String setGet(String key, String value) throws Exception {
    try (Client client = Dagger.connect()) {
      // bind redis service to container
      // execute redis-cli command
      // return response
      return client
          .container()
          .from("redis:7.2-alpine")
          .withServiceBinding("redis-srv", this.redisService())
          .withExec(List.of("redis-cli", "-h", "redis-srv", "set", key, value))
          .withExec(List.of("redis-cli", "-h", "redis-srv", "get", key))
          .stdout()
          .get();
    }
  }
}
</file>

<file path="docs/dagger.io/state.md">
---
slug: /api/state
---

# State and Getters

Object state can be exposed as a Dagger Function, without having to create a getter function explicitly. Depending on the language used, this state is exposed using struct fields (Go), object attributes (Python) or object properties (TypeScript).

### Go
Dagger only exposes a struct's public fields; private fields will not be exposed.

Here's an example where one struct field is exposed as a Dagger Function, while the other is not:

```go
package main

import (
	"fmt"
)

type MyModule struct {
	// The greeting to use
	Greeting string
	// Who to greet
	name string
}

func New(
	// +optional
	// +default="Hello"
	greeting string,
	// +optional
	// +default="World"
	name string,
) *MyModule {
	return &MyModule{
		Greeting: greeting,
		name:     name,
	}
}

// Return the greeting message
func (m *MyModule) Message() string {
	return fmt.Sprintf("%s, %s!", m.Greeting, m.name)
}

```

### Python
The [`dagger.field`](https://dagger-io.readthedocs.io/en/latest/module.html#dagger.field) descriptor is a wrapper of
[`dataclasses.field`](https://docs.python.org/3/library/dataclasses.html#mutable-default-values). It creates a getter function for the attribute as well so that it's accessible from the Dagger API.

Here's an example where one attribute is exposed as a Dagger Function, while the other is not:

```python
from typing import Annotated

import dagger
from dagger import Doc, field, function, object_type


@object_type
class MyModule:
    greeting: Annotated[str, Doc("The greeting to use")] = field(default="Hello")
    name: str = field(default="World", init=False)

    def __init__(self, name: str = "World"):
        self.name = name

    @function
    def message(self) -> str:
        """Return the greeting message"""
        return f"{self.greeting}, {self.name}!"

```

Notice that compared to [`dataclasses.field`](https://docs.python.org/3/library/dataclasses.html#mutable-default-values), the [`dagger.field`](https://dagger-io.readthedocs.io/en/latest/module.html#dagger.field) wrapper only supports setting `init: bool`, and both `default` and `default_factory` in the same `default` parameter.

> **Note:**
> In a future version of the Python SDK, the `dagger.function` decorator will be used as a descriptor in place of `dagger.field` to make the distinction clearer.

### TypeScript
TypeScript already offers `private`, `protected` and `public` keywords to handle member visibility in a class. However, Dagger will only expose those members of a Dagger module that are explicitly decorated with the `@func()` decorator. Others will remain private.

Here's an example where one field is exposed as a Dagger Function, while the other is not:

```typescript
import { field, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * The greeting to use
   *
   * @default "Hello"
   */
  @field()
  greeting = "Hello"

  /**
   * Who to greet
   *
   * @default "World"
   */
  name = "World"

  constructor(name?: string, greeting?: string) {
    if (name) {
      this.name = name
    }
    if (greeting) {
      this.greeting = greeting
    }
  }

  /**
   * Return the greeting message
   */
  @func()
  message(): string {
    return `${this.greeting}, ${this.name}!`
  }
}

```

### Java
Dagger will automatically expose all public fields of a class as Dagger Functions. It's also possible to expose a package, `protected` or `private` field by annotating it with the `@Function` annotation.

In case of a field that shouldn't be serialized at all, this can be achieved by marking it as `transient` in Java.

Here's an example where one field is exposed as a Dagger Function, while the other is not:

```java
package io.dagger.modules.mymodule;

import io.dagger.module.annotation.Module;
import io.dagger.module.annotation.Object;
import io.dagger.module.annotation.Function;
import io.dagger.module.annotation.Description;
import io.dagger.module.annotation.Default;

@Module
@Object
public class MyModule {

  @Description("The greeting to use")
  @Default("\"Hello\"")
  public String greeting;

  @Description("Who to greet")
  @Default("\"World\"")
  private String name;

  public MyModule() {}

  public MyModule(String greeting, String name) {
    this.greeting = greeting;
    this.name = name;
  }

  /**
   * Return the greeting message
   */
  @Function
  public String message() {
    return String.format("%s, %s!", this.greeting, this.name);
  }
}

```

Confirm with `dagger call --help` or `.help my-module` that only the `greeting` function was created, with `name` remaining only a constructor argument:

```
FUNCTIONS
  greeting      The greeting to use
  message       Return the greeting message

ARGUMENTS
      --greeting string   The greeting to use (default "Hello")
      --name string       Who to greet (default "World")
</file>

<file path="docs/dagger.io/terminal.md">
---
slug: /api/terminal
---

# Interactive Terminal

Dagger provides an interactive terminal that can help greatly when trying to debug a pipeline failure.

To use this, set one or more explicit breakpoints in your Dagger pipeline with the `Container.terminal()` method. Dagger then starts an interactive terminal session at each breakpoint. This lets you inspect a `Directory` or a `Container` at any point in your pipeline run, with all the necessary context available to you.

Here is a simple example, which opens an interactive terminal in an `alpine` container:

### System shell
```shell
dagger -c 'container | from alpine | terminal'
```

### Dagger Shell
```shell title="First type 'dagger' for interactive mode."
container | from alpine | terminal
```

### Dagger CLI
```shell
dagger core container from --address=alpine terminal
```

Here is an example of a Dagger Function which opens an interactive terminal at two different points in the Dagger pipeline to inspect the built container:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Build a container and open a terminal at two different points
func (m *MyModule) Build(ctx context.Context) (*dagger.Container, error) {
	ctr := dag.Container().From("alpine:latest")

	// open terminal before adding packages
	ctr = ctr.Terminal()

	// add packages
	ctr = ctr.WithExec([]string{"apk", "add", "curl"})

	// open terminal after adding packages
	ctr = ctr.Terminal()

	return ctr, nil
}

```

### Python
```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def build(self) -> dagger.Container:
        """Build a container and open a terminal at two different points"""
        ctr = dag.container().from_("alpine:latest")

        # open terminal before adding packages
        ctr = await ctr.terminal()

        # add packages
        ctr = ctr.with_exec(["apk", "add", "curl"])

        # open terminal after adding packages
        ctr = await ctr.terminal()

        return ctr

```

### TypeScript

```typescript
import { dag, Container, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Build a container and open a terminal at two different points
   */
  @func()
  async build(): Promise<Container> {
    let ctr = dag.container().from("alpine:latest")

    // open terminal before adding packages
    ctr = await ctr.terminal()

    // add packages
    ctr = ctr.withExec(["apk", "add", "curl"])

    // open terminal after adding packages
    ctr = await ctr.terminal()

    return ctr
  }
}

```

The `Container.terminal()` method can be chained. It returns a `Container`, so it can be injected at any point in a pipeline (in this example, between `Container.from()` and `Container.withExec()` methods).

> **Tip:**
> Multiple terminals are supported in the same Dagger Function; they will open in sequence.

It's also possible to inspect a directory using the `Container.terminal()` method. Here is an example of a Dagger Function which opens an interactive terminal on a directory:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Open a terminal on a directory
func (m *MyModule) DebugDir(ctx context.Context, dir *dagger.Directory) (*dagger.Container, error) {
	return dir.Terminal(ctx)
}

```

### Python
```python
import dagger
from dagger import function, object_type


@object_type
class MyModule:
    @function
    async def debug_dir(self, dir: dagger.Directory) -> dagger.Container:
        """Open a terminal on a directory"""
        return await dir.terminal()

```

### TypeScript

```typescript
import { Directory, Container, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Open a terminal on a directory
   */
  @func()
  async debugDir(dir: Directory): Promise<Container> {
    return await dir.terminal()
  }
}

```

Under the hood, this creates a new container (defaults to `alpine`) and starts a shell, mounting the directory inside. This container can be customized using additional options. Here is a more complex example, which produces the same result as the previous one but this time using an `ubuntu` container image and `bash` shell instead of the default `alpine` container image and `sh` shell:

### Go

```go
package main

import (
	"context"

	"dagger.io/dagger"
)

type MyModule struct{}

// Open a terminal on a directory using a custom container and shell
func (m *MyModule) DebugDirCustom(ctx context.Context, dir *dagger.Directory) (*dagger.Container, error) {
	return dir.Terminal(ctx, dagger.DirectoryTerminalOpts{
		Cmd:       []string{"bash"},
		Container: dag.Container().From("ubuntu:latest"),
	})
}

```

### Python
```python
import dagger
from dagger import dag, function, object_type


@object_type
class MyModule:
    @function
    async def debug_dir_custom(self, dir: dagger.Directory) -> dagger.Container:
        """Open a terminal on a directory using a custom container and shell"""
        return await dir.terminal(
            cmd=["bash"],
            container=dag.container().from_("ubuntu:latest"),
        )

```

### TypeScript

```typescript
import { dag, Directory, Container, func, object } from "@dagger.io/dagger"

@object()
class MyModule {
  /**
   * Open a terminal on a directory using a custom container and shell
   */
  @func()
  async debugDirCustom(dir: Directory): Promise<Container> {
    return await dir.terminal({
      cmd: ["bash"],
      container: dag.container().from("ubuntu:latest"),
    })
  }
}
</file>

<file path="docs/dagger.io/troubleshooting.md">
---
slug: /troubleshooting
---

# Troubleshooting

This page describes problems you may encounter when using Dagger, and their solutions.

## Dagger is unresponsive with a BuildKit error

A Dagger Function may hang or become unresponsive, eventually generating a BuildKit error such as `buildkit failed to respond` or `container state improper`.

To resolve this error, you must stop and remove the Dagger Engine container and (optionally) clear the container state.

1. Stop and remove the Dagger Engine container:

   ```shell
   DAGGER_ENGINE_DOCKER_CONTAINER="$(docker container list --all --filter 'name=^dagger-engine-*' --format '{{.Names}}')"
   docker container stop "$DAGGER_ENGINE_DOCKER_CONTAINER"
   docker container rm "$DAGGER_ENGINE_DOCKER_CONTAINER"
   ```

1. Clear unused volumes and data:

   > **Info:**
   > This step is optional. It will remove the cache and result in a slow first run when the container is re-provisioned.

   ```shell
   docker volume prune
   docker system prune
   ```

You should now be able to run your Dagger Function successfully.

> **Note:**
> If you have custom-provisioned the Dagger Engine, please adjust the above commands to your environment.

## Dagger is unable to resolve host names after network configuration changes

If the network configuration of the host changes after the Dagger Engine container starts, Docker does not notify the Dagger Engine of the change. This may cause Dagger to fail with network-related errors.

As an example, if the nameserver configuration of the host changes after switching to a different network connection or connecting/disconnecting a VPN result, Dagger may fail with DNS resolution errors.

To resolve this error, you must restart the Dagger Engine container after the host network configuration changes.

```shell
DAGGER_ENGINE_DOCKER_CONTAINER="$(docker container list --all --filter 'name=^dagger-engine-*' --format '{{.Names}}')"
docker restart "$DAGGER_ENGINE_DOCKER_CONTAINER"
```

You should now be able to re-run your Dagger Function successfully.

## Dagger restarts with a "CNI setup error"

The Dagger Engine requires the `iptable_nat` Linux kernel module in order to function properly. On some Linux distributions this module is not loaded by default.

Known affected platforms include Red Hat Enterprise Linux (8.x and 9.x) and Podman Desktop on Mac.

You can load this module by running `sudo modprobe iptable_nat`.

To have this module loaded automatically on startup, add it to the `/etc/modules-load.d/modules` file with the following command:

```shell
echo iptable_nat | sudo tee -a /etc/modules-load.d/modules
```

## Calling a Dagger Function fails with an error

### Errors related to code generation

A Dagger Function may fail with one of the following errors and/or cause the Dagger Engine to crash:

- `unable to start container process`
- `failed to update codegen and runtime`
- `failed to generate code`
- `failed to get modified source directory for go module sdk codegen`

This can occur when you have the `DOCKER_DEFAULT_PLATFORM` environment variable set and/or when Rosetta is enabled in Docker Desktop for Mac.

To resolve this error, you must remove the environment variable, disable Rosetta if applicable, and remove existing Dagger Engine containers.

1. Remove the `DOCKER_DEFAULT_PLATFORM` variable in your current shell and/or your equivalent shell config files (`.bashrc`, `.profile`, `.zshrc`, ...) and restart the shell.
1. Ensure that [Rosetta is disabled in Docker Desktop on Mac](https://docs.docker.com/desktop/settings/mac/).
1. Remove any running Dagger Engine containers and Docker images:

    ```shell
    docker rm -fv $(docker ps --filter name="dagger-engine-*" -q) && docker rmi $(docker images -q --filter reference=registry.dagger.io/engine)
    ```

### An agent isn't behaving properly

First, to get more information about the agent's behavior, increase the verbosity of the shell or review the trace in Dagger Cloud. This is the best way to see what steps the LLM took, the details of its thinking between each tool call, and what the tool calls were.

Second, once you understand how the agent is behaving, you can try to adjust the prompt to get the desired behavior. This is often a matter of trial and error, and can be a bit of an art.

### Other errors

To troubleshoot other Dagger Function errors, try the following techniques.

#### Rerun commands with `--interactive`

Run `dagger call` with the `--interactive` (`-i` for short) flag to open a terminal in the context of a pipeline failure. No changes are required to your Dagger Function code.

> **Tip:**
> Interactive mode defaults to executing `/bin/sh` when opening a terminal. Change the command to execute with the `--interactive-command` flag.

#### Rerun commands with `--debug`

The Dagger CLI tries to keep its output concise by default. If you're running
into issues and want to debug with more detailed output, you can run any `dagger`
subcommand with the `--debug` flag to have it reveal all available information.

#### Access the Dagger Engine logs

The Dagger Engine runs in a dedicated Docker container and emits log messages as it works. Here's how to access these logs:

```shell
DAGGER_ENGINE_DOCKER_CONTAINER="$(docker container list --all --filter 'name=^dagger-engine-*' --format '{{.Names}}')"
docker logs $DAGGER_ENGINE_DOCKER_CONTAINER
```

#### Enable SDK debug logs

> **Important:**
> The information in this section is only applicable to the Python SDK. Debug logs are not currently available for the Go and TypeScript SDKs.

The Python SDK prints debugging information at various steps of the execution
flow of a module, which can be very useful in understanding what's being
received from and sent to the API.

This is done using standard Python [logging](https://docs.python.org/3/howto/logging.html),
so it's highly configurable (for example, saving to a file or sending to an
external system like Sentry). But for a simple lowering of the default logging
level to [logging.DEBUG](https://docs.python.org/3/library/logging.html#logging.DEBUG),
there's a convenience function for that:

```python
import logging

from dagger import function, object_type
from dagger.log import configure_logging

configure_logging(logging.DEBUG)


@object_type
class MyModule:
    @function
    def echo(self, msg: str) -> str:
        return msg
```

> **Important:**
> With the TUI, you need to use a progress output that doesn't collapse on success
> like `--progress=plain` or `--debug`, otherwise it won't show in the terminal.

Using the command `dagger call --debug echo --msg="hello"`, you should see a large number of debug messages, eventually ending with output similar to the below:

```
  ‚úî connect 0.1s
‚úî Debug.echo(msg: "hello"): String! 0.9s
‚îÉ [DEBUG] dagger.mod._resolver: func => <Signature (msg: str) -> str>
‚îÉ [DEBUG] dagger.mod._resolver: input args => {'msg': 'hello'}
‚îÉ [DEBUG] dagger.mod._resolver: structured args => {'msg': 'hello'}
‚îÉ [DEBUG] dagger.mod._module: result => 'hello'
‚îÉ [DEBUG] dagger.mod._module: output => '"hello"'
‚îÉ [DEBUG] dagger.client._session: Closing client session to GraphQL server

hello
```

The above gives a lot of useful information:
- The function and parent object that the API wants to execute
- The parent object's state
- The function's signature
- The user inputs before and after deserialization
- The user inputs after being converted to more complex types (structuring)
- The function's result before and after serialization
</file>

<file path="docs/dagger.io/types.md">
---
slug: /api/types
---

# Core Types

In addition to basic types (string, boolean, integer, arrays...), the Dagger API also provides powerful core types which serve as both arguments and return values for Dagger Functions.

> **Note:**
> The types listed on this page are indicative and not exhaustive. For a complete list of supported types and their fields, refer to the [Dagger API reference](https://docs.dagger.io/api/reference).

The following table lists available types and what they represent:

| Type | Description |
|------|-------------|
| `CacheVolume` | A directory whose contents persist across runs |
| `Container` | An OCI-compatible container |
| `CurrentModule` | The current Dagger module and its context |
| `Engine` | The Dagger Engine configuration and state |
| `Directory` | A directory (local path or Git reference) |
| `EnvVariable` | An environment variable name and value |
| `File` | A file |
| `GitRepository` | A Git repository |
| `GitRef` | A Git reference (tag, branch, or commit) |
| `Host` | The Dagger host environment |
| `LLM` | A Large Language Model (LLM) |
| `Module` | A Dagger module |
| `Port` | A port exposed by a container |
| `Secret` | A secret credential like a password, access token or key) |
| `Service` | A content-addressed service providing TCP connectivity |
| `Socket` | A Unix or TCP/IP socket that can be mounted into a container |
| `Terminal` | An interactive terminal session |

Each type exposes additional fields. Some of these are discussed below.

## Container

The `Container` type represents the state of an OCI-compatible container. This `Container` object is not merely a string referencing an image on a remote registry. It is the actual state of a container, managed by the Dagger Engine, and passed to a Dagger Function's code as if it were just another variable.

Some of the `Container` type's important fields are:

| Field | Description |
|-------|-------------|
| `from` | Initializes the container from a specified base image |
| `asService` | Turns the container into a `Service` |
| `asTarball` | Returns a serialized tarball of the container as a `File` |
| `export` / `import` | Writes / reads the container as an OCI tarball to / from a file path on the host |
| `publish` | Publishes the container image to a registry |
| `stdout` / `stderr` | Returns the output / error stream of the last executed command |
| `withDirectory` / `withMountedDirectory` | Returns the container plus a directory copied / mounted at the given path |
| `withEntrypoint` | Returns the container with a custom entrypoint command |
| `withExec` | Returns the container after executing a command inside it |
| `withFile` / `withMountedFile` | Returns the container plus a file copied / mounted at the given path |
| `withMountedCache` | Returns the container plus a cache volume mounted at the given path |
| `withRegistryAuth` | Returns the container with registry authentication configured |
| `withWorkdir` | Returns the container configured with a specific working directory |
| `withServiceBinding` | Returns the container with runtime dependency on another `Service` |

## Directory

The `Directory` type represents the state of a directory. This could be either a local directory path or a remote Git reference. Some of its important fields are:

| Field | Description |
|-------|-------------|
| `dockerBuild` | Builds a new Docker container from the directory |
| `entries` | Returns a list of files and directories in the directory |
| `export` | Writes the contents of the directory to a path on the host |
| `file` | Returns a file at the given path as a `File`  |
| `withFile` / `withFiles` | Returns the directory plus the file(s) copied to the given path |

## File

The `File` type represents a single file. Some of its important fields are:

| Field | Description |
|-------|-------------|
| `contents` | Returns the contents of the file |
| `export` | Writes the file to a path on the host |

## Service

The `Service` type represents a content-addressed service providing TCP connectivity. Some of its important fields are:

| Field | Description |
|-------|-------------|
| `endpoint` | Returns a URL or host:port pair to reach the service |
| `hostname` | Returns a hostname to reach the service |
| `ports` | Returns the list of ports provided by the service |
| `up` | Creates a tunnel that forwards traffic from the caller's network to the service  |

## Secret

Dagger allows you to utilize confidential information ("secrets") such as passwords, API keys, SSH keys and so on, without exposing those secrets in plaintext logs, writing them into the filesystem of containers you're building, or inserting them into the cache. The `Secret` type is used to represent these secret values. Some of its important fields are:

| Field | Description |
|-------|-------------|
| `name` | Returns the name of the secret |
| `plaintext` | Returns the plaintext value of the secret |

## LLM

The `LLM` type initializes a Large Language Model (LLM). Some of its important fields are:

| Field | Description |
|-------|-------------|
| `model` | Returns the model used by the LLM |
| `lastReply` | Returns the last reply from the LLM |
| `history` | Returns the LLM message history |
| `withPrompt` | Appends a prompt to the LLM context |
| `withPromptFile` | Appends a prompt file to the LLM context |
| `withPromptVar` | Adds a string variable to the LLM context |
| `withModel` | Sets the model used by the LLM |

## CurrentModule

The `CurrentModule` type provides capabilities to introspect the Dagger Function's module and interface between the current execution environment and the Dagger API. Some of its important fields are:

| Field | Description |
|-------|-------------|
| `source` | Returns the directory containing the module's source code |
| `workdir` | Loads and returns a directory from the module's working directory, including any changes that may have been made to it during function execution |
| `workdirFile` | Loads and returns a file from the module's working directory, including any changes that may have been made to it during function execution |
</file>

<file path="docs/aws-remote-backend-setup.md">
# AWS Remote Backend Setup

## Overview

This guide explains how to set up a secure AWS-based remote backend for Terraform and Terragrunt state management. A properly configured remote backend provides:

- **State Locking**: Prevents concurrent state modifications that could corrupt your infrastructure state
- **State Versioning**: Maintains a history of your state files for auditing and recovery
- **Encryption**: Ensures sensitive information in your state files is protected
- **Access Control**: Centralizes and secures access to infrastructure state

## Components

The AWS remote backend consists of two primary components:

1. **S3 Bucket**: Stores the Terraform state files
2. **DynamoDB Table**: Provides state locking to prevent concurrent modifications

## Prerequisites

- AWS CLI installed and configured with appropriate credentials
- Permissions to create and configure S3 buckets and DynamoDB tables

## Setup Instructions

### Option 1: One-Step Setup (Recommended)

The following command will create and configure all required resources with best security practices:

```bash
aws s3api create-bucket \
    --bucket terraform-state-makemyinfra \
    --region us-east-1 && \
aws s3api put-bucket-versioning \
    --bucket terraform-state-makemyinfra \
    --versioning-configuration Status=Enabled && \
aws s3api put-bucket-encryption \
    --bucket terraform-state-makemyinfra \
    --server-side-encryption-configuration '{"Rules": [{"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}]}' && \
aws s3api put-public-access-block \
    --bucket terraform-state-makemyinfra \
    --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true" && \
aws dynamodb create-table \
    --table-name terraform-state-makemyinfra \
    --region us-east-1 \
    --billing-mode PAY_PER_REQUEST \
    --attribute-definitions AttributeName=LockID,AttributeType=S \
    --key-schema AttributeName=LockID,KeyType=HASH
```

You may need to modify the bucket name and region to fit your requirements.

### Option 2: Step-by-Step Setup

If you prefer to understand and execute each step individually:

#### 1. Create the S3 Bucket

```bash
aws s3api create-bucket \
    --bucket terraform-state-makemyinfra \
    --region us-east-1
```

For regions other than `us-east-1`, use:

```bash
aws s3api create-bucket \
    --bucket terraform-state-makemyinfra \
    --region your-region \
    --create-bucket-configuration LocationConstraint=your-region
```

#### 2. Enable Bucket Versioning

```bash
aws s3api put-bucket-versioning \
    --bucket terraform-state-makemyinfra \
    --versioning-configuration Status=Enabled
```

#### 3. Enable Server-Side Encryption

```bash
aws s3api put-bucket-encryption \
    --bucket terraform-state-makemyinfra \
    --server-side-encryption-configuration '{"Rules": [{"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}]}'
```

#### 4. Block Public Access

```bash
aws s3api put-public-access-block \
    --bucket terraform-state-makemyinfra \
    --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
```

#### 5. Create DynamoDB Table for State Locking

```bash
aws dynamodb create-table \
    --table-name terraform-state-makemyinfra \
    --region us-east-1 \
    --billing-mode PAY_PER_REQUEST \
    --attribute-definitions AttributeName=LockID,AttributeType=S \
    --key-schema AttributeName=LockID,KeyType=HASH
```

## Configuration in Terragrunt

Once the backend is created, update your environment configuration to reference it. The reference architecture already includes functionality to use the backend specified in your environment variables.

In your `.env` or `.envrc` file:

```bash
# Remote State Configuration
TG_STACK_REMOTE_STATE_BUCKET_NAME="terraform-state-makemyinfra"
TG_STACK_REMOTE_STATE_LOCK_TABLE="terraform-state-makemyinfra"
TG_STACK_REMOTE_STATE_REGION="us-east-1"
```

## Security Best Practices

- **IAM Policies**: Restrict access to the S3 bucket and DynamoDB table to only authorized users/roles
- **Access Logging**: Enable S3 access logging to monitor bucket access
- **Lifecycle Policies**: Consider adding lifecycle policies to manage old state versions
- **HTTPS Only**: Enforce HTTPS-only communication with the S3 bucket

### Sample IAM Policy for Terraform/Terragrunt Access

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject"
      ],
      "Resource": [
        "arn:aws:s3:::terraform-state-makemyinfra",
        "arn:aws:s3:::terraform-state-makemyinfra/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "dynamodb:GetItem",
        "dynamodb:PutItem",
        "dynamodb:DeleteItem"
      ],
      "Resource": "arn:aws:dynamodb:*:*:table/terraform-state-makemyinfra"
    }
  ]
}
```

## Troubleshooting

### Common Issues

1. **Access Denied**
   - Verify AWS credentials have appropriate permissions
   - Check IAM policies attached to your user/role

2. **Bucket Already Exists**
   - S3 bucket names must be globally unique
   - Choose a different bucket name or use an existing bucket

3. **State Locking Failures**
   - Ensure DynamoDB table exists and is correctly named
   - Verify permissions include DynamoDB actions
   - Check for stale locks with `terragrunt force-unlock`

4. **Region Consistency**
   - Ensure S3 bucket and DynamoDB table are in the same AWS region
   - Verify `TG_STACK_REMOTE_STATE_REGION` matches actual resource region

## Cleanup

If you need to remove the backend infrastructure:

```bash
# First, remove all files from the bucket
aws s3 rm s3://terraform-state-makemyinfra --recursive

# Delete the bucket
aws s3api delete-bucket --bucket terraform-state-makemyinfra

# Delete the DynamoDB table
aws dynamodb delete-table --table-name terraform-state-makemyinfra
```

‚ö†Ô∏è **WARNING**: Deleting the remote backend will remove all Terraform state files, which could make managing existing infrastructure extremely difficult. Only do this if you're sure you want to completely reset your infrastructure management.
</file>

<file path="infra/terraform/modules/age-generator/locals.tf">
locals {
  # No complex local computations needed at this time
  # Ensure min_age is less than max_age
  validated_min_age = var.min_age < var.max_age ? var.min_age : var.max_age
  validated_max_age = var.max_age > var.min_age ? var.max_age : var.min_age
}
</file>

<file path="infra/terraform/modules/age-generator/main.tf">
resource "random_integer" "age" {
  min = local.validated_min_age
  max = local.validated_max_age
}
</file>

<file path="infra/terraform/modules/age-generator/outputs.tf">
output "generated_age" {
  description = "Generated age within specified range"
  value       = random_integer.age.result
}

output "min_age" {
  description = "Minimum age used for generation"
  value       = local.validated_min_age
}

output "max_age" {
  description = "Maximum age used for generation"
  value       = local.validated_max_age
}
</file>

<file path="infra/terraform/modules/age-generator/variables.tf">
variable "min_age" {
  description = "Minimum age for generation"
  type        = number
  default     = 18
  validation {
    condition     = var.min_age >= 0 && var.min_age <= 100
    error_message = "Minimum age must be between 0 and 100."
  }
}

variable "max_age" {
  description = "Maximum age for generation"
  type        = number
  default     = 65
  validation {
    condition     = var.max_age >= 18 && var.max_age <= 100
    error_message = "Maximum age must be between 18 and 100."
  }
}
</file>

<file path="infra/terraform/modules/dni-generator/locals.tf">
locals {
  # DNI control letter calculation
  dni_control_letters = ["T", "R", "W", "A", "G", "M", "Y", "F", "P", "D", "X", "B", "N", "J", "Z", "S", "Q", "V", "H", "L", "C", "K", "E"]

  # Generate a deterministic prefix based on name, lastname, and age
  name_hash = substr(md5("${var.name}${var.lastname}"), 0, 4)
  age_hash = substr(md5("${var.age}"), 0, 4)
  prefix = substr("${local.name_hash}${local.age_hash}", 0, 6)

  base_dni_number = format("%08d", random_integer.dni_number.result)
  control_letter = local.dni_control_letters[tonumber(local.base_dni_number) % 23]
  full_dni = "${local.base_dni_number}${local.control_letter}"
  full_name = "${var.name} ${var.lastname}"

  # DNI generation logic based on country
  spain_dni_prefix = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
  argentina_dni_prefix = ["1", "2", "3", "4", "5", "6", "7", "8", "9"]
  mexico_dni_prefix = ["1", "2", "3", "4", "5", "6", "7", "8", "9"]

  # Select DNI prefix based on country
  available_dni_prefixes = var.country == "Spain" ? local.spain_dni_prefix : var.country == "Argentina" ? local.argentina_dni_prefix : local.mexico_dni_prefix
}
</file>

<file path="infra/terraform/modules/dni-generator/main.tf">
resource "random_string" "dni_prefix" {
  length  = 1
  special = false
  upper   = false
  numeric = true
}

resource "random_shuffle" "dni_prefix" {
  input        = local.available_dni_prefixes
  result_count = 1
}

resource "random_integer" "dni_number" {
  min = 10000000
  max = 99999999
}
</file>

<file path="infra/terraform/modules/dni-generator/outputs.tf">
output "dni_prefix" {
  description = "Randomly generated DNI prefix"
  value       = random_shuffle.dni_prefix.result[0]
}

output "dni_number" {
  description = "Randomly generated DNI number"
  value       = random_integer.dni_number.result
}

output "full_dni" {
  description = "Complete DNI with prefix and number"
  value       = "${random_shuffle.dni_prefix.result[0]}${random_integer.dni_number.result}"
}

output "country" {
  description = "Country of DNI generation"
  value       = var.country
}

output "full_name" {
  description = "Full name of the generated citizen"
  value       = local.full_name
}

output "age" {
  description = "Age of the generated citizen"
  value       = var.age
}
</file>

<file path="infra/terraform/modules/dni-generator/variables.tf">
variable "prefix" {
  description = "Optional prefix for the DNI number (first digits)"
  type        = string
  default     = ""
  validation {
    condition     = can(regex("^\\d{0,8}$", var.prefix))
    error_message = "Prefix must be a string of digits with a maximum length of 8."
  }
}

variable "generate_control_letter" {
  description = "Whether to generate the control letter"
  type        = bool
  default     = true
}

variable "name" {
  description = "First name for DNI generation"
  type        = string
}

variable "lastname" {
  description = "Last name for DNI generation"
  type        = string
}

variable "age" {
  description = "Age for DNI generation"
  type        = number
}

variable "country" {
  description = "Country for DNI generation"
  type        = string
  default     = "Spain"
  validation {
    condition     = contains(["Spain", "Argentina", "Mexico"], var.country)
    error_message = "Country must be 'Spain', 'Argentina', or 'Mexico'."
  }
}

variable "tags" {
  description = "A map of tags to add to all resources"
  type        = map(string)
  default     = {}
}
</file>

<file path="infra/terraform/modules/lastname-generator/locals.tf">
locals {
  # Predefined lastname lists with gender-specific variations
  male_lastnames = ["Garcia", "Rodriguez", "Martinez", "Lopez", "Gonzalez"]
  female_lastnames = ["Garcia", "Rodriguez", "Martinez", "Lopez", "Gonzalez"]

  # Combine lastnames based on gender
  available_lastnames = var.gender == "male" ? local.male_lastnames : var.gender == "female" ? local.female_lastnames : concat(local.male_lastnames, local.female_lastnames)
}
</file>

<file path="infra/terraform/modules/lastname-generator/main.tf">
resource "random_string" "lastname_suffix" {
  length  = 4
  special = false
  upper   = false
}

resource "random_shuffle" "lastname" {
  input        = local.available_lastnames
  result_count = 1
}
</file>

<file path="infra/terraform/modules/lastname-generator/outputs.tf">
output "lastname" {
  description = "Randomly generated lastname"
  value       = random_shuffle.lastname.result[0]
}

output "full_lastname" {
  description = "Generated lastname with random suffix"
  value       = "${var.input_lastname}-${random_string.lastname_suffix.result}"
}

output "suffix" {
  description = "Generated random suffix"
  value       = random_string.lastname_suffix.result
}

output "gender" {
  description = "Gender of the generated lastname"
  value       = var.gender
}
</file>

<file path="infra/terraform/modules/lastname-generator/variables.tf">
variable "input_lastname" {
  description = "Base lastname to concatenate with random string"
  type        = string
  validation {
    condition     = length(var.input_lastname) > 0
    error_message = "Input lastname must not be empty."
  }
}

variable "suffix_length" {
  description = "Length of the random suffix"
  type        = number
  default     = 6
  validation {
    condition     = var.suffix_length > 0 && var.suffix_length <= 16
    error_message = "Suffix length must be between 1 and 16 characters."
  }
}

variable "gender" {
  description = "Gender for lastname generation"
  type        = string
  default     = "any"
  validation {
    condition     = contains(["male", "female", "any"], var.gender)
    error_message = "Gender must be 'male', 'female', or 'any'."
  }
}
</file>

<file path="infra/terraform/modules/name-generator/locals.tf">
locals {
  # Predefined name lists with gender-specific variations
  male_names = ["Juan", "Carlos", "Miguel", "Pedro", "Luis"]
  female_names = ["Maria", "Ana", "Carmen", "Sofia", "Elena"]

  # Combine names based on gender
  available_names = var.gender == "male" ? local.male_names : var.gender == "female" ? local.female_names : concat(local.male_names, local.female_names)
}
</file>

<file path="infra/terraform/modules/name-generator/main.tf">
# Generate random suffix for the name
resource "random_string" "name_suffix" {
  length  = var.suffix_length
  special = false
  upper   = false
}

# Randomly select a name from the available names list
resource "random_shuffle" "name" {
  input        = local.available_names
  result_count = 1
}
</file>

<file path="infra/terraform/modules/name-generator/outputs.tf">
output "name" {
  description = "Randomly generated name"
  value       = random_shuffle.name.result[0]
}

output "full_name" {
  description = "Generated name with random suffix"
  value       = "${var.input_name}-${random_string.name_suffix.result}"
}

output "suffix" {
  description = "Generated random suffix"
  value       = random_string.name_suffix.result
}

output "gender" {
  description = "Gender of the generated name"
  value       = var.gender
}
</file>

<file path="infra/terraform/modules/name-generator/variables.tf">
variable "input_name" {
  description = "Base name to concatenate with random string"
  type        = string
  validation {
    condition     = length(var.input_name) > 0
    error_message = "Input name must not be empty."
  }
}

variable "suffix_length" {
  description = "Length of the random suffix"
  type        = number
  default     = 6
  validation {
    condition     = var.suffix_length > 0 && var.suffix_length <= 16
    error_message = "Suffix length must be between 1 and 16 characters."
  }
}

variable "gender" {
  description = "Gender for name generation"
  type        = string
  default     = "any"
  validation {
    condition     = contains(["male", "female", "any"], var.gender)
    error_message = "Gender must be 'male', 'female', or 'any'."
  }
}
</file>

<file path="infra/terraform/modules/README.md">
# Terraform Modules

## Overview

This directory contains a collection of modular Terraform components designed to provide a flexible, reusable, and scalable infrastructure-as-code (IaC) approach within our Terragrunt-based reference architecture.

## Architecture Philosophy

The modules in this directory embody key principles of modern infrastructure design:

- **Modularity**: Each module represents a discrete, self-contained infrastructure component
- **Reusability**: Modules are crafted to be environment-agnostic and easily composable
- **Flexibility**: Supports multiple sourcing strategies for enhanced development and deployment workflows

## Module Sourcing Strategies

The architecture supports multiple module sourcing mechanisms, as demonstrated in the `terragrunt.hcl` configuration:

```hcl
locals {
  tf_module_local_path       = "${get_repo_root()}/infra/terraform/modules/dni-generator"
  tf_module_version_override = ""
  tf_module_version          = local.tf_module_version_override != "" ? local.tf_module_version_override : include.shared.locals.tf_module_version_default
  tf_module_source           = include.shared.locals.tf_module_source
}

terraform {
  source = local.tf_module_local_path != "" ? local.tf_module_local_path : format("%s?ref=%s", local.tf_module_source, local.tf_module_version)
}
```

### Sourcing Mechanisms

1. **Local Development Path**

   - During development, modules can be sourced directly from the local filesystem
   - Enables rapid iteration and testing without pushing changes to a remote repository
   - Set `tf_module_local_path` to the local module directory

2. **Version-Controlled Remote Source**

   - Modules can be sourced from a remote repository with specific version references
   - Supports consistent, reproducible infrastructure deployments
   - Version controlled through `tf_module_version`

3. **Fallback Mechanism**
   - Intelligent fallback to default module source if no local path is specified
   - Ensures flexibility across different development and deployment environments

## Module Structure

Each module typically contains:

- `main.tf`: Primary resource definitions
- `variables.tf`: Input variable declarations
- `outputs.tf`: Module output definitions
- `locals.tf`: Local value computations
- `versions.tf`: Provider and Terraform version constraints

## Best Practices

- **Minimal Complexity**: Each module should have a single, well-defined responsibility
- **Parameterization**: Maximize configurability through input variables
- **Consistent Naming**: Use clear, descriptive names that reflect the module's purpose
- **Documentation**: Maintain comprehensive inline documentation

## Example Module Usage

```hcl
module "example_generator" {
  source = "path/to/module"

  # Module-specific input variables
  name        = var.name
  environment = var.environment
}
```

## Integration with Terragrunt

Modules are seamlessly integrated with Terragrunt through:

- Shared configuration files
- Dynamic source resolution
- Environment-specific parameter injection
</file>

<file path="infra/terragrunt/_shared/_config/README.md">
# Terragrunt Shared Configuration Management üõ†Ô∏è

## Overview

This directory contains centralized configuration files that provide a robust, flexible framework for managing infrastructure deployments across different environments and projects.

## Architecture Principles üèóÔ∏è

### Core Design Concepts

1. **Centralization**: Single source of truth for infrastructure configurations
2. **Flexibility**: Environment-aware, dynamically resolvable settings
3. **Traceability**: Comprehensive metadata and resource identification
4. **Consistency**: Uniform configuration management across infrastructure components

## Configuration Components üì¶

### 1. Resource Tagging Strategy (`tags.hcl`)

Implements a sophisticated, multi-layered tagging mechanism:

```hcl
locals {
  # Hierarchical Tagging Approach
  global_tags = {
    ManagedBy     = "Terragrunt"
    OrchestratedBy = "Infrastructure-as-Code"
    Architecture  = "Reference"
  }

  environment_tags = {
    Environment = var.environment
    Region      = var.region
  }

  resource_tags = {
    Type    = "infrastructure-component"
    Project = local.project_name
    Version = local.project_version
  }

  # Merged tags with intelligent precedence
  all_tags = merge(
    local.global_tags,
    local.environment_tags,
    local.resource_tags
  )
}
```

#### Tagging Benefits

- **Resource Identification**: Precise, hierarchical resource tracking
- **Cost Allocation**: Granular resource categorization
- **Compliance Management**: Standardized metadata enforcement
- **Automated Governance**: Consistent resource labeling

### 2. Project Metadata (`app.hcl`)

Centralizes project-level configuration with dynamic resolution:

```hcl
locals {
  # Dynamic project metadata configuration
  project_name    = get_env("TG_STACK_APP_PRODUCT_NAME", "default-project")
  project_version = get_env("TG_STACK_APP_PRODUCT_VERSION", "0.0.0")
  environment     = get_env("TG_ENVIRONMENT", "development")

  # Computed project attributes
  project_identifier = lower(replace(local.project_name, "/[^a-zA-Z0-9]/", "-"))
}
```

#### Metadata Management Advantages

- Environment-driven configuration
- Flexible default value handling
- Normalized project identification
- Consistent metadata across infrastructure

### 3. Remote State Management (`remote_state.hcl`)

Implements a robust, secure remote state configuration strategy:

```hcl
locals {
  # Intelligent remote state configuration
  remote_state_bucket = get_env("TG_STACK_REMOTE_STATE_BUCKET_NAME", "")
  lock_table_name     = get_env("TG_STACK_REMOTE_STATE_LOCK_TABLE", "")
  state_region        = get_env("TG_STACK_REMOTE_STATE_REGION", "us-east-1")

  # State file naming strategy
  state_object_basename = get_env("TG_STACK_REMOTE_STATE_OBJECT_BASENAME", "terraform.tfstate")
  backend_filename      = get_env("TG_STACK_REMOTE_STATE_BACKEND_TF_FILENAME", "backend.tf")

  # Computed state configuration
  state_key = format(
    "%s/%s/%s/terraform.tfstate",
    local.project_identifier,
    local.environment,
    basename(get_terragrunt_dir())
  )
}
```

#### Remote State Management Benefits

- Dynamic, environment-driven configuration
- Secure, consistent state file naming
- Flexible region and bucket management
- Predictable state key generation

### 4. Git Source Configuration (`git.hcl`)

Centralizes base URLs for sourcing Terraform modules from various Git providers or local paths.

```hcl
locals {
  # Centralized Git base URLs
  git_base_urls = {
    github = "git::git@github.com:"
    gitlab = "git::gitlab.com:"
    local  = "${get_repo_root()}/infra/terraform/modules"
    # Add other providers as needed
  }
}
```

#### Git Source Management Benefits

- **Consistency**: Ensures uniform module source URLs across all Terragrunt configurations.
- **Maintainability**: Simplifies updates to source locations (e.g., migrating from GitHub to GitLab).
- **Flexibility**: Easily switch between remote and local module sources during development or testing.

## Environment Variable Management üåê

### Recommended Configuration

```bash
# Project Metadata
export TG_STACK_APP_PRODUCT_NAME="infrastructure-reference-arch"
export TG_STACK_APP_PRODUCT_VERSION="1.0.0"
export TG_ENVIRONMENT="production"

# Remote State Configuration
export TG_STACK_REMOTE_STATE_BUCKET_NAME="org-terraform-state"
export TG_STACK_REMOTE_STATE_LOCK_TABLE="terraform-state-locks"
export TG_STACK_REMOTE_STATE_REGION="us-east-1"
```

## Configuration Resolution Mechanism üîÑ

### Dynamic Loading Strategy

1. Terragrunt recursively loads configurations from `_shared/_config`
2. Local variables computed using `read_terragrunt_config()`
3. Configurations merged with intelligent precedence
4. Environment variables override default values

### Inheritance and Override Patterns

- Shared configurations serve as base templates
- Module-specific configurations can extend or override shared settings
- Use `read_terragrunt_config()` for flexible configuration loading
- Implement merge strategies to control configuration inheritance

## Best Practices üåü

1. **Naming Conventions**

   - Use lowercase, hyphen-separated project names
   - Follow semantic versioning
   - Maintain consistent naming across environments

2. **Security Considerations**

   - Never commit sensitive information
   - Use environment variables for dynamic configuration
   - Implement least-privilege access for state management

3. **Configuration Management**
   - Keep configurations DRY (Don't Repeat Yourself)
   - Document configuration purpose and usage
   - Validate configurations across different environments

## Troubleshooting üõ†Ô∏è

### Common Resolution Strategies

1. **Configuration Validation**

   - Verify environment variable settings
   - Check configuration file syntax
   - Validate merge strategy implementations

2. **State Management Diagnostics**

   - Confirm S3 bucket and DynamoDB table existence
   - Validate IAM permissions
   - Check region and endpoint configurations

3. **Tagging Consistency**
   - Audit resource tags across infrastructure
   - Verify tag inheritance mechanisms
   - Implement automated tag validation
</file>

<file path="infra/terragrunt/_shared/_config/remote_state.hcl">
// üåç Terragrunt Configuration Local Variables
// This section specifies local variables utilized across various child configurations.
// These locals ensure uniformity in naming conventions and metadata throughout the infrastructure.
locals {
  // üóÑÔ∏è S3 Bucket Name for Terraform State
  bucket_name_unnormalized = get_env("TG_STACK_REMOTE_STATE_BUCKET_NAME")
  bucket_name              = lower(trimspace(local.bucket_name_unnormalized))

  // üîí DynamoDB Lock Table Name
  lock_table_unnormalized = get_env("TG_STACK_REMOTE_STATE_LOCK_TABLE")
  lock_table              = lower(trimspace(local.lock_table_unnormalized))

  // üåê AWS Region for Deployment
  region_unnormalized = get_env("TG_STACK_REMOTE_STATE_REGION", "us-east-1")
  region              = lower(trimspace(local.region_unnormalized))

  // üì¶ Basename for the Terraform state object
  state_object_basename_unnormalized = get_env("TG_STACK_REMOTE_STATE_OBJECT_BASENAME", "terraform.tfstate.json")
  state_object_basename              = lower(trimspace(local.state_object_basename_unnormalized))

  // üìÅ Backend Terraform File
  backend_tf_filename_unnormalized = get_env("TG_STACK_REMOTE_STATE_BACKEND_TF_FILENAME", "backend.tf")
  backend_tf_filename              = lower(trimspace(local.backend_tf_filename_unnormalized))
}
</file>

<file path="infra/terragrunt/_shared/_units/README.md">
# Terragrunt Shared Infrastructure Units üß©

## Overview

This directory contains shared infrastructure unit configurations that provide a modular, flexible, and standardized approach to defining infrastructure components across different environments and modules.

## Architecture Principles üèóÔ∏è

### Core Design Concepts

1. **Modularity**: Discrete, self-contained infrastructure components
2. **Flexibility**: Environment-specific customizations with consistent base configuration
3. **Traceability**: Intelligent metadata and tagging management
4. **Dependency Orchestration**: Efficient cross-module dependency management

## Configuration Strategy üì¶

### 1. Module Source Management

Implements a sophisticated, flexible module sourcing mechanism:

```hcl
locals {
  # Centralized module source configuration
  git_base_url           = "git::git@github.com:"
  tf_module_repository   = "your-org/terraform-modules"
  tf_module_version_default = "v0.1.0"
  tf_module_path_default = "modules/infrastructure-component"

  # Dynamic module source generation
  tf_module_source = format(
    "%s%s//%s",
    local.git_base_url,
    local.tf_module_repository,
    local.tf_module_path_default
  )
}
```

#### Source Management Benefits

- Centralized version control
- Consistent module referencing
- Flexible versioning
- Simplified update process

### 2. Intelligent Tagging System

Multi-layered tagging strategy for comprehensive resource management:

```hcl
locals {
  # Hierarchical Tagging Approach
  global_tags = {
    ManagedBy     = "Terragrunt"
    Architecture  = "Reference"
  }

  env_tags = {
    Environment = var.environment
    Region      = var.region
  }

  unit_tags = {
    Unit = "infrastructure-component"
    Type = "generator"
  }

  # Merged tags with clear precedence
  all_tags = merge(
    local.global_tags,
    local.env_tags,
    local.unit_tags
  )
}
```

#### Tagging Advantages

- Consistent resource identification
- Flexible tag management
- Enhanced tracking capabilities
- Cost allocation support
- Clear ownership definition

### 3. Dynamic Configuration Loading

Leverages Terragrunt's advanced configuration capabilities:

```hcl
locals {
  # Hierarchical configuration resolution
  global_config = read_terragrunt_config(find_in_parent_folders("global.hcl", ""))
  env_config    = read_terragrunt_config(find_in_parent_folders("env.hcl", ""))
  stack_config  = read_terragrunt_config(find_in_parent_folders("stack.hcl", ""))
}
```

#### Configuration Management Benefits

- Hierarchical configuration control
- Environment-specific customization
- Single source of truth
- Flexible override mechanisms

### 4. Dependency Management

Robust dependency resolution and mocking:

```hcl
dependency "prerequisite_module" {
  config_path = "../dependent-module"

  mock_outputs = {
    # Provides predictable outputs for planning
    module_output = "mock-value"
  }
}
```

#### Dependency Handling Advantages

- Explicit dependency declaration
- Validation-friendly mock outputs
- Controlled module interactions
- Simplified testing workflows

## Integration Mechanism üîó

Child Terragrunt configurations include shared units using a standardized approach:

```hcl
include "shared" {
  path           = "${get_terragrunt_dir()}/../../../_shared/_units/component.hcl"
  expose         = true
  merge_strategy = "deep"
}
```

## Best Practices üåü

### 1. Module Management

- Use semantic versioning
- Maintain stable module interfaces
- Document breaking changes
- Implement comprehensive input validation

### 2. Configuration Design

- Maintain focused, modular units
- Follow consistent naming conventions
- Implement clear tagging strategies
- Document configuration purpose and usage

### 3. Dependency Handling

- Define explicit, clear dependencies
- Provide meaningful mock outputs
- Document inter-module relationships
- Implement defensive configuration checks

## Troubleshooting üõ†Ô∏è

### Common Resolution Strategies

1. **Module Source Verification**

   - Validate git repository access
   - Confirm module path accuracy
   - Check version constraint compatibility

2. **Configuration Loading Diagnostics**

   - Verify file path correctness
   - Validate environment variable configurations
   - Check configuration syntax and structure

3. **Dependency Resolution**
   - Validate dependency paths
   - Verify mock output completeness
   - Ensure output references are correct
</file>

<file path="infra/terragrunt/_shared/README.md">
# Terragrunt Shared Infrastructure Configuration üèóÔ∏è

## Overview

This directory provides a centralized, modular approach to managing infrastructure configurations and reusable components for Terragrunt-based deployments.

## Architecture Principles üõ†Ô∏è

### Core Design Concepts

1. **Centralization**: Single source of truth for infrastructure management
2. **Modularity**: Discrete, composable infrastructure components
3. **Flexibility**: Environment-aware, dynamically resolvable configurations
4. **Traceability**: Comprehensive metadata and resource identification

## Directory Structure üìÇ

```
_shared/
‚îú‚îÄ‚îÄ _config/       # Centralized configuration management
‚îÇ   ‚îú‚îÄ‚îÄ README.md  # Detailed configuration strategy
‚îÇ   ‚îú‚îÄ‚îÄ tags.hcl   # Resource tagging mechanism
‚îÇ   ‚îú‚îÄ‚îÄ app.hcl    # Project metadata configuration
‚îÇ   ‚îî‚îÄ‚îÄ remote_state.hcl  # State management configuration
‚îÇ
‚îî‚îÄ‚îÄ _units/        # Reusable infrastructure components
    ‚îú‚îÄ‚îÄ README.md  # Component architecture overview
    ‚îú‚îÄ‚îÄ dni_generator.hcl
    ‚îú‚îÄ‚îÄ lastname_generator.hcl
    ‚îú‚îÄ‚îÄ name_generator.hcl
    ‚îî‚îÄ‚îÄ age_generator.hcl
```

## Configuration Management üîß

### Shared Configuration (`_config/`)

Provides centralized, environment-aware configuration management:

- **Resource Tagging**: Implement consistent, hierarchical resource identification
- **Project Metadata**: Dynamic, environment-driven project configuration
- **Remote State Management**: Secure, flexible state storage strategies

**Detailed Documentation**: [\_config/README.md](_config/README.md)

### Infrastructure Units (`_units/`)

Offers modular, reusable infrastructure components:

- **Generator Components**: Discrete infrastructure building blocks
- **Dynamic Configuration**: Flexible, environment-specific customization
- **Intelligent Dependency Management**: Robust cross-module interactions

**Detailed Documentation**: [\_units/README.md](_units/README.md)

## Key Benefits üåü

- **Consistent Configuration**: Uniform infrastructure management
- **Environment Flexibility**: Adaptable to different deployment contexts
- **Enhanced Traceability**: Comprehensive resource metadata
- **Simplified Maintenance**: Centralized, modular approach

## Getting Started üöÄ

1. Review configuration strategies in `_config/README.md`
2. Explore available infrastructure units in `_units/README.md`
3. Set up required environment variables
4. Customize configurations for your specific infrastructure needs
</file>

<file path="infra/terragrunt/_templates/.terraform-version.tpl">
# Terraform Version File Generation Disabled
# To enable version file generation, set TG_STACK_FLAG_ENABLE_TERRAFORM_VERSION_FILE_OVERRIDE to "true"
# Current setting prevents automatic version file creation
</file>

<file path="infra/terragrunt/global/dni/age_generator/README.md">
# Infrastructure Unit Configuration üõ†Ô∏è

## Overview

This Terragrunt unit demonstrates a dynamic provider and version management system implemented in the infrastructure. It provides a flexible, modular approach to configuring infrastructure components.

## üìÅ File Structure

```
dns-zone/
‚îú‚îÄ‚îÄ unit_cfg_providers.hcl  # Provider configurations (optional)
‚îú‚îÄ‚îÄ unit_cfg_versions.hcl   # Version constraints (optional)
‚îú‚îÄ‚îÄ terragrunt.hcl          # Unit-specific Terragrunt configuration
‚îî‚îÄ‚îÄ README.md              # This documentation
```

## üîå Provider Configuration Management

### Dynamic Configuration System

The unit implements a flexible provider configuration mechanism:

1. **Local Provider Configuration** (`unit_cfg_providers.hcl`):

   - Defines provider-specific settings
   - Credentials sourced from environment variables
   - Supports multiple provider configurations

2. **Version Management** (`unit_cfg_versions.hcl`):
   - Specifies provider version constraints
   - Ensures consistent provider versions across deployments

## üîÑ Configuration Loading Strategy

### Intelligent Configuration Resolution

The system employs a robust configuration loading approach:

1. **Primary Configuration**:

   - Prioritizes unit-specific provider configurations
   - Dynamically loads provider settings from `unit_cfg_providers.hcl`
   - Applies version constraints from `unit_cfg_versions.hcl`

2. **Fallback Mechanism**:
   - Provides safe default configurations when specific files are missing
   - Includes a null provider to prevent initialization errors
   - Maintains infrastructure deployment capabilities

## üõ†Ô∏è Configuration Examples

### Basic Provider Setup

```hcl
# unit_cfg_providers.hcl
locals {
  providers = [
    <<-EOF
    provider "example" {
      # Provider-specific configuration
      credential = var.provider_credential
    }
    EOF
  ]
}

# unit_cfg_versions.hcl
locals {
  versions = [
    <<-EOF
    terraform {
      required_providers {
        example = {
          source  = "example/provider"
          version = "~> 1.0"
        }
      }
    }
    EOF
  ]
}
```

## üîí Security Considerations

- Never commit sensitive credentials in configuration files
- Use environment variables for credential management
- Follow the principle of least privilege
- Implement secure credential rotation strategies

## ü§ù Contributing Guidelines

When modifying the unit:

1. Update provider configurations in `unit_cfg_providers.hcl`
2. Modify version constraints in `unit_cfg_versions.hcl`
3. Validate changes using `terragrunt plan`
4. Ensure no sensitive information is exposed

## üåê Environment Variable Management

### Provider Credential Setup

```bash
# Generic provider credential example
export TG_STACK_PROVIDER_EXAMPLE_CREDENTIAL="your-secure-credential"
```

## üîç Troubleshooting

### Common Configuration Issues

1. **Provider Configuration Errors**

   - Verify environment variable names
   - Check credential formatting
   - Ensure correct provider source and version

2. **Version Constraint Problems**
   - Validate version syntax
   - Confirm provider compatibility
   - Check for version conflicts

## üìö Related Documentation

- [Terragrunt Documentation](https://terragrunt.gruntwork.io/docs/)
- [Terraform Provider Development](https://developer.hashicorp.com/terraform/plugin/best-practices)

## Conclusion

This configuration approach provides a flexible, secure, and maintainable method for managing infrastructure providers across different deployment units.

## üå≥ Configuration Hierarchy and Inheritance

### Infrastructure Configuration Layers

The unit's configuration follows a multi-layered approach:

1. **Root Configuration** (`@root.hcl`):

   - Provides global infrastructure management logic
   - Implements dynamic provider and version generation
   - Manages shared configuration loading
   - Defines core Terragrunt generation rules

2. **Shared Configurations** (`@_shared/_config`):

   - Centralize common infrastructure metadata
   - Define reusable locals and configuration patterns
   - Provide baseline settings for remote state, tagging, and application metadata

3. **Unit Configuration** (`@terragrunt.hcl`):
   - Specific to this infrastructure unit
   - Inherits and extends root and shared configurations
   - Defines unit-specific resource modules and dependencies

### Configuration Flow

```
Root Config (@root.hcl)
‚îÇ
‚îú‚îÄ‚îÄ Shared Configs (@_shared/_config)
‚îÇ   ‚îú‚îÄ‚îÄ app.hcl
‚îÇ   ‚îú‚îÄ‚îÄ remote_state.hcl
‚îÇ   ‚îî‚îÄ‚îÄ tags.hcl
‚îÇ
‚îî‚îÄ‚îÄ Unit Config (@terragrunt.hcl)
    ‚îú‚îÄ‚îÄ unit_cfg_providers.hcl
    ‚îî‚îÄ‚îÄ unit_cfg_versions.hcl
```

### Inheritance Mechanism

- **Provider Configuration**:

  - Root configuration dynamically loads provider settings from unit-level `unit_cfg_providers.hcl`
  - Fallback to null provider if no configuration is found

- **Version Management**:

  - Root configuration reads version constraints from `unit_cfg_versions.hcl`
  - Generates `versions.tf` with unit-specific or default constraints

- **Shared Metadata**:
  - Unit inherits common tags, remote state configuration, and application metadata
  - Allows for consistent resource management across infrastructure

### Configuration Precedence

1. Unit-specific configurations take highest priority
2. Shared configurations provide default values
3. Root configuration manages dynamic generation and fallback mechanisms

### Best Practices

- Keep unit configurations minimal and focused
- Leverage shared configurations for common settings
- Use environment variables for sensitive or environment-specific configurations
- Maintain clear separation of concerns between configuration layers
</file>

<file path="infra/terragrunt/global/dni/age_generator/unit_cfg_providers.hcl">
locals {
  # üåê PROVIDER CREDENTIAL MANAGEMENT
  # Purpose: Securely handle and normalize provider credentials
  #
  # Key Features:
  # - Environment variable-based credential retrieval
  # - Automatic normalization (lowercase, trimmed)
  # - Flexible error handling for missing credentials

  # üîë Raw Credential Retrieval
  # Captures original, unmodified environment variable values
  # TODO: Add environment variable retrieval, or specific credential retrieval for your provider
  # provider_credential_unnormalized = get_env("TG_STACK_PROVIDER_CREDENTIAL", "")

  # üßº Credential Normalization
  # Applies consistent formatting to credentials
  # - Converts to lowercase
  # - Removes leading/trailing whitespaces
  # - Handles empty input gracefully
  # TODO: Add normalization logic, and uncomment the following block
  # provider_credential = local.provider_credential_unnormalized != "" ? lower(trimspace(local.provider_credential_unnormalized)) : ""

  # üìã Provider Configuration
  # Centralizes provider-specific settings and credentials
  # provider_config = {
  #   credential = local.provider_credential
  # }

  # üé≤ RANDOM PROVIDER CONFIGURATION
  # Purpose: Configure the Random provider for generating random values
  #
  # Key Features:
  # - No credentials required
  # - Simple provider configuration
  # - Used for generating random values in a deterministic way

  # ‚öôÔ∏è Provider configuration for Terragrunt
  # Generates the provider block with normalized credentials
  providers = [
    <<-EOF
provider "random" {
}
    EOF
  ]
}
</file>

<file path="infra/terragrunt/global/dni/age_generator/unit_cfg_versions.hcl">
locals {
  # üì¶ Versions configuration for Terragrunt
  # Specifies required provider version and source
  # TODO: Add version constraints, and uncomment the following block according to your unit's requirements
  versions = [
    <<-EOF
terraform {
  required_providers {
    random = {
      source  = "hashicorp/random"
      version = "~> 3.6.0"
    }
  }
}
    EOF
  ]
}
</file>

<file path="infra/terragrunt/global/dni/dni_generator/README.md">
# Infrastructure Unit Configuration üõ†Ô∏è

## Overview

This Terragrunt unit demonstrates a dynamic provider and version management system implemented in the infrastructure. It provides a flexible, modular approach to configuring infrastructure components.

## üìÅ File Structure

```
dns-zone/
‚îú‚îÄ‚îÄ unit_cfg_providers.hcl  # Provider configurations (optional)
‚îú‚îÄ‚îÄ unit_cfg_versions.hcl   # Version constraints (optional)
‚îú‚îÄ‚îÄ terragrunt.hcl          # Unit-specific Terragrunt configuration
‚îî‚îÄ‚îÄ README.md              # This documentation
```

## üîå Provider Configuration Management

### Dynamic Configuration System

The unit implements a flexible provider configuration mechanism:

1. **Local Provider Configuration** (`unit_cfg_providers.hcl`):

   - Defines provider-specific settings
   - Credentials sourced from environment variables
   - Supports multiple provider configurations

2. **Version Management** (`unit_cfg_versions.hcl`):
   - Specifies provider version constraints
   - Ensures consistent provider versions across deployments

## üîÑ Configuration Loading Strategy

### Intelligent Configuration Resolution

The system employs a robust configuration loading approach:

1. **Primary Configuration**:

   - Prioritizes unit-specific provider configurations
   - Dynamically loads provider settings from `unit_cfg_providers.hcl`
   - Applies version constraints from `unit_cfg_versions.hcl`

2. **Fallback Mechanism**:
   - Provides safe default configurations when specific files are missing
   - Includes a null provider to prevent initialization errors
   - Maintains infrastructure deployment capabilities

## üõ†Ô∏è Configuration Examples

### Basic Provider Setup

```hcl
# unit_cfg_providers.hcl
locals {
  providers = [
    <<-EOF
    provider "example" {
      # Provider-specific configuration
      credential = var.provider_credential
    }
    EOF
  ]
}

# unit_cfg_versions.hcl
locals {
  versions = [
    <<-EOF
    terraform {
      required_providers {
        example = {
          source  = "example/provider"
          version = "~> 1.0"
        }
      }
    }
    EOF
  ]
}
```

## üîí Security Considerations

- Never commit sensitive credentials in configuration files
- Use environment variables for credential management
- Follow the principle of least privilege
- Implement secure credential rotation strategies

## ü§ù Contributing Guidelines

When modifying the unit:

1. Update provider configurations in `unit_cfg_providers.hcl`
2. Modify version constraints in `unit_cfg_versions.hcl`
3. Validate changes using `terragrunt plan`
4. Ensure no sensitive information is exposed

## üåê Environment Variable Management

### Provider Credential Setup

```bash
# Generic provider credential example
export TG_STACK_PROVIDER_EXAMPLE_CREDENTIAL="your-secure-credential"
```

## üîç Troubleshooting

### Common Configuration Issues

1. **Provider Configuration Errors**

   - Verify environment variable names
   - Check credential formatting
   - Ensure correct provider source and version

2. **Version Constraint Problems**
   - Validate version syntax
   - Confirm provider compatibility
   - Check for version conflicts

## üìö Related Documentation

- [Terragrunt Documentation](https://terragrunt.gruntwork.io/docs/)
- [Terraform Provider Development](https://developer.hashicorp.com/terraform/plugin/best-practices)

## Conclusion

This configuration approach provides a flexible, secure, and maintainable method for managing infrastructure providers across different deployment units.

## üå≥ Configuration Hierarchy and Inheritance

### Infrastructure Configuration Layers

The unit's configuration follows a multi-layered approach:

1. **Root Configuration** (`@root.hcl`):

   - Provides global infrastructure management logic
   - Implements dynamic provider and version generation
   - Manages shared configuration loading
   - Defines core Terragrunt generation rules

2. **Shared Configurations** (`@_shared/_config`):

   - Centralize common infrastructure metadata
   - Define reusable locals and configuration patterns
   - Provide baseline settings for remote state, tagging, and application metadata

3. **Unit Configuration** (`@terragrunt.hcl`):
   - Specific to this infrastructure unit
   - Inherits and extends root and shared configurations
   - Defines unit-specific resource modules and dependencies

### Configuration Flow

```
Root Config (@root.hcl)
‚îÇ
‚îú‚îÄ‚îÄ Shared Configs (@_shared/_config)
‚îÇ   ‚îú‚îÄ‚îÄ app.hcl
‚îÇ   ‚îú‚îÄ‚îÄ remote_state.hcl
‚îÇ   ‚îî‚îÄ‚îÄ tags.hcl
‚îÇ
‚îî‚îÄ‚îÄ Unit Config (@terragrunt.hcl)
    ‚îú‚îÄ‚îÄ unit_cfg_providers.hcl
    ‚îî‚îÄ‚îÄ unit_cfg_versions.hcl
```

### Inheritance Mechanism

- **Provider Configuration**:

  - Root configuration dynamically loads provider settings from unit-level `unit_cfg_providers.hcl`
  - Fallback to null provider if no configuration is found

- **Version Management**:

  - Root configuration reads version constraints from `unit_cfg_versions.hcl`
  - Generates `versions.tf` with unit-specific or default constraints

- **Shared Metadata**:
  - Unit inherits common tags, remote state configuration, and application metadata
  - Allows for consistent resource management across infrastructure

### Configuration Precedence

1. Unit-specific configurations take highest priority
2. Shared configurations provide default values
3. Root configuration manages dynamic generation and fallback mechanisms

### Best Practices

- Keep unit configurations minimal and focused
- Leverage shared configurations for common settings
- Use environment variables for sensitive or environment-specific configurations
- Maintain clear separation of concerns between configuration layers
</file>

<file path="infra/terragrunt/global/dni/dni_generator/unit_cfg_providers.hcl">
locals {
  # üåê PROVIDER CREDENTIAL MANAGEMENT
  # Purpose: Securely handle and normalize provider credentials
  #
  # Key Features:
  # - Environment variable-based credential retrieval
  # - Automatic normalization (lowercase, trimmed)
  # - Flexible error handling for missing credentials

  # üîë Raw Credential Retrieval
  # Captures original, unmodified environment variable values
  # TODO: Add environment variable retrieval, or specific credential retrieval for your provider
  # provider_credential_unnormalized = get_env("TG_STACK_PROVIDER_CREDENTIAL", "")

  # üßº Credential Normalization
  # Applies consistent formatting to credentials
  # - Converts to lowercase
  # - Removes leading/trailing whitespaces
  # - Handles empty input gracefully
  # TODO: Add normalization logic, and uncomment the following block
  # provider_credential = local.provider_credential_unnormalized != "" ? lower(trimspace(local.provider_credential_unnormalized)) : ""

  # üìã Provider Configuration
  # Centralizes provider-specific settings and credentials
  # provider_config = {
  #   credential = local.provider_credential
  # }

  # üé≤ RANDOM PROVIDER CONFIGURATION
  # Purpose: Configure the Random provider for generating random values
  #
  # Key Features:
  # - No credentials required
  # - Simple provider configuration
  # - Used for generating random values in a deterministic way

  # ‚öôÔ∏è Provider configuration for Terragrunt
  # Generates the provider block with normalized credentials
  providers = [
    <<-EOF
provider "random" {
}
    EOF
  ]
}
</file>

<file path="infra/terragrunt/global/dni/dni_generator/unit_cfg_versions.hcl">
locals {
  # üì¶ Versions configuration for Terragrunt
  # Specifies required provider version and source
  # TODO: Add version constraints, and uncomment the following block according to your unit's requirements
  versions = [
    <<-EOF
terraform {
  required_providers {
    random = {
      source  = "hashicorp/random"
      version = "~> 3.6.0"
    }
  }
}
    EOF
  ]
}
</file>

<file path="infra/terragrunt/global/dni/lastname_generator/README.md">
# Infrastructure Unit Configuration üõ†Ô∏è

## Overview

This Terragrunt unit demonstrates a dynamic provider and version management system implemented in the infrastructure. It provides a flexible, modular approach to configuring infrastructure components.

## üìÅ File Structure

```
dns-zone/
‚îú‚îÄ‚îÄ unit_cfg_providers.hcl  # Provider configurations (optional)
‚îú‚îÄ‚îÄ unit_cfg_versions.hcl   # Version constraints (optional)
‚îú‚îÄ‚îÄ terragrunt.hcl          # Unit-specific Terragrunt configuration
‚îî‚îÄ‚îÄ README.md              # This documentation
```

## üîå Provider Configuration Management

### Dynamic Configuration System

The unit implements a flexible provider configuration mechanism:

1. **Local Provider Configuration** (`unit_cfg_providers.hcl`):

   - Defines provider-specific settings
   - Credentials sourced from environment variables
   - Supports multiple provider configurations

2. **Version Management** (`unit_cfg_versions.hcl`):
   - Specifies provider version constraints
   - Ensures consistent provider versions across deployments

## üîÑ Configuration Loading Strategy

### Intelligent Configuration Resolution

The system employs a robust configuration loading approach:

1. **Primary Configuration**:

   - Prioritizes unit-specific provider configurations
   - Dynamically loads provider settings from `unit_cfg_providers.hcl`
   - Applies version constraints from `unit_cfg_versions.hcl`

2. **Fallback Mechanism**:
   - Provides safe default configurations when specific files are missing
   - Includes a null provider to prevent initialization errors
   - Maintains infrastructure deployment capabilities

## üõ†Ô∏è Configuration Examples

### Basic Provider Setup

```hcl
# unit_cfg_providers.hcl
locals {
  providers = [
    <<-EOF
    provider "example" {
      # Provider-specific configuration
      credential = var.provider_credential
    }
    EOF
  ]
}

# unit_cfg_versions.hcl
locals {
  versions = [
    <<-EOF
    terraform {
      required_providers {
        example = {
          source  = "example/provider"
          version = "~> 1.0"
        }
      }
    }
    EOF
  ]
}
```

## üîí Security Considerations

- Never commit sensitive credentials in configuration files
- Use environment variables for credential management
- Follow the principle of least privilege
- Implement secure credential rotation strategies

## ü§ù Contributing Guidelines

When modifying the unit:

1. Update provider configurations in `unit_cfg_providers.hcl`
2. Modify version constraints in `unit_cfg_versions.hcl`
3. Validate changes using `terragrunt plan`
4. Ensure no sensitive information is exposed

## üåê Environment Variable Management

### Provider Credential Setup

```bash
# Generic provider credential example
export TG_STACK_PROVIDER_EXAMPLE_CREDENTIAL="your-secure-credential"
```

## üîç Troubleshooting

### Common Configuration Issues

1. **Provider Configuration Errors**

   - Verify environment variable names
   - Check credential formatting
   - Ensure correct provider source and version

2. **Version Constraint Problems**
   - Validate version syntax
   - Confirm provider compatibility
   - Check for version conflicts

## üìö Related Documentation

- [Terragrunt Documentation](https://terragrunt.gruntwork.io/docs/)
- [Terraform Provider Development](https://developer.hashicorp.com/terraform/plugin/best-practices)

## Conclusion

This configuration approach provides a flexible, secure, and maintainable method for managing infrastructure providers across different deployment units.

## üå≥ Configuration Hierarchy and Inheritance

### Infrastructure Configuration Layers

The unit's configuration follows a multi-layered approach:

1. **Root Configuration** (`@root.hcl`):

   - Provides global infrastructure management logic
   - Implements dynamic provider and version generation
   - Manages shared configuration loading
   - Defines core Terragrunt generation rules

2. **Shared Configurations** (`@_shared/_config`):

   - Centralize common infrastructure metadata
   - Define reusable locals and configuration patterns
   - Provide baseline settings for remote state, tagging, and application metadata

3. **Unit Configuration** (`@terragrunt.hcl`):
   - Specific to this infrastructure unit
   - Inherits and extends root and shared configurations
   - Defines unit-specific resource modules and dependencies

### Configuration Flow

```
Root Config (@root.hcl)
‚îÇ
‚îú‚îÄ‚îÄ Shared Configs (@_shared/_config)
‚îÇ   ‚îú‚îÄ‚îÄ app.hcl
‚îÇ   ‚îú‚îÄ‚îÄ remote_state.hcl
‚îÇ   ‚îî‚îÄ‚îÄ tags.hcl
‚îÇ
‚îî‚îÄ‚îÄ Unit Config (@terragrunt.hcl)
    ‚îú‚îÄ‚îÄ unit_cfg_providers.hcl
    ‚îî‚îÄ‚îÄ unit_cfg_versions.hcl
```

### Inheritance Mechanism

- **Provider Configuration**:

  - Root configuration dynamically loads provider settings from unit-level `unit_cfg_providers.hcl`
  - Fallback to null provider if no configuration is found

- **Version Management**:

  - Root configuration reads version constraints from `unit_cfg_versions.hcl`
  - Generates `versions.tf` with unit-specific or default constraints

- **Shared Metadata**:
  - Unit inherits common tags, remote state configuration, and application metadata
  - Allows for consistent resource management across infrastructure

### Configuration Precedence

1. Unit-specific configurations take highest priority
2. Shared configurations provide default values
3. Root configuration manages dynamic generation and fallback mechanisms

### Best Practices

- Keep unit configurations minimal and focused
- Leverage shared configurations for common settings
- Use environment variables for sensitive or environment-specific configurations
- Maintain clear separation of concerns between configuration layers
</file>

<file path="infra/terragrunt/global/dni/lastname_generator/unit_cfg_providers.hcl">
locals {
  # üåê PROVIDER CREDENTIAL MANAGEMENT
  # Purpose: Securely handle and normalize provider credentials
  #
  # Key Features:
  # - Environment variable-based credential retrieval
  # - Automatic normalization (lowercase, trimmed)
  # - Flexible error handling for missing credentials

  # üîë Raw Credential Retrieval
  # Captures original, unmodified environment variable values
  # TODO: Add environment variable retrieval, or specific credential retrieval for your provider
  # provider_credential_unnormalized = get_env("TG_STACK_PROVIDER_CREDENTIAL", "")

  # üßº Credential Normalization
  # Applies consistent formatting to credentials
  # - Converts to lowercase
  # - Removes leading/trailing whitespaces
  # - Handles empty input gracefully
  # TODO: Add normalization logic, and uncomment the following block
  # provider_credential = local.provider_credential_unnormalized != "" ? lower(trimspace(local.provider_credential_unnormalized)) : ""

  # üìã Provider Configuration
  # Centralizes provider-specific settings and credentials
  # provider_config = {
  #   credential = local.provider_credential
  # }

  # üé≤ RANDOM PROVIDER CONFIGURATION
  # Purpose: Configure the Random provider for generating random values
  #
  # Key Features:
  # - No credentials required
  # - Simple provider configuration
  # - Used for generating random values in a deterministic way

  # ‚öôÔ∏è Provider configuration for Terragrunt
  # Generates the provider block with normalized credentials
  providers = [
    <<-EOF
provider "random" {
}
    EOF
  ]
}
</file>

<file path="infra/terragrunt/global/dni/lastname_generator/unit_cfg_versions.hcl">
locals {
  # üì¶ Versions configuration for Terragrunt
  # Specifies required provider version and source
  # TODO: Add version constraints, and uncomment the following block according to your unit's requirements
  versions = [
    <<-EOF
terraform {
  required_providers {
    random = {
      source  = "hashicorp/random"
      version = "~> 3.6.0"
    }
  }
}
    EOF
  ]
}
</file>

<file path="infra/terragrunt/global/dni/name_generator/README.md">
# Infrastructure Unit Configuration üõ†Ô∏è

## Overview

This Terragrunt unit demonstrates a dynamic provider and version management system implemented in the infrastructure. It provides a flexible, modular approach to configuring infrastructure components.

## üìÅ File Structure

```
dns-zone/
‚îú‚îÄ‚îÄ unit_cfg_providers.hcl  # Provider configurations (optional)
‚îú‚îÄ‚îÄ unit_cfg_versions.hcl   # Version constraints (optional)
‚îú‚îÄ‚îÄ terragrunt.hcl          # Unit-specific Terragrunt configuration
‚îî‚îÄ‚îÄ README.md              # This documentation
```

## üîå Provider Configuration Management

### Dynamic Configuration System

The unit implements a flexible provider configuration mechanism:

1. **Local Provider Configuration** (`unit_cfg_providers.hcl`):

   - Defines provider-specific settings
   - Credentials sourced from environment variables
   - Supports multiple provider configurations

2. **Version Management** (`unit_cfg_versions.hcl`):
   - Specifies provider version constraints
   - Ensures consistent provider versions across deployments

## üîÑ Configuration Loading Strategy

### Intelligent Configuration Resolution

The system employs a robust configuration loading approach:

1. **Primary Configuration**:

   - Prioritizes unit-specific provider configurations
   - Dynamically loads provider settings from `unit_cfg_providers.hcl`
   - Applies version constraints from `unit_cfg_versions.hcl`

2. **Fallback Mechanism**:
   - Provides safe default configurations when specific files are missing
   - Includes a null provider to prevent initialization errors
   - Maintains infrastructure deployment capabilities

## üõ†Ô∏è Configuration Examples

### Basic Provider Setup

```hcl
# unit_cfg_providers.hcl
locals {
  providers = [
    <<-EOF
    provider "example" {
      # Provider-specific configuration
      credential = var.provider_credential
    }
    EOF
  ]
}

# unit_cfg_versions.hcl
locals {
  versions = [
    <<-EOF
    terraform {
      required_providers {
        example = {
          source  = "example/provider"
          version = "~> 1.0"
        }
      }
    }
    EOF
  ]
}
```

## üîí Security Considerations

- Never commit sensitive credentials in configuration files
- Use environment variables for credential management
- Follow the principle of least privilege
- Implement secure credential rotation strategies

## ü§ù Contributing Guidelines

When modifying the unit:

1. Update provider configurations in `unit_cfg_providers.hcl`
2. Modify version constraints in `unit_cfg_versions.hcl`
3. Validate changes using `terragrunt plan`
4. Ensure no sensitive information is exposed

## üåê Environment Variable Management

### Provider Credential Setup

```bash
# Generic provider credential example
export TG_STACK_PROVIDER_EXAMPLE_CREDENTIAL="your-secure-credential"
```

## üîç Troubleshooting

### Common Configuration Issues

1. **Provider Configuration Errors**

   - Verify environment variable names
   - Check credential formatting
   - Ensure correct provider source and version

2. **Version Constraint Problems**
   - Validate version syntax
   - Confirm provider compatibility
   - Check for version conflicts

## üìö Related Documentation

- [Terragrunt Documentation](https://terragrunt.gruntwork.io/docs/)
- [Terraform Provider Development](https://developer.hashicorp.com/terraform/plugin/best-practices)

## Conclusion

This configuration approach provides a flexible, secure, and maintainable method for managing infrastructure providers across different deployment units.

## üå≥ Configuration Hierarchy and Inheritance

### Infrastructure Configuration Layers

The unit's configuration follows a multi-layered approach:

1. **Root Configuration** (`@root.hcl`):

   - Provides global infrastructure management logic
   - Implements dynamic provider and version generation
   - Manages shared configuration loading
   - Defines core Terragrunt generation rules

2. **Shared Configurations** (`@_shared/_config`):

   - Centralize common infrastructure metadata
   - Define reusable locals and configuration patterns
   - Provide baseline settings for remote state, tagging, and application metadata

3. **Unit Configuration** (`@terragrunt.hcl`):
   - Specific to this infrastructure unit
   - Inherits and extends root and shared configurations
   - Defines unit-specific resource modules and dependencies

### Configuration Flow

```
Root Config (@root.hcl)
‚îÇ
‚îú‚îÄ‚îÄ Shared Configs (@_shared/_config)
‚îÇ   ‚îú‚îÄ‚îÄ app.hcl
‚îÇ   ‚îú‚îÄ‚îÄ remote_state.hcl
‚îÇ   ‚îî‚îÄ‚îÄ tags.hcl
‚îÇ
‚îî‚îÄ‚îÄ Unit Config (@terragrunt.hcl)
    ‚îú‚îÄ‚îÄ unit_cfg_providers.hcl
    ‚îî‚îÄ‚îÄ unit_cfg_versions.hcl
```

### Inheritance Mechanism

- **Provider Configuration**:

  - Root configuration dynamically loads provider settings from unit-level `unit_cfg_providers.hcl`
  - Fallback to null provider if no configuration is found

- **Version Management**:

  - Root configuration reads version constraints from `unit_cfg_versions.hcl`
  - Generates `versions.tf` with unit-specific or default constraints

- **Shared Metadata**:
  - Unit inherits common tags, remote state configuration, and application metadata
  - Allows for consistent resource management across infrastructure

### Configuration Precedence

1. Unit-specific configurations take highest priority
2. Shared configurations provide default values
3. Root configuration manages dynamic generation and fallback mechanisms

### Best Practices

- Keep unit configurations minimal and focused
- Leverage shared configurations for common settings
- Use environment variables for sensitive or environment-specific configurations
- Maintain clear separation of concerns between configuration layers
</file>

<file path="infra/terragrunt/global/dni/name_generator/unit_cfg_providers.hcl">
locals {
  # üåê PROVIDER CREDENTIAL MANAGEMENT
  # Purpose: Securely handle and normalize provider credentials
  #
  # Key Features:
  # - Environment variable-based credential retrieval
  # - Automatic normalization (lowercase, trimmed)
  # - Flexible error handling for missing credentials

  # üîë Raw Credential Retrieval
  # Captures original, unmodified environment variable values
  # TODO: Add environment variable retrieval, or specific credential retrieval for your provider
  # provider_credential_unnormalized = get_env("TG_STACK_PROVIDER_CREDENTIAL", "")

  # üßº Credential Normalization
  # Applies consistent formatting to credentials
  # - Converts to lowercase
  # - Removes leading/trailing whitespaces
  # - Handles empty input gracefully
  # TODO: Add normalization logic, and uncomment the following block
  # provider_credential = local.provider_credential_unnormalized != "" ? lower(trimspace(local.provider_credential_unnormalized)) : ""

  # üìã Provider Configuration
  # Centralizes provider-specific settings and credentials
  # provider_config = {
  #   credential = local.provider_credential
  # }

  # üé≤ RANDOM PROVIDER CONFIGURATION
  # Purpose: Configure the Random provider for generating random values
  #
  # Key Features:
  # - No credentials required
  # - Simple provider configuration
  # - Used for generating random values in a deterministic way

  # ‚öôÔ∏è Provider configuration for Terragrunt
  # Generates the provider block with normalized credentials
  providers = [
    <<-EOF
provider "random" {
}
    EOF
  ]
}
</file>

<file path="infra/terragrunt/global/dni/name_generator/unit_cfg_versions.hcl">
locals {
  # üì¶ Versions configuration for Terragrunt
  # Specifies required provider version and source
  # TODO: Add version constraints, and uncomment the following block according to your unit's requirements
  versions = [
    <<-EOF
terraform {
  required_providers {
    random = {
      source  = "hashicorp/random"
      version = "~> 3.6.0"
    }
  }
}
    EOF
  ]
}
</file>

<file path="infra/terragrunt/global/dni/README.md">
# Infrastructure Stack: Modular Component Architecture üß©

## Overview

This stack represents a reference implementation of a sophisticated, modular approach to infrastructure component design, demonstrating key principles of scalable and flexible infrastructure management.

## Architectural Principles üèóÔ∏è

### Core Design Concepts

1. **Modularity**: Discrete, composable infrastructure units
2. **Separation of Concerns**: Each component focuses on a specific, well-defined responsibility
3. **Reusability**: Standardized, interchangeable design patterns
4. **Flexibility**: Adaptable to diverse use cases and requirements

## Stack Structure üìÇ

```
stack/
‚îú‚îÄ‚îÄ stack.hcl           # Stack-level configuration manifest
‚îú‚îÄ‚îÄ unit-a/        # Primary orchestration component
‚îú‚îÄ‚îÄ unit-b/        # Specialized functional module
‚îú‚îÄ‚îÄ unit-c/        # Supporting infrastructure unit
‚îî‚îÄ‚îÄ unit-d/        # Auxiliary generation or transformation module
```

## Stack Configuration (`stack.hcl`) üîß

### Purpose

Defines stack-level metadata, configuration strategies, and shared infrastructure settings.

### Configuration Philosophy

- **Centralized Metadata Management**
- **Consistent Tagging Strategies**
- **Environment-Agnostic Design**

## Component Architecture ü§ñ

### Component Design Principles

1. **Single Responsibility**

   - Each component solves a specific problem
   - Clear, well-defined input and output interfaces
   - Minimal dependencies on other components

2. **Standardized Interaction**

   - Consistent communication protocols
   - Well-defined contract interfaces
   - Predictable behavior and error handling

3. **Independent Scalability**
   - Components can be scaled independently
   - Support for horizontal and vertical scaling strategies
   - Minimal performance overhead between components

## Configuration Strategies üõ†Ô∏è

### Flexible Component Parameters

- Seed-based reproducibility
- Configurable output constraints
- Extensible generation and transformation logic

### Metadata Management

- Comprehensive logging mechanisms
- Traceability of component interactions
- Detailed operational metadata
</file>

<file path="infra/terragrunt/global/default.tfvars">
# üåê Global Default Terraform Variables
#
# Purpose: Provide baseline configuration defaults for infrastructure
#
# üîç Dynamic Variable Loading Mechanism:
# - Automatically loaded by Terragrunt during apply, plan, destroy commands
# - Can be overridden by environment or region-specific .tfvars files
# - Serves as a fallback configuration for infrastructure components
#
# üí° Usage in root.hcl:
# - Dynamically included via extra_arguments "optional_vars"
# - Supports flexible, hierarchical configuration management
# - Enables environment-agnostic default settings
</file>

<file path="infra/terragrunt/global/env.hcl">
// üåç Environment Configuration Manifest
//
// This file defines environment-specific configurations and metadata.
// It serves as a central point for environment-level settings that can be
// referenced across different Terragrunt and Terraform modules.
//
// üîç Purpose:
// - Define environment-specific variables
// - Provide consistent tagging strategy
// - Enable environment-level customizations
//
// üí° Configuration Guidelines:
// - Modify values to match your specific environment requirements
// - Ensure consistency across different infrastructure components
// - Use meaningful, descriptive names for environments

// üåç Define local variables for the environment
locals {
  // üè∑Ô∏è Environment Naming Convention
  // - Use descriptive, lowercase names
  // - Recommended formats:
  //   * Development: "dev"
  //   * Staging: "staging"
  //   * Production: "prod"
  //   * Global/Shared: "global"
  //
  // üí° Tip: Consistent naming helps in resource identification and management
  environment_name = "global"

  // üåê Short Environment Identifier
  // - Useful for resource naming, tagging, and quick reference
  // - Should match the full environment name or be a clear abbreviation
  environment = "global"

  // üìõ Resource Tagging Strategy
  //
  // Tags provide crucial metadata for:
  // - Resource identification
  // - Cost allocation
  // - Access management
  // - Compliance tracking
  //
  // üîç Best Practices:
  // - Keep tags consistent across all resources
  // - Use clear, descriptive tag values
  // - Consider adding more tags like:
  //   * Project
  //   * ManagedBy
  //   * Owner
  //   * CostCenter
  tags = {
    // Primary environment identifier
    // TODO: Add more tags
    Environment = local.environment_name
  }
}
</file>

<file path="infra/terragrunt/global/README.md">
# Environment Architecture Framework üåê

## Overview

This directory represents a standardized approach to environment configuration in infrastructure-as-code, providing a flexible, modular framework for managing complex infrastructure deployments across different contexts and environments.

## Architectural Principles üèóÔ∏è

### Core Design Concepts

1. **Hierarchical Configuration**: Multi-level configuration management
2. **Environment Abstraction**: Consistent, context-independent infrastructure definition
3. **Dynamic Configuration**: Adaptive, context-aware settings
4. **Metadata Enrichment**: Comprehensive resource identification and tracking

## Reference Architecture Structure üìÇ

```
environment/
‚îú‚îÄ‚îÄ env.hcl           # Environment-level configuration manifest
‚îú‚îÄ‚îÄ .envrc            # Environment variable management
‚îú‚îÄ‚îÄ default.tfvars    # Baseline variable configurations
‚îî‚îÄ‚îÄ stack-name/       # Infrastructure stack
    ‚îú‚îÄ‚îÄ stack.hcl     # Stack-specific configuration
    ‚îî‚îÄ‚îÄ units/        # Modular infrastructure components
        ‚îú‚îÄ‚îÄ unit-a/
        ‚îú‚îÄ‚îÄ unit-b/
        ‚îî‚îÄ‚îÄ unit-c/
```

## Configuration Files Deep Dive üîç

### 1. Environment Configuration Manifest (`env.hcl`) üè∑Ô∏è

#### Purpose

A centralized configuration file that defines environment-specific settings, metadata, and tagging strategies.

#### Key Components

- **Environment Naming**: Standardized identification
- **Tagging Strategy**: Consistent resource metadata
- **Naming Conventions**: Structured resource identification

#### Example Configuration

```hcl
locals {
  # Environment Identification
  environment_name = "dev"  # dev, staging, prod, global
  environment      = "dev"  # Short identifier

  # Resource Tagging Strategy
  tags = {
    Environment = local.environment_name
    ManagedBy   = "Terragrunt"
    Project     = "Infrastructure"
  }
}
```

#### Best Practices

- Use lowercase, descriptive environment names
- Maintain consistent tagging across all resources
- Include metadata that aids in resource management

### 2. Environment Variable Management (`.envrc`) üåç

#### Purpose

A bash script that provides robust environment variable management, logging, and configuration detection.

#### Key Features

- **Dynamic .env File Loading**: Searches for and loads `.env` files
- **Environment Root Detection**: Identifies infrastructure configuration roots
- **Secure Variable Export**: Safely exports and logs environment variables
- **Configuration Validation**: Ensures critical variables are defined

#### Core Functions

- `_env_init()`: Initializes environment configuration
- `_load_env_dotenv()`: Loads environment variables from `.env` files
- `_env_export()`: Securely exports environment-specific variables
- `_validate_env_config()`: Validates critical configuration settings

#### Example Usage

```bash
# Automatically sets environment variables
# Logs configuration details
# Provides flexible, secure configuration management
```

#### Best Practices

- Never commit sensitive information
- Use environment-specific `.env` files
- Implement least-privilege access controls

### 3. Default Terraform Variables (`default.tfvars`) üìã

#### Purpose

Provides baseline configuration defaults for infrastructure components.

#### Key Characteristics

- Automatically loaded by Terragrunt
- Serves as a fallback configuration
- Can be overridden by environment-specific `.tfvars` files

#### Configuration Strategy

- Define default values for infrastructure components
- Enable flexible, hierarchical configuration management
- Support environment-agnostic default settings
</file>

<file path="infra/terragrunt/default.tfvars">
# üåê Global Default Terraform Variables
#
# Purpose: Provide baseline configuration defaults for infrastructure
#
# üîç Dynamic Variable Loading Mechanism:
# - Automatically loaded by Terragrunt during apply, plan, destroy commands
# - Can be overridden by environment or region-specific .tfvars files
# - Serves as a fallback configuration for infrastructure components
#
# üí° Usage in root.hcl:
# - Dynamically included via extra_arguments "optional_vars"
# - Supports flexible, hierarchical configuration management
# - Enables environment-agnostic default settings
</file>

<file path="scripts/hooks/pre-commit-init.sh">
#!/usr/bin/env bash
# shellcheck disable=SC2317

# Pre-Commit Hook Management Script
#
# This script provides functionality to manage pre-commit hooks for the repository.
# It follows Google's Bash Style Guide and provides reliable hook management.

# Strict error handling
set -euo pipefail

# Logging function with timestamp and color
log() {
    local -r level="${1}"
    local -r message="${2}"
    local -r timestamp=$(date "+%Y-%m-%d %H:%M:%S")
    local color=""

    case "${level}" in
        INFO)    color="\033[0;32m" ;;  # Green
        WARNING) color="\033[0;33m" ;;  # Yellow
        ERROR)   color="\033[0;31m" ;;  # Red
        *)       color="\033[0m" ;;     # Default
    esac

    # shellcheck disable=SC2059
    printf "${color}[${level}] ${timestamp}: ${message}\033[0m\n" >&2
}

# Ensure pre-commit is installed
ensure_pre_commit_installed() {
    if ! command -v pre-commit &> /dev/null; then
        log ERROR "pre-commit is not installed. Installing via pip..."
        if ! pip3 install pre-commit; then
            log ERROR "Failed to install pre-commit. Please install manually."
            return 1
        fi
    fi
    log INFO "pre-commit is installed and ready."
}

# Verify hook installation
verify_hook_installation() {
    local hook_types=("pre-commit" "pre-push")
    local git_dir
    git_dir=$(git rev-parse --git-dir)

    for hook_type in "${hook_types[@]}"; do
        if [ ! -f "${git_dir}/hooks/${hook_type}" ]; then
            log ERROR "Hook ${hook_type} not installed correctly"
            return 1
        fi
    done

    log INFO "All Git hooks verified successfully"
}

# Install pre-commit hooks
# Exposed function 1: Initialize repository hooks
pc_init() {
    ensure_pre_commit_installed

    log INFO "Updating pre-commit hooks to the latest version..."
    if ! pre-commit autoupdate; then
        log WARNING "Failed to update pre-commit hooks to the latest version. Continuing with existing hooks."
    fi

    log INFO "Installing pre-commit hooks..."
    if ! pre-commit install; then
        log ERROR "Failed to install pre-commit hooks"
        return 1
    fi

    if ! pre-commit install --hook-type pre-commit; then
        log ERROR "Failed to install pre-commit hooks for commit stage"
        return 1
    fi

    if ! pre-commit install --hook-type pre-push; then
        log ERROR "Failed to install pre-commit hooks for pre-push stage"
        return 1
    fi

    # Verify hook installation
    if ! verify_hook_installation; then
        log ERROR "Hook verification failed. Please check your Git configuration."
        return 1
    fi

    # Configure Git to always run hooks
    if ! git config --global core.hooksPath .git/hooks; then
        log WARNING "Could not set global hooks path. Hooks might not run automatically."
    fi

    log INFO "All pre-commit hooks installed and verified successfully"
}

# Run pre-commit hooks on all files
# Exposed function 2: Run hooks across all files
pc_run() {
    log INFO "Running pre-commit hooks on all files..."
    if ! pre-commit run --all-files; then
        log ERROR "Pre-commit hooks failed on some files"
        return 1
    fi
    log INFO "Pre-commit hooks completed successfully"
}

# Main function for script execution
main() {
    local command="${1:-}"

    case "${command}" in
        init)
            pc_init
            ;;
        run)
            pc_run
            ;;
        *)
            log ERROR "Invalid command. Use 'init' or 'run'."
            exit 1
            ;;
    esac
}

# Allow sourcing for function access or direct script execution
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
</file>

<file path=".shellcheckrc">
# ShellCheck configuration file
# https://github.com/koalaman/shellcheck/wiki/Ignore

# Specify shell dialect
shell=bash

# Disable specific warnings
# SC2155: Declare and assign separately to avoid masking return values
# We're intentionally using compact variable declarations in some cases
disable=SC2155

# SC2250: Prefer putting braces around variable references
# This is a style warning that's redundant with require-variable-braces
disable=SC2250

# Enable optional checks
# require-variable-braces: Suggest putting braces around all variable references
# quote-safe-variables: Suggest quoting variables without metacharacters
# check-unassigned-uppercase: Warn when uppercase variables are unassigned
# deprecate-which: Suggest 'command -v' instead of 'which'
enable=require-variable-braces
enable=quote-safe-variables
enable=check-unassigned-uppercase
enable=deprecate-which

# Set severity level (error, warning, info, style)
severity=warning

# External sources
# Allow sourcing of external files
external-sources=true
</file>

<file path=".yamlfmt.yml">
---
# yamlfmt configuration file
# Comprehensive configuration to align with yamllint rules

# Basic formatting rules
formatter:
  # Indent with 2 spaces for GitHub Actions workflows
  indent: 2

  # Line length configuration
  max_line_length: 120

  # Sequence and document handling
  include_document_start: true
  retain_line_breaks: true
  canonical_lists: true

# File matching
include:
  - '*.yaml'
  - '*.yml'

# Ignore patterns
exclude:
  - 'vendor/**'
  - 'node_modules/**'
  - 'dist/**'

# Specific formatting rules
rules:
  # Braces and brackets spacing
  braces:
    max_spaces_inside: 1
  brackets:
    max_spaces_inside: 1

  # Empty line control
  empty_lines:
    max: 2
    max_start: 0
    max_end: 0

  # Truthy values handling
  truthy:
    allowed_values: ['true', 'false', 'yes', 'no']

  # Trailing whitespace and line endings
  trailing_whitespace: true
  line_ending: unix

  # Comments formatting
  comments:
    min_spaces_from_content: 1
    require_starting_space: true

  # Indentation rules
  indentation:
    indent_sequences: true
    check_multi_line_strings: true

  # Workflow-specific rules
  workflow:
    # Ensure consistent indentation for GitHub Actions workflows
    indent_workflow_steps: true
    # Normalize truthy values
    normalize_truthy: true
</file>

<file path=".yamllint.yml">
---
# yamllint configuration file
# Flexible configuration for various YAML files, with special handling for GitHub Actions

extends: default

rules:
  # # Indentation rules with more flexibility
  # indentation:
  #   # Allow 2 spaces for indentation
  #   spaces: 2
  #   indent-sequences: true
  #   check-multi-line-strings: true

  # Line length with some exceptions
  line-length:
    max: 250
    level: warning
    # Allow longer lines in specific files or sections
    allow-non-breakable-words: true
    allow-non-breakable-inline-mappings: true
    ignore:
      - '.github/workflows/*.yaml'
      - '.github/workflows/*.yml'

  # Truthy values handling
  truthy:
    allowed-values: ['true', 'false', 'yes', 'no']
    check-keys: false # More lenient for workflow files

  # Comments formatting
  comments:
    min-spaces-from-content: 1
    require-starting-space: true

  # Empty line control
  empty-lines:
    max: 2
    max-start: 0
    max-end: 0

  # Braces and brackets
  braces:
    max-spaces-inside: 1
    level: warning
  brackets:
    max-spaces-inside: 1
    level: warning

# Ignore patterns
ignore: |
  vendor/
  node_modules/
  dist/
  .github/stale.yml
  .pre-commit-config.yaml
  **/.terraform-docs.yml

# Specific overrides for GitHub Actions workflows
overrides:
  - files:
      - '.github/workflows/*.yml'
      - '.github/workflows/*.yaml'
    rules:
      # Even more flexible indentation for workflow files
      indentation:
        spaces: 2
      # Slightly more relaxed line length for workflow files
      line-length:
        max: 140
        level: warning
      # More lenient truthy checks
      truthy:
        level: warning
</file>

<file path="flake.lock">
{
  "nodes": {
    "flake-utils": {
      "inputs": {
        "systems": "systems"
      },
      "locked": {
        "lastModified": 1731533236,
        "narHash": "sha256-l0KFg5HjrsfsO/JpG+r7fRrqm12kzFHyUHqHCVpMMbI=",
        "owner": "numtide",
        "repo": "flake-utils",
        "rev": "11707dc2f618dd54ca8739b309ec4fc024de578b",
        "type": "github"
      },
      "original": {
        "owner": "numtide",
        "repo": "flake-utils",
        "type": "github"
      }
    },
    "nixpkgs": {
      "locked": {
        "lastModified": 1743964447,
        "narHash": "sha256-nEo1t3Q0F+0jQ36HJfbJtiRU4OI+/0jX/iITURKe3EE=",
        "owner": "NixOS",
        "repo": "nixpkgs",
        "rev": "063dece00c5a77e4a0ea24e5e5a5bd75232806f8",
        "type": "github"
      },
      "original": {
        "owner": "NixOS",
        "ref": "nixos-unstable",
        "repo": "nixpkgs",
        "type": "github"
      }
    },
    "root": {
      "inputs": {
        "flake-utils": "flake-utils",
        "nixpkgs": "nixpkgs"
      }
    },
    "systems": {
      "locked": {
        "lastModified": 1681028828,
        "narHash": "sha256-Vy1rq5AaRuLzOxct8nz4T6wlgyUR7zLU309k9mBC768=",
        "owner": "nix-systems",
        "repo": "default",
        "rev": "da67096a3b9bf56a91d16901293e51ba5b49a27e",
        "type": "github"
      },
      "original": {
        "owner": "nix-systems",
        "repo": "default",
        "type": "github"
      }
    }
  },
  "root": "root",
  "version": 7
}
</file>

<file path="flake.nix">
{
  description = "Terragrunt Reference Architecture Development Environment";

  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
    flake-utils.url = "github:numtide/flake-utils";
  };

  outputs =
    {
      self,
      nixpkgs,
      flake-utils,
    }:
    flake-utils.lib.eachDefaultSystem (
      system:
      let
        pkgs = import nixpkgs {
          inherit system;
          config = {
            allowUnfree = true;
            allowUnfreePredicate =
              pkg:
              builtins.elem (pkgs.lib.getName pkg) [
                "terraform"
                "opentofu"
              ];
          };
        };

        # Essential tools - minimal set for fast shell startup
        essentialTools = with pkgs; [
          # Core infrastructure tools
          terraform
          terragrunt

          # Basic utilities
          git
          bash
        ];

        # Extended tools - available in the full development shell
        extendedTools = with pkgs; [
          # Go toolchain
          go
          go-tools
          golangci-lint

          # Additional Terraform tools
          terraform-ls
          tflint
          opentofu
          terraform-docs

          # Development and utility tools
          just

          # YAML tools
          yamllint
          yamlfmt

          # Shell scripting
          shellcheck

          # Environment management
          direnv
        ];

        # Show versions of key tools - extracted to a function to keep shellHook minimal
        showVersions = pkgs.writeShellScriptBin "show-versions" ''
          echo "Tool versions:"
          echo "-------------"
          echo "Terraform: $(terraform version | head -n 1)"
          echo "Terragrunt: $(terragrunt --version)"
          [ -x "$(command -v tofu)" ] && echo "OpenTofu: $(tofu version | head -n 1)"
          [ -x "$(command -v go)" ] && echo "Go: $(go version)"
          echo ""
          echo "Run 'show-versions' anytime to see this information again."
        '';
      in
      {
        # Fast startup default shell with minimal tools
        devShells.default = pkgs.mkShell {
          buildInputs = essentialTools ++ [ showVersions ];

          shellHook = ''
            echo "üöÄ Fast Terragrunt Ref Arch Shell (minimal) üõ†Ô∏è"
            echo "Type 'show-versions' for tool version information"
            echo "For full development environment with all tools: nix develop .#full"
          '';
        };

        # Full development shell with all tools
        devShells.full = pkgs.mkShell {
          buildInputs = essentialTools ++ extendedTools ++ [ showVersions ];

          shellHook = ''
            echo "üöÄ Complete Terragrunt Ref Arch Development Environment üõ†Ô∏è"
            echo "Type 'show-versions' for tool version information"
          '';
        };
      }
    );
}
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2021-2024 Alex Torres

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path=".github/ISSUE_TEMPLATE/bug_report.yml">
---
name: Bug Report
description: Report an issue in the Terragrunt Reference Architecture
title: "[Bug]: "
labels: ["bug", "triage"]
body:
  - type: markdown
    attributes:
      value: |
        Thanks for helping improve the Terragrunt Reference Architecture.

  - type: dropdown
    id: component
    attributes:
      label: Affected Component
      description: Select the primary component experiencing the issue
      options:
        - Root Configuration
        - Shared Configuration
        - Environment Configuration
        - Stack Configuration
        - Unit Configuration
        - CLI/Tooling
        - Other
    validations:
      required: true

  - type: textarea
    id: description
    attributes:
      label: Problem Description
      description: Provide a clear, concise explanation of the unexpected behavior
      placeholder: What went wrong and under what circumstances?
    validations:
      required: true

  - type: input
    id: terragrunt-version
    attributes:
      label: Terragrunt Version
      placeholder: e.g., 0.45.6
    validations:
      required: true

  - type: input
    id: terraform-version
    attributes:
      label: Terraform Version
      placeholder: e.g., 1.5.3
    validations:
      required: true

  - type: textarea
    id: reproduction
    attributes:
      label: Steps to Reproduce
      description: Provide minimal, precise steps to consistently reproduce the issue
      placeholder: |
        1. Run '...'
        2. Configure '...'
        3. Observe error
    validations:
      required: true

  - type: textarea
    id: logs
    attributes:
      label: Relevant Logs
      description: Paste any error logs or command output
      render: shell
    validations:
      required: false

  - type: dropdown
    id: severity
    attributes:
      label: Issue Severity
      description: How critical is this issue?
      options:
        - Critical (Workflow Blocking)
        - High (Significant Impact)
        - Medium (Partial Functionality)
        - Low (Minor Issue)
    validations:
      required: true

  - type: textarea
    id: additional-context
    attributes:
      label: Additional Context
      description: Any other relevant information
      placeholder: Configuration details, custom modifications, etc.
    validations:
      required: false

  - type: checkboxes
    id: terms
    attributes:
      label: Contribution Guidelines
      description: By submitting this issue, you agree to follow our project's guidelines
      options:
        - label: I have searched existing issues and verified this is not a duplicate
          required: true
</file>

<file path=".github/ISSUE_TEMPLATE/feature_request.yml">
---
name: Feature Request
description: Propose improvements for the Terragrunt Reference Architecture
title: "[Feature]: "
labels: ["enhancement", "needs-triage"]
body:
  - type: markdown
    attributes:
      value: |
        Thanks for contributing to the Terragrunt Reference Architecture.

  - type: dropdown
    id: component
    attributes:
      label: Affected Component
      description: Select the primary component this feature impacts
      options:
        - Root Configuration
        - Shared Configuration
        - Environment Configuration
        - Stack Configuration
        - Unit Configuration
        - CLI/Tooling
        - Cross-cutting Concern
    validations:
      required: true

  - type: textarea
    id: problem
    attributes:
      label: Problem Statement
      description: Describe the specific limitation or challenge in the current architecture
      placeholder: What problem are you trying to solve?
    validations:
      required: true

  - type: textarea
    id: solution
    attributes:
      label: Proposed Solution
      description: Outline your proposed feature or improvement
      placeholder: |
        - Key functionality
        - Implementation approach
        - Configuration modifications
    validations:
      required: true

  - type: textarea
    id: use-cases
    attributes:
      label: Use Cases
      description: Describe specific scenarios where this feature provides value
      placeholder: |
        1. Primary Use Case:
        2. Secondary Use Case:
    validations:
      required: false

  - type: dropdown
    id: priority
    attributes:
      label: Feature Priority
      description: How critical is this feature?
      options:
        - High (Critical for workflow)
        - Medium (Significant improvement)
        - Low (Nice to have)
    validations:
      required: true

  - type: textarea
    id: technical-considerations
    attributes:
      label: Technical Considerations
      description: Compatibility and configuration impact
      placeholder: |
        - Terragrunt version requirements
        - Potential configuration changes
        - Provider compatibility
    validations:
      required: false

  - type: textarea
    id: additional-context
    attributes:
      label: Additional Context
      description: Any supplementary information
      placeholder: Links to documentation, references, or related discussions
    validations:
      required: false

  - type: checkboxes
    id: contribution
    attributes:
      label: Contribution Details
      description: Are you willing to help implement this feature?
      options:
        - label: I'm interested in contributing to this feature
        - label: I have searched existing issues and verified this is not a duplicate
          required: true
</file>

<file path=".github/workflows/labels-assigner.yml">
---
name: "Pull Request Labeler"
on:
  pull_request_target:
    types: [opened, labeled, unlabeled, synchronize]
permissions:
  contents: read
  pull-requests: write
jobs:
  labeler:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/labeler@v5
        with:
          repo-token: "${{ secrets.GITHUB_TOKEN }}"
          configuration-path: .github/labeler.yml
          sync-labels: true
</file>

<file path=".github/workflows/lock-threads.yml">
---
name: Lock Inactive Threads

on:
  schedule:
    - cron: '0 0 * * 0' # Run weekly on Sunday at midnight

permissions:
  issues: write
  pull-requests: write

jobs:
  lock:
    runs-on: ubuntu-latest
    steps:
      - uses: dessant/lock-threads@v4
        with:
          github-token: ${{ github.token }}
          issue-lock-comment: >
            This issue has been inactive for 30 days and is now locked. If you have a similar concern, please open a new issue
            with detailed information about the Terragrunt Reference Architecture. üèóÔ∏è
          issue-lock-inactive-days: '30'
          pr-lock-comment: >
            This pull request has been inactive for 30 days and is now locked. If you wish to continue this contribution,
            please open a new PR with updated changes. üöß
          pr-lock-inactive-days: '30'
</file>

<file path=".github/workflows/semantic-pr.yml">
---
name: Semantic Pull Request Validation

on:
  pull_request_target:
    types:
      - opened
      - edited
      - synchronize

permissions:
  pull-requests: read

jobs:
  semantic-pr:
    runs-on: ubuntu-latest
    steps:
      - uses: amannn/action-semantic-pull-request@v5
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          requireScope: false
          validateSingleCommit: true
          types: |
            feat
            fix
            docs
            refactor
            perf
            test
            build
            ci
            chore
            infra
            breaking
</file>

<file path=".github/config.yml">
---
# Configuration for new-issue-welcome - https://github.com/behaviorbot/new-issue-welcome
# Comment to be posted to on first time issues
newIssueWelcomeComment: >
  Thanks for opening an issue in the Terragrunt Reference Architecture! üèóÔ∏è Please ensure you've followed our issue template
  and provided detailed information.
# Configuration for new-pr-welcome - https://github.com/behaviorbot/new-pr-welcome
# Comment to be posted to on PRs from first time contributors in your repository
newPRWelcomeComment: >
  Welcome to the Terragrunt Reference Architecture! üöÄ Thank you for your contribution. Please review our contributing guidelines
  and PR checklist.
# Configuration for first-pr-merge - https://github.com/behaviorbot/first-pr-merge
# Comment to be posted to on pull requests merged by a first time user
firstPRMergeComment: >-
  Congratulations on merging your first pull request to the Terragrunt Reference Architecture! Your contribution helps improve
  infrastructure as code practices. üéâ
</file>

<file path=".github/no-response.yml">
---
# Configuration for probot-no-response - https://github.com/probot/no-response

# Number of days of inactivity before an Issue is closed for lack of response
daysUntilClose: 21
# Label requiring a response
responseRequiredLabel: needs-info
# Comment to post when closing an Issue for lack of response. Set to `false` to disable
closeComment: >-
  This issue has been automatically closed due to lack of response. For Terragrunt Reference Architecture issues, please provide:
  - Specific configuration context - Exact Terragrunt/Terraform version - Detailed error logs or configuration snippets Feel
  free to reopen with more details. üèóÔ∏è
</file>

<file path=".github/stale.yml">
---
# Number of days of inactivity before an issue becomes stale
daysUntilStale: 90
# Number of days of inactivity before a stale issue is closed
daysUntilClose: 14
# Issues with these labels will never be considered stale
exemptLabels:
  - pinned
  - security
  - infra/critical
  - infra/core-component
  - tool/core-functionality
  - config/root
  - config/shared
# Label to use when marking an issue as stale
staleLabel: stale
# Comment to post when marking an issue as stale
markComment: >
  This issue in the Terragrunt Reference Architecture has been automatically marked as stale due to 90 days of inactivity.
  It will be closed if no further activity occurs within 14 days. If this issue remains relevant, please comment to keep it
  open. üèóÔ∏è
# Comment to post when closing a stale issue
closeComment: >
  This issue has been automatically closed due to inactivity. If the issue is still relevant to the Terragrunt Reference Architecture,
  please reopen and provide updated context. Feel free to create a new issue with the most recent information about infrastructure
  configuration. üëã
</file>

<file path=".gitlab/issue_templates/bug_report.md">
<!-- Title suggestion: [Bug]: Brief description -->

/title [Bug]:

## Summary

(Summarize the bug encountered concisely)

## Environment Details

- Terragrunt Ref Arch Version:
- Terragrunt Version:
- Terraform Version:
- Operating System:

## Component Affected

Select the primary component experiencing the issue:

- [ ] Root Configuration
- [ ] Shared Configuration
- [ ] Environment Configuration
- [ ] Stack Configuration
- [ ] Unit Configuration
- [ ] CLI/Tooling
- [ ] Other (specify)

## Steps to Reproduce

1.
2.
3.

## Expected vs Actual Behavior

**Expected:**
(Describe what should happen)

**Actual:**
(Describe what actually happens)

## Diagnostic Information

### Configuration Files Affected
- [ ] root.hcl
- [ ] config.hcl
- [ ] terragrunt.hcl
- [ ] unit_cfg_providers.hcl
- [ ] unit_cfg_versions.hcl
- [ ] Other (specify)

### Relevant Logs

```
(Paste any relevant logs - please use code blocks)
```

### Error Messages

```
(Paste any error messages here)
```

## Impact Assessment

**Severity:**
- [ ] Critical (workflow blocking)
- [ ] High (significant functionality impaired)
- [ ] Medium (partial functionality affected)
- [ ] Low (minor issue)

**Impacted Workflows/Components:**
(List affected workflows or components)

## Additional Context

(Add any other context about the problem here)

## Proposed Solution

(If you have any ideas for how to solve the issue, add them here)

/label ~bug ~needs-investigation
/assign me
</file>

<file path=".gitlab/issue_templates/feature_request.md">
<!-- Title suggestion: [Feature]: Brief description -->

/title [Feature]:

## Feature Request

Thanks for contributing to the Terragrunt Reference Architecture.

## Component Affected

Select the primary component this feature impacts:

- [ ] Root Configuration
- [ ] Shared Configuration
- [ ] Environment Configuration
- [ ] Stack Configuration
- [ ] Unit Configuration
- [ ] CLI/Tooling
- [ ] Cross-cutting Concern

## Problem Statement

(Describe the specific limitation or challenge in the current architecture)

## Proposed Solution

Outline your proposed feature or improvement:

- Key functionality:
- Implementation approach:
- Configuration modifications:

## Use Cases

1. Primary Use Case:
   (Describe the main scenario where this feature provides value)

2. Secondary Use Case:
   (Describe additional scenarios where this feature would be useful)

## Feature Priority

Select the priority level:

- [ ] High (Critical for workflow)
- [ ] Medium (Significant improvement)
- [ ] Low (Nice to have)

## Technical Considerations

- Terragrunt version requirements:
- Potential configuration changes:
- Provider compatibility:

## Additional Context

(Add any supplementary information, links to documentation, references, or related discussions)

## Contribution Interest

- [ ] I'm interested in contributing to this feature
- [ ] I have searched existing issues and verified this is not a duplicate

/label ~enhancement ~needs-triage
/assign me
</file>

<file path=".gitlab/merge_request_templates/default.md">
<!-- Title suggestion: [Component]: Brief description -->

/title [Component]:

## üèóÔ∏è Terragrunt Reference Architecture MR

## What Changes

- Brief description of changes:
- Key modifications or enhancements to infrastructure configuration:

## Change Type

Select all that apply:

- [ ] Terragrunt Configuration
- [ ] Shared Configuration
- [ ] Environment Configuration
- [ ] Terraform Module
- [ ] Documentation
- [ ] Dependency Update

## Checklist

- [ ] Followed Terragrunt best practices
- [ ] Maintained configuration modularity
- [ ] Added/updated tests
- [ ] Updated documentation
- [ ] Verified cross-environment compatibility
- [ ] Ensured DRY (Don't Repeat Yourself) principles

## Configuration Impact

### Affected Components
(List affected stacks/environments)

### Breaking Changes
(List any potential breaking changes)

### Performance Considerations
(Describe any performance impacts)

## Additional Context

- Related issues:
- Configuration diffs:
- Screenshots or logs (if applicable):

/label ~feature
/assign me
/milestone %"Next Release"
</file>

<file path="ci/README.md">
# Terragrunt CI Dagger Module

This Dagger module provides a reusable environment and functions for running Terragrunt and Terraform commands, primarily intended for Continuous Integration (CI) workflows within the context of the `terragrunt-ref-arch-v3` repository structure.

## Core Concepts

*   **Dagger Module:** This is a self-contained unit of Dagger configuration and code, written in Go, that exposes functions callable by Dagger clients (CLI, other SDKs). It encapsulates the setup and execution logic for Terragrunt/Terraform tasks.
*   **`Terragrunt` Struct:** The central component of this module (`main.go`). It holds the Dagger `Container` (`Ctr`) where commands are executed and a reference to the source `Directory` (`Src`).
*   **Container Setup:** The `New()` constructor initializes the module. It can:
    *   Use a pre-built container image (`imageURL`).
    *   Use a custom container (`ctr`).
    *   Build a default container based on Alpine (`main.go:defaultImage`), installing specific versions of Git, Terraform (`main.go:defaultTerraformVersion`), and Terragrunt (`main.go:defaultTerragruntVersion`).
    *   The `CommonSetup()` method (`main.go`) applies standard configurations like installing tools and setting up caching.
*   **Caching:** The module configures Dagger cache volumes for Terraform plugins (`main.go:configterraformPluginCachePath`) and Terragrunt artifacts (`main.go:configterragruntCachePath`) to speed up subsequent runs. It also enables the Terragrunt provider cache server by default (`WithTerragruntProvidersCacheServerEnabled`). Caching can be bypassed using the `noCache` flag in job functions or the `WithCacheBuster` method.

## Features & Configuration (`main.go`)

The module offers various methods (primarily on the `Terragrunt` struct) to configure the execution environment:

*   **Source Code:** `WithSRC()` mounts your project source code into the container.
*   **Tool Versions:** `WithTerraform()`, `WithTerragrunt()` install specific tool versions if not using a pre-built image.
*   **Environment Variables:**
    *   `WithEnvVars()`: Sets multiple environment variables.
    *   `WithDotEnvFile()`: Loads variables from `.env` files found in the source directory.
*   **Secrets:**
    *   `WithSecrets()`: Mounts multiple Dagger secrets as environment variables.
    *   `WithToken()`: A specific helper for mounting a single secret.
    *   `WithAWSOIDC()`: Configures AWS authentication via OIDC, mounting the token as a secret file.
    *   `WithAWSKeys()`: Configures AWS authentication using access keys (secrets).
    *   `WithGitlabToken()`, `WithGitHubToken()`, `WithTerraformToken()`: Specific helpers for common tokens (mounted as env vars).
*   **Authentication:**
    *   `WithNewNetrcFile*()`: Creates a `.netrc` file for Git authentication (GitHub/GitLab). Use `WithNewNetrcFileAsSecret*` for secure password handling.
    *   `WithSSHAuthSocket()`: Mounts an SSH agent socket for Git SSH authentication.
    *   AWS Auth: Covered by `WithAWSKeys` and `WithAWSOIDC`.
*   **Caching:**
    *   `WithTerraformPluginCache()`: Configures TF plugin cache dir and env var.
    *   `WithTerragruntCache()`: Configures TG cache volume.
    *   `WithTerragruntProvidersCacheServerEnabled() / Disabled()`: Toggles the TG provider cache server.
    *   `WithRegistriesToCacheProvidersFrom()`: Adds custom registries to the TG provider cache.
    *   `WithCacheBuster()`: Adds an environment variable to break build cache layers if needed.
*   **Terragrunt Options:**
    *   `WithTerragruntLogLevel()`: Sets `TERRAGRUNT_LOG_LEVEL`.
    *   `WithTerragruntNonInteractive()`: Sets `TERRAGRUNT_NON_INTERACTIVE`.
    *   `WithTerragruntNoColor()`: Sets `TERRAGRUNT_NO_COLOR`.
*   **Terminal:** `OpenTerminal()` provides an interactive terminal within the fully configured container.

## Available Jobs

The module provides pre-defined functions for common CI tasks:

*   **`JobTerraformModulesStaticCheck(ctx)` (`job_ci_tf.go`):**
    *   **Purpose:** Performs static checks (`init -backend=false`, `validate`, `fmt -recursive`) on Terraform modules.
    *   **Note:** Currently iterates over a hardcoded list of module names specific to this reference architecture.
    *   **Execution:** Synchronous execution across modules.
*   **`JobTerragruntUnitsStaticCheck(ctx, ...)` (`job_ci_tg.go`):**
    *   **Purpose:** Performs static checks (`init`, `terragrunt-info`, `hclfmt --check`, `validate-inputs`, `hclvalidate`) on Terragrunt units (directories containing `terragrunt.hcl`).
    *   **Parameters:** Accepts AWS credentials/OIDC config, cache options, custom arguments (`tgArgs`), env vars.
    *   **Note:** Currently iterates over a hardcoded list of unit names specific to this reference architecture (`dni_generator`, etc.) within a fixed env/layer (`global`/`dni`).
    *   **Execution:** Asynchronous execution across units using goroutines.
*   **`JobTerragruntUnitsPlan(ctx, ...)` (`job_ci_tg.go`):**
    *   **Purpose:** Runs `terragrunt plan` on Terragrunt units.
    *   **Parameters:** Accepts AWS credentials/OIDC config, cache options, custom arguments (`tgArgs`), env vars.
    *   **Note:** Currently iterates over a hardcoded list of unit names specific to this reference architecture.
    *   **Execution:** Asynchronous execution across units using goroutines.

*   **Generic Execution:** `Exec(ctx, binary, command, args...)` (`main.go`):
    *   **Purpose:** Allows executing arbitrary `terragrunt` or `terraform` commands with fine-grained control over arguments, environment variables, secrets, tokens, AWS auth, SSH sockets, etc.
    *   **Use Case:** Useful for running commands not covered by the pre-defined jobs or for custom workflows.

## Usage Example (Dagger Go Client)

```go
package main

import (
	"context"
	"fmt"
	"log"
	"os"

	"dagger/terragrunt/internal/dagger"
)

func main() {
	ctx := context.Background()

	// Initialize Dagger client
	client, err := dagger.Connect(ctx, dagger.WithLogOutput(os.Stdout))
	if err != nil {
		log.Fatalf("Error connecting to Dagger engine: %v", err)
	}
	defer client.Close()

	// Get reference to source code directory
	srcDir := client.Host().Directory(".") // Assumes run from repo root

	// Initialize the Terragrunt module
	// Uses default versions and Alpine base image
	tgModule, err := client.Terragrunt().New(ctx,
		// No imageURL, tgVersion, tfVersion specified, will use defaults
		dagger.TerragruntNewOpts{
			SrcDir: srcDir,
			// Add any necessary EnvVars here if not using .env files
			// EnvVars: []string{"MY_VAR=value"},
		},
	)
	if err != nil {
		log.Fatalf("Error initializing Terragrunt module: %v", err)
	}

	// Example: Run Terragrunt static checks
	// Assumes AWS credentials are set as environment variables or via OIDC in the CI environment
	// Secrets need to be created using client.SetSecret()
	// awsAccessKey = client.SetSecret(...)
	// awsSecretKey = client.SetSecret(...)
	// oidcToken = client.SetSecret(...)

	output, err := tgModule.JobTerragruntUnitsStaticCheck(ctx,
		dagger.TerragruntJobTerragruntUnitsStaticCheckOpts{
			// Pass AWS secrets if needed:
			// AwsAccessKeyID: awsAccessKey,
			// AwsSecretAccessKey: awsSecretKey,
			// Or OIDC token:
			// AwsOidcToken: oidcToken,
			// AwsRoleArn: "arn:aws:iam::ACCOUNT:role/MyCIRole",
			LoadDotEnvFile: true, // Example: Load .env files from srcDir root
		},
	)

	if err != nil {
		log.Fatalf("Error running Terragrunt static checks: %v", err)
	}

	fmt.Println("Terragrunt static checks completed successfully:")
	fmt.Println(output)

	// Example: Run Terraform module checks
	tfModOutput, err := tgModule.JobTerraformModulesStaticCheck(ctx)
	if err != nil {
		log.Fatalf("Error running Terraform module checks: %v", err)
	}
	fmt.Println("Terraform module checks completed successfully:")
	fmt.Println(tfModOutput)
}

```

## Extending the Module

This module can be extended to add new functionality or adapt existing jobs:

1.  **Adding New Jobs:**
    *   Create a new file (e.g., `job_my_custom_task.go`).
    *   Define a new method on the `*Terragrunt` struct (e.g., `func (m *Terragrunt) JobMyCustomTask(...)`).
    *   Within the method, use the existing `m.Ctr` and `With*` methods (`main.go`) to configure the container as needed.
    *   Use `m.Ctr.WithExec([]string{"terragrunt", "your-command", ...})` to run the desired command(s).
    *   Return the output string and error, potentially using the result processing helpers from `job.go` if running multiple steps or concurrently.
2.  **Modifying Existing Jobs:**
    *   Update the hardcoded lists of modules/units in `job_ci_tf.go` and `job_ci_tg.go` or modify the logic to dynamically discover them (e.g., by scanning the source directory).
    *   Adjust the specific `WithExec` commands within the job functions.
3.  **Adding Configuration:**
    *   Add new `With*` methods to `main.go` to support additional configuration options (e.g., different cloud providers, new tools).
    *   These methods should typically modify `m.Ctr` by adding environment variables, mounting files/secrets, or installing packages.

Refer to the Dagger Go SDK documentation (concepts similar to those in `docs/dagger.io`) for more details on interacting with containers, directories, secrets, and executing commands.
</file>

<file path="docs/environment-variables.md">
# Environment Variables Management

## Overview

The Terragrunt Reference Architecture implements a simplified yet powerful environment variable management system powered by [direnv](https://direnv.net/). This architecture now uses a single root `.envrc` file with category-based organization for improved clarity and easier customization.

## Key Features

- **Category-Based Organization**: Variables grouped by functional categories
- **Secure Variable Handling**: Validation and export of environment variables
- **Flexible Customization**: Dedicated section for custom user variables
- **Visual Clarity**: Emoji-tagged sections for improved readability

## Configuration Structure

```
/
‚îî‚îÄ‚îÄ .envrc                      # Comprehensive root-level configuration
```

## Configuration Principles

### Organizational Structure

The root `.envrc` file is organized into the following categories:

1. **Project Metadata**: Core project information and authorship
2. **Cloud Provider Settings**: Region configuration and provider-specific settings
3. **Terraform & Terragrunt Configuration**: Tool versions and behavior settings
4. **Logging & Diagnostics**: Output verbosity and log storage
5. **Remote State Configuration**: Backend storage for Terraform state
6. **Custom Use-Case Variables**: User-defined environment variables

### Core Utility Functions

- `_safe_export`: Securely export environment variables
- `_layer_export`: Export variables with additional layer-specific logging
- `_display_exported_vars`: Display current environment configuration
- `_log`: Standardized logging mechanism
- `_layer_env_info`: Display organized layer information with descriptions

## Root .envrc (Comprehensive Configuration)

```bash
#!/usr/bin/env bash
# Terragrunt Reference Architecture - Environment Configuration
# Simplified and modular environment setup

# Exit immediately if any command fails
set -e

# Ensure PROJECT_ROOT is set reliably
PROJECT_ROOT="${PROJECT_ROOT:-$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)}"
export PROJECT_ROOT

# Source utility functions
source "${PROJECT_ROOT}/scripts/envrc-utils.sh"

# Core initialization
_core_init

# =====================================================================
# üîß CUSTOMIZATION SECTION
# =====================================================================
# Configuration variables are grouped by functional categories for easier
# customization and maintenance.

# ---------------------------------------------------------------------
# 1Ô∏è‚É£ PROJECT METADATA
# ---------------------------------------------------------------------
# Define core project information and authorship
# ---------------------------------------------------------------------
TG_STACK_APP_AUTHOR="${TG_STACK_APP_AUTHOR:-Your Name}"
_safe_export TG_STACK_APP_AUTHOR "$TG_STACK_APP_AUTHOR"

TG_STACK_APP_PRODUCT_NAME="${TG_STACK_APP_PRODUCT_NAME:-your-app-name}"
_safe_export TG_STACK_APP_PRODUCT_NAME "$TG_STACK_APP_PRODUCT_NAME"

# ---------------------------------------------------------------------
# 2Ô∏è‚É£ CLOUD PROVIDER & REGION SETTINGS
# ---------------------------------------------------------------------
# Configure cloud provider-specific settings
# ---------------------------------------------------------------------
DEFAULT_REGION="${DEFAULT_REGION:-us-east-1}"
_safe_export DEFAULT_REGION "$DEFAULT_REGION"

# ---------------------------------------------------------------------
# 3Ô∏è‚É£ TERRAFORM & TERRAGRUNT CONFIGURATION
# ---------------------------------------------------------------------
# Control Terraform behavior and version requirements
# ---------------------------------------------------------------------
# Core Terraform Settings
TF_INPUT="${TF_INPUT:-0}"
_safe_export TF_INPUT "$TF_INPUT"

TG_STACK_TF_VERSION="${TG_STACK_TF_VERSION:-1.9.0}"
_safe_export TG_STACK_TF_VERSION "$TG_STACK_TF_VERSION"

# Terragrunt Performance Settings
TERRAGRUNT_DOWNLOAD_DIR="${TERRAGRUNT_DOWNLOAD_DIR:-${HOME}/.terragrunt-cache/$(basename "${PROJECT_ROOT}")}"
_safe_export TERRAGRUNT_DOWNLOAD_DIR "$TERRAGRUNT_DOWNLOAD_DIR"

TERRAGRUNT_CACHE_MAX_AGE="${TERRAGRUNT_CACHE_MAX_AGE:-168h}"
_safe_export TERRAGRUNT_CACHE_MAX_AGE "$TERRAGRUNT_CACHE_MAX_AGE"

# Terragrunt Behavior Settings
TERRAGRUNT_LOG_LEVEL="${TERRAGRUNT_LOG_LEVEL:-info}"
_safe_export TERRAGRUNT_LOG_LEVEL "$TERRAGRUNT_LOG_LEVEL"

TERRAGRUNT_AUTO_INIT="${TERRAGRUNT_AUTO_INIT:-true}"
_safe_export TERRAGRUNT_AUTO_INIT "$TERRAGRUNT_AUTO_INIT"

# ---------------------------------------------------------------------
# 6Ô∏è‚É£ CUSTOM USE-CASE VARIABLES
# ---------------------------------------------------------------------
# Add your custom environment variables below
# Examples:
# _safe_export TG_CUSTOM_VAR_1 "value1"
# _safe_export TG_CUSTOM_VAR_2 "value2"
# ---------------------------------------------------------------------
```

## Variable Customization

### Adding New Custom Variables

To add your own custom variables, locate the "CUSTOM USE-CASE VARIABLES" section in the root `.envrc` file and add your variables there:

```bash
# ---------------------------------------------------------------------
# 6Ô∏è‚É£ CUSTOM USE-CASE VARIABLES
# ---------------------------------------------------------------------
# Add your custom environment variables below

# Development-specific settings
_safe_export TG_DEV_DEBUG_MODE "true"
_safe_export TG_DEV_API_ENDPOINT "https://dev-api.example.com"

# Production-specific settings
_safe_export TG_PROD_RESOURCE_SCALING "high"
_safe_export TG_PROD_REPLICA_COUNT "5"
```

## Best Practices

- Use descriptive variable names with appropriate prefixes
- Group related variables together in the custom section
- Use `_safe_export` for all variable exports to ensure proper validation
- Add comments to document the purpose of custom variables
- Keep sensitive information out of version control

## Troubleshooting

### Common Issues

1. **Variables Not Loading**
   - Ensure `direnv` is installed: `which direnv`
   - Run `direnv allow` in the directory
   - Check for syntax errors in the `.envrc` file

2. **Validation Failures**
   - Verify required variables are properly defined
   - Check the output of `_validate_layer_config` in the logs

### Debugging Commands

```bash
# Show all environment variables
env

# Show variables with specific prefix
env | grep TG_

# Direnv debug mode
DIRENV_LOG_FORMAT="" direnv allow
```

## Recommended Tools

- [direnv](https://direnv.net/): Environment variable management
- [sops](https://github.com/mozilla/sops): Secrets management
</file>

<file path="infra/terraform/modules/age-generator/versions.tf">
terraform {
  required_version = ">= 1.11.3"

  required_providers {
    random = {
      source  = "hashicorp/random"
      version = "~> 3.5.1"
    }
  }
}
</file>

<file path="infra/terraform/modules/dni-generator/versions.tf">
terraform {
  required_version = ">= 1.11.3"

  required_providers {
    random = {
      source  = "hashicorp/random"
      version = "~> 3.5.1"
    }
  }
}
</file>

<file path="infra/terraform/modules/lastname-generator/versions.tf">
terraform {
  required_version = ">= 1.11.3"

  required_providers {
    random = {
      source  = "hashicorp/random"
      version = "~> 3.5.1"
    }
  }
}
</file>

<file path="infra/terraform/modules/name-generator/versions.tf">
terraform {
  required_version = ">= 1.11.3"

  required_providers {
    random = {
      source  = "hashicorp/random"
      version = "~> 3.5.1"
    }
  }
}
</file>

<file path="infra/terragrunt/_shared/_config/app.hcl">
// üéØ Local Variables for Terragrunt Configuration
// This section defines local variables that will be used across different child configurations.
// These locals help maintain consistency in naming conventions and metadata throughout the architecture.
locals {
  // üì¶ Product Name
  product_name = get_env("TG_STACK_APP_PRODUCT_NAME", "my-app") // The name of the project for identification purposes.
  // üì¶ Product Version
  product_version = "0.0.1" // The version of the project.
}
</file>

<file path="infra/terragrunt/_shared/_config/tags.hcl">
# ---------------------------------------------------------------------------------------------------------------------
# üåç COMMON TAGS CONFIGURATION
# This block establishes a standardized set of metadata tags applicable to all infrastructure resources
# managed by Terraform and orchestrated by Terragrunt within the project. Tags are key-value pairs associated
# with resources that serve multiple purposes, including identification, organization, and governance of
# resources across cloud environments. üèóÔ∏è

# Utilizing a consistent tagging strategy across all modules enhances the ability to:
# - üîç Identify resources, their purpose, and their lifecycle owner at a glance.
# - üí∞ Implement cost allocation, reporting, and optimization strategies based on tags.
# - üîí Enforce security and compliance policies through tag-based resource segmentation.
# - ‚öôÔ∏è Automate operations and management tasks that depend on the categorization of resources.

# This block defines a reusable set of global tags that can be incorporated into any module by including it in the
# `locals` block. Subsequently, these tags can be merged with module-specific tags and applied to resources in the
# Terraform `resource` blocks, ensuring a unified and comprehensive tagging approach across the project's infrastructure.
# ---------------------------------------------------------------------------------------------------------------------
locals {
  tags = {
    ManagedBy      = "Terraform"                                    // üõ†Ô∏è Indicates the tool used for resource provisioning.
    OrchestratedBy = "Terragrunt"                                   // üéõÔ∏è Indicates the tool used for workflow orchestration.
    Author         = get_env("TG_STACK_APP_AUTHOR", "")             // ‚úçÔ∏è The author of the configuration.
    Type           = "infrastructure"                               // üì¶ Categorizes the resource within the broader infrastructure ecosystem.
    Application    = get_env("TG_STACK_APP_PRODUCT_NAME", "my-app") // üì± The application or service that the resource supports.
    # TODO: Add git-sha tag. Uncomment when ready. Ensure there's at least one commit in the repository.
    # "git-sha"      = run_cmd("--terragrunt-global-cache", "--terragrunt-quiet", "git", "rev-parse", "--short", "HEAD")
  }
}
</file>

<file path="infra/terragrunt/_shared/_units/age_generator.hcl">
// üß© Shared Unit Configuration for Age Generator Module

locals {
  # ---------------------------------------------------------------------------------------------------------------------
  # üèóÔ∏è ENVIRONMENT CONFIGURATION ORCHESTRATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Dynamically load and aggregate environment-specific configuration files
  # This mechanism enables a flexible, layered infrastructure configuration approach
  #
  # Configuration Layers:
  # - Environment-level settings (env.hcl)
  # - Global architecture configurations
  # - Hierarchical tag management
  #
  # Key Benefits:
  # - Modular configuration management
  # - Centralized environment settings
  # - Flexible tag inheritance
  # ---------------------------------------------------------------------------------------------------------------------
  env_cfg = read_terragrunt_config(find_in_parent_folders("env.hcl"))

  # üåê Stack Configuration
  # Loads the stack configuration file that serves as the single source of truth
  # for stack-level settings and metadata
  stack_cfg = read_terragrunt_config(find_in_parent_folders("stack.hcl"))

  # ---------------------------------------------------------------------------------------------------------------------
  # üåê GLOBAL ARCHITECTURE CONFIGURATION RESOLVER
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Centralize and standardize infrastructure-wide configuration
  # Loads the root configuration file that serves as the single source of truth
  # for infrastructure-level settings and metadata
  #
  # Key Responsibilities:
  # - Provide global configuration context
  # - Enable consistent infrastructure metadata
  # - Support cross-module configuration sharing
  # ---------------------------------------------------------------------------------------------------------------------
  cfg = read_terragrunt_config("${find_in_parent_folders("config.hcl")}")

  # ---------------------------------------------------------------------------------------------------------------------
  # üè∑Ô∏è INTELLIGENT TAG ORCHESTRATION SYSTEM
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Create a sophisticated, hierarchical tag management mechanism
  # Implements a multi-layered tagging strategy that allows for:
  # - Global tag inheritance
  # - Environment-specific tag augmentation
  # - Unit-level tag customization
  #
  # Tag Hierarchy (from broadest to most specific):
  # 1. Global Architecture Tags üåê
  # 2. Environment-Level Tags üåç
  # 3. Unit-Specific Tags üß©
  # 4. Stack-Level Tags üìö
  #
  # Benefits:
  # - Consistent resource identification
  # - Flexible tag management
  # - Enhanced resource tracking and compliance
  # ---------------------------------------------------------------------------------------------------------------------
  unit_tags = {
    Unit = "age-generator"
    Type = "random-generator"
  }

  # üîó TAG SOURCE AGGREGATION
  # Collect tags from different configuration levels
  env_tags    = local.env_cfg.locals.tags
  global_tags = local.cfg.locals.tags
  stack_tags  = local.stack_cfg.locals.tags

  # üß© FINAL TAG COMPOSITION
  # Merge tags with a clear precedence strategy
  # Precedence: Unit Tags > Environment Tags > Global Tags
  all_tags = merge(
    local.env_tags,
    local.unit_tags,
    local.global_tags,
    local.stack_tags
  )

  # ---------------------------------------------------------------------------------------------------------------------
  # üîß GIT MODULE SOURCE CONFIGURATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Intelligent Terraform module source management with:
  # - Centralized default configuration
  # - Flexible repository and path selection
  # - Semantic version control
  #
  git_base_url              = local.cfg.locals.cfg_git.git_base_urls.github
  tf_module_repository      = "your-org/terraform-modules.git"
  tf_module_version_default = get_env("TG_STACK_TF_MODULE_AGE_GENERATOR_VERSION_DEFAULT", "v0.1.0")
  tf_module_path_default    = "modules/age-generator"

  tf_module_source = format(
    "%s%s//%s",
    local.git_base_url,
    local.tf_module_repository,
    local.tf_module_path_default
  )

  echo_tf_module_source = run_cmd("sh", "-c", "echo 'üîß  TF Module Source (parent): ${local.tf_module_source} version: ${local.tf_module_version_default}'")

  # ---------------------------------------------------------------------------------------------------------------------
  # üåê SPECIFIC MODULE CONFIGURATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Here we define the specific configuration for the module.
  # This is useful when we need to override the default configuration for the module.
  #
  # TODO: Add specific locals, or configuration for your module to be computed here.
}

# üîó DEPENDENCY
# This block defines a dependency for the Terragrunt configuration.
# Dependencies allow for the management of resources that rely on
# other configurations, ensuring that they are created or updated
# in the correct order. This promotes modularity and reusability
# of infrastructure components, making it easier to manage complex
# setups. Dependencies can also include mock outputs for testing
# purposes without needing to provision the actual resources.
# dependency "cloudflare_dns_zone" {
#   config_path = find_in_parent_folders("<stack>/<unit>")
#   mock_outputs = {
#     cloudflare_zone_ids = {
#       "fake-zone-id" = "fake-zone-id"
#     }
#   }
# }

# üöÄ TERRAGRUNT INFRASTRUCTURE UNIT CONFIGURATION
# Defines the input parameters for the infrastructure unit
# Combines global configuration, metadata, and tag management
inputs = {
  min_age = 18
  max_age = 65
  tags    = local.all_tags
}
</file>

<file path="infra/terragrunt/_shared/_units/name_generator.hcl">
// üß© Shared Unit Configuration for Name Generator Module

locals {
  # ---------------------------------------------------------------------------------------------------------------------
  # üèóÔ∏è ENVIRONMENT CONFIGURATION ORCHESTRATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Dynamically load and aggregate environment-specific configuration files
  # This mechanism enables a flexible, layered infrastructure configuration approach
  #
  # Configuration Layers:
  # - Environment-level settings (env.hcl)
  # - Global architecture configurations
  # - Hierarchical tag management
  #
  # Key Benefits:
  # - Modular configuration management
  # - Centralized environment settings
  # - Flexible tag inheritance
  # ---------------------------------------------------------------------------------------------------------------------
  env_cfg = read_terragrunt_config(find_in_parent_folders("env.hcl"))

  # üåê Stack Configuration
  # Loads the stack configuration file that serves as the single source of truth
  # for stack-level settings and metadata
  stack_cfg = read_terragrunt_config(find_in_parent_folders("stack.hcl"))

  # ---------------------------------------------------------------------------------------------------------------------
  # üåê GLOBAL ARCHITECTURE CONFIGURATION RESOLVER
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Centralize and standardize infrastructure-wide configuration
  # Loads the root configuration file that serves as the single source of truth
  # for infrastructure-level settings and metadata
  #
  # Key Responsibilities:
  # - Provide global configuration context
  # - Enable consistent infrastructure metadata
  # - Support cross-module configuration sharing
  # ---------------------------------------------------------------------------------------------------------------------
  cfg = read_terragrunt_config("${find_in_parent_folders("config.hcl")}")

  # ---------------------------------------------------------------------------------------------------------------------
  # üè∑Ô∏è INTELLIGENT TAG ORCHESTRATION SYSTEM
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Create a sophisticated, hierarchical tag management mechanism
  # Implements a multi-layered tagging strategy that allows for:
  # - Global tag inheritance
  # - Environment-specific tag augmentation
  # - Unit-level tag customization
  #
  # Tag Hierarchy (from broadest to most specific):
  # 1. Global Architecture Tags üåê
  # 2. Environment-Level Tags üåç
  # 3. Unit-Specific Tags üß©
  # 4. Stack-Level Tags üìö
  #
  # Benefits:
  # - Consistent resource identification
  # - Flexible tag management
  # - Enhanced resource tracking and compliance
  # ---------------------------------------------------------------------------------------------------------------------
  unit_tags = {
    Unit = "name-generator"
    Type = "random-generator"
  }

  # üîó TAG SOURCE AGGREGATION
  # Collect tags from different configuration levels
  env_tags    = local.env_cfg.locals.tags
  global_tags = local.cfg.locals.tags
  stack_tags  = local.stack_cfg.locals.tags

  # üß© FINAL TAG COMPOSITION
  # Merge tags with a clear precedence strategy
  # Precedence: Unit Tags > Environment Tags > Global Tags
  all_tags = merge(
    local.env_tags,
    local.unit_tags,
    local.global_tags,
    local.stack_tags
  )

  # ---------------------------------------------------------------------------------------------------------------------
  # üîß GIT MODULE SOURCE CONFIGURATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Intelligent Terraform module source management with:
  # - Centralized default configuration
  # - Flexible repository and path selection
  # - Semantic version control
  #
  git_base_url              = local.cfg.locals.cfg_git.git_base_urls.github
  tf_module_repository      = "your-org/terraform-modules.git"
  tf_module_version_default = get_env("TG_STACK_TF_MODULE_NAME_GENERATOR_VERSION_DEFAULT", "v0.1.0")
  tf_module_path_default    = "modules/name-generator"

  tf_module_source = format(
    "%s%s//%s",
    local.git_base_url,
    local.tf_module_repository,
    local.tf_module_path_default
  )

  echo_tf_module_source = run_cmd("sh", "-c", "echo 'üîß  TF Module Source (parent): ${local.tf_module_source} version: ${local.tf_module_version_default}'")

  # ---------------------------------------------------------------------------------------------------------------------
  # üåê SPECIFIC MODULE CONFIGURATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Here we define the specific configuration for the module.
  # This is useful when we need to override the default configuration for the module.
  #
  # TODO: Add specific locals, or configuration for your module to be computed here.
}

# üîó DEPENDENCY
# This block defines a dependency for the Terragrunt configuration.
# Dependencies allow for the management of resources that rely on
# other configurations, ensuring that they are created or updated
# in the correct order. This promotes modularity and reusability
# of infrastructure components, making it easier to manage complex
# setups. Dependencies can also include mock outputs for testing
# purposes without needing to provision the actual resources.
# dependency "cloudflare_dns_zone" {
#   config_path = find_in_parent_folders("<stack>/<unit>")
#   mock_outputs = {
#     cloudflare_zone_ids = {
#       "fake-zone-id" = "fake-zone-id"
#     }
#   }
# }

# üöÄ TERRAGRUNT INFRASTRUCTURE UNIT CONFIGURATION
# Defines the input parameters for the infrastructure unit
# Combines global configuration, metadata, and tag management
inputs = {
  input_name    = "john"
  suffix_length = 6
  gender        = "any"
  tags          = local.all_tags
}
</file>

<file path="infra/terragrunt/_templates/README.md">
# Infrastructure Templates üß©

## Overview

This directory contains template files used across the infrastructure deployment pipeline, providing standardized configurations and version management.

## Current Templates

### `.terraform-version.tpl`

A version constraint template for Terraform, ensuring consistent tooling across different environments and team members.

#### Usage Example

```hcl
# Terraform version constraint
terraform {
  required_version = file("${path_relative_to_include()}/../_templates/.terraform-version.tpl")
}
```

## Purpose

- **Standardization**: Maintain consistent configuration across projects
- **Version Control**: Centralize version management for critical tools
- **Flexibility**: Easy to update and propagate changes

This template can be used in the `generate` block of a Terragrunt configuration file to ensure that the correct version of Terraform is used. See [root.hcl](../root.hcl) for an example.

# Environment Configuration Templates

This directory contains templates for environment configuration files that can be used across the project.

## Layer-Agnostic Environment Configuration

The environment configuration system is designed to be:

1. **Reusable** - Common functions are extracted to shared utility scripts
2. **Flexible** - Not tied to specific environment naming conventions
3. **Hierarchical** - Each layer inherits from its parent
4. **Self-documenting** - Includes clear documentation and logging

## Available Templates

- `layer.envrc.template` - Template for layer-specific `.envrc` files

## How to Use

1. Copy the template to your new layer directory:
   ```bash
   cp _templates/layer.envrc.template your/new/layer/.envrc
   ```

2. Customize the layer name and variables:
   ```bash
   # Change this to match your layer
   LAYER_NAME="YOUR_LAYER_NAME"
   ```

3. Add your layer-specific variables using the `_layer_export` function:
   ```bash
   _layer_export YOUR_VAR "your-value" "$LAYER_NAME"
   ```

4. Update the layer information display:
   ```bash
   _layer_env_info "$LAYER_NAME" \
     "YOUR_VAR:Description of YOUR_VAR" \
     "ANOTHER_VAR:Description of ANOTHER_VAR"
   ```

## Inheritance

Each `.envrc` file automatically inherits from its parent directory's `.envrc` file through the `source_up` directive. This creates a hierarchical configuration where:

1. Root `.envrc` sets global defaults
2. Each layer can override or extend these defaults
3. Nested layers inherit from their parent layers

## Utility Functions

The shared utility script (`scripts/envrc-utils.sh`) provides several helpful functions:

- `_layer_export` - Export a variable with layer-specific logging
- `_layer_log` - Log a message with layer-specific prefix
- `_layer_env_info` - Display layer environment information
- `_validate_layer_config` - Validate required variables for a layer

## Example Layer Structure

```
project/
‚îú‚îÄ‚îÄ .envrc                  # Root environment variables
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ envrc-utils.sh      # Shared utility functions
‚îî‚îÄ‚îÄ infra/
    ‚îî‚îÄ‚îÄ terragrunt/
        ‚îú‚îÄ‚îÄ .envrc          # Terragrunt-specific variables
        ‚îú‚îÄ‚îÄ dev/
        ‚îÇ   ‚îî‚îÄ‚îÄ .envrc      # Development layer variables
        ‚îú‚îÄ‚îÄ staging/
        ‚îÇ   ‚îî‚îÄ‚îÄ .envrc      # Staging layer variables
        ‚îî‚îÄ‚îÄ prod/
            ‚îî‚îÄ‚îÄ .envrc      # Production layer variables
```

Each layer can be named according to your specific needs - the system is not tied to specific environment names.
</file>

<file path="infra/terragrunt/global/dni/age_generator/.terraform-version">
1.9.8
</file>

<file path="infra/terragrunt/global/dni/age_generator/terragrunt.hcl">
# üåê Root Terragrunt Configuration Inclusion
# This block imports the common configuration from the parent directory's terragrunt.hcl file.
# It enables consistent configuration sharing across multiple Terragrunt modules, ensuring that
# all modules can access shared settings and parameters defined at the root level.
include "root" {
  path           = find_in_parent_folders("root.hcl")
  merge_strategy = "deep"
}

# üß© Shared Units Configuration
# This block imports standardized component configuration from the shared components directory.
# It is important to note that any modifications should be made in the shared component configuration file
# located at: `_shared/_components/quota-generator.hcl`. This ensures that changes are reflected
# across all modules that utilize this shared configuration.
include "shared" {
  path           = "${get_terragrunt_dir()}/../../../_shared/_units/age_generator.hcl"
  expose         = true
  merge_strategy = "deep"
}

locals {
  # üîß Terraform Module Source Resolution
  # ---------------------------------------------------------------------------------------------------------------------
  # Intelligent Terraform module source management with:
  # - Local path testing support
  # - Version override capability
  # - Fallback to default version
  # - Dynamic source path computation
  #
  # Resolution Strategy:
  # 1. If local_path is provided, use it for local testing
  # 2. If version_override is set, use that version
  # 3. Otherwise, fall back to the default version from shared configuration
  #
  # Enables flexible module sourcing for:
  # - Local development and testing
  # - Precise version control
  tf_module_local_path       = "${include.shared.locals.cfg_git.git_base_urls.local}/age-generator"
  tf_module_version_override = ""
  tf_module_version          = local.tf_module_version_override != "" ? local.tf_module_version_override : include.shared.locals.tf_module_version_default
  tf_module_source           = include.shared.locals.tf_module_source

  tg_source_computed    = local.tf_module_local_path != "" ? local.tf_module_local_path : format("%s?ref=%s", local.tf_module_source, local.tf_module_version)
  echo_tf_module_source = run_cmd("sh", "-c", "echo 'üîß  TF Module Source (child): ${local.tg_source_computed}'")
}

# üöÄ Terraform Source Configuration
# Dynamically resolves the Terraform module source based on:
# - Local testing path (if provided)
# - Shared module source
# - Specific version reference
terraform {
  source = local.tg_source_computed
}

# üì¶ Inputs Configuration
# Provides an empty inputs block to satisfy Terragrunt configuration requirements
# Actual inputs are managed in the shared configuration
inputs = {}
</file>

<file path="infra/terragrunt/global/dni/dni_generator/.terraform-version">
1.9.8
</file>

<file path="infra/terragrunt/global/dni/dni_generator/terragrunt.hcl">
# üåê Root Terragrunt Configuration Inclusion
# This block imports the common configuration from the parent directory's terragrunt.hcl file.
# It enables consistent configuration sharing across multiple Terragrunt modules, ensuring that
# all modules can access shared settings and parameters defined at the root level.
include "root" {
  path           = find_in_parent_folders("root.hcl")
  merge_strategy = "deep"
}

# üß© Shared Units Configuration
# This block imports standardized component configuration from the shared components directory.
# It is important to note that any modifications should be made in the shared component configuration file
# located at: `_shared/_components/quota-generator.hcl`. This ensures that changes are reflected
# across all modules that utilize this shared configuration.
include "shared" {
  path           = "${get_terragrunt_dir()}/../../../_shared/_units/dni_generator.hcl"
  expose         = true
  merge_strategy = "deep"
}

locals {
  # üîß Terraform Module Source Resolution
  # ---------------------------------------------------------------------------------------------------------------------
  # Intelligent Terraform module source management with:
  # - Local path testing support
  # - Version override capability
  # - Fallback to default version
  # - Dynamic source path computation
  #
  # Resolution Strategy:
  # 1. If local_path is provided, use it for local testing
  # 2. If version_override is set, use that version
  # 3. Otherwise, fall back to the default version from shared configuration
  #
  # Enables flexible module sourcing for:
  # - Local development and testing
  # - Precise version control
  # - Consistent module referencing across infrastructure
  # ---------------------------------------------------------------------------------------------------------------------
  tf_module_local_path       = "${include.shared.locals.cfg_git.git_base_urls.local}/dni-generator"
  tf_module_version_override = ""
  tf_module_version          = local.tf_module_version_override != "" ? local.tf_module_version_override : include.shared.locals.tf_module_version_default
  tf_module_source           = include.shared.locals.tf_module_source

  tg_source_computed    = local.tf_module_local_path != "" ? local.tf_module_local_path : format("%s?ref=%s", local.tf_module_source, local.tf_module_version)
  echo_tf_module_source = run_cmd("sh", "-c", "echo 'üîß  TF Module Source (child): ${local.tg_source_computed}'")
}

# üöÄ Terraform Source Configuration
# Dynamically resolves the Terraform module source based on:
# - Local testing path (if provided)
# - Shared module source
# - Specific version reference
terraform {
  source = local.tg_source_computed
}

# üì¶ Inputs Configuration
# Provides an empty inputs block to satisfy Terragrunt configuration requirements
# Actual inputs are managed in the shared configuration
inputs = {}
</file>

<file path="infra/terragrunt/global/dni/lastname_generator/.terraform-version">
1.9.8
</file>

<file path="infra/terragrunt/global/dni/lastname_generator/terragrunt.hcl">
# üåê Root Terragrunt Configuration Inclusion
# This block imports the common configuration from the parent directory's terragrunt.hcl file.
# It enables consistent configuration sharing across multiple Terragrunt modules, ensuring that
# all modules can access shared settings and parameters defined at the root level.
include "root" {
  path           = find_in_parent_folders("root.hcl")
  merge_strategy = "deep"
}

# üß© Shared Units Configuration
# This block imports standardized component configuration from the shared components directory.
# It is important to note that any modifications should be made in the shared component configuration file
# located at: `_shared/_components/quota-generator.hcl`. This ensures that changes are reflected
# across all modules that utilize this shared configuration.
include "shared" {
  path           = "${get_terragrunt_dir()}/../../../_shared/_units/lastname_generator.hcl"
  expose         = true
  merge_strategy = "deep"
}

locals {
  # üîß Terraform Module Source Resolution
  # ---------------------------------------------------------------------------------------------------------------------
  # Intelligent Terraform module source management with:
  # - Local path testing support
  # - Version override capability
  # - Fallback to default version
  # - Dynamic source path computation
  #
  # Resolution Strategy:
  # 1. If local_path is provided, use it for local testing
  # 2. If version_override is set, use that version
  # 3. Otherwise, fall back to the default version from shared configuration
  #
  # Enables flexible module sourcing for:
  # - Local development and testing
  # - Precise version control
  # - Consistent module referencing across infrastructure
  # ---------------------------------------------------------------------------------------------------------------------
  tf_module_local_path       = "${include.shared.locals.cfg_git.git_base_urls.local}/lastname-generator"
  tf_module_version_override = ""
  tf_module_version          = local.tf_module_version_override != "" ? local.tf_module_version_override : include.shared.locals.tf_module_version_default
  tf_module_source           = include.shared.locals.tf_module_source

  tg_source_computed    = local.tf_module_local_path != "" ? local.tf_module_local_path : format("%s?ref=%s", local.tf_module_source, local.tf_module_version)
  echo_tf_module_source = run_cmd("sh", "-c", "echo 'üîß  TF Module Source (child): ${local.tg_source_computed}'")
}

# üöÄ Terraform Source Configuration
# Dynamically resolves the Terraform module source based on:
# - Local testing path (if provided)
# - Shared module source
# - Specific version reference
terraform {
  source = local.tg_source_computed
}

# üì¶ Inputs Configuration
# Provides an empty inputs block to satisfy Terragrunt configuration requirements
# Actual inputs are managed in the shared configuration
inputs = {}
</file>

<file path="infra/terragrunt/global/dni/name_generator/.terraform-version">
1.9.8
</file>

<file path="infra/terragrunt/global/dni/stack.hcl">
// üåê Stack Configuration Manifest
//
// This file defines stack-specific configurations and metadata for the DNS infrastructure.
// It serves as a central point for stack-level settings that can be referenced across
// different Terragrunt and Terraform modules within the DNS domain.
//
// üîç Purpose:
// - Define stack-specific variables and metadata
// - Provide consistent tagging strategy for DNS-related resources
// - Enable stack-level customizations and grouping
//
// üí° Configuration Guidelines:
// - Modify values to match specific DNS infrastructure requirements
// - Ensure consistency across different DNS-related components
// - Use meaningful, descriptive names and tags

locals {
  // üè∑Ô∏è Stack Naming Convention
  // - Use descriptive, lowercase names that represent the stack's primary function
  // - Recommended format: [domain-type-purpose]
  //
  // üí° Tip: Stack name should clearly indicate its infrastructure domain and purpose
  stack_name = "dni"

  // üåê Stack Description
  // Provides a human-readable description of the stack's purpose and scope
  stack_description = "DNI generation"

  // üìõ Stack-Level Tagging Strategy
  //
  // Tags provide crucial metadata for:
  // - Resource identification
  // - Logical grouping
  // - Infrastructure organization
  // - Compliance and management tracking
  //
  // üîç Best Practices:
  // - Extend environment-level tags with stack-specific metadata
  // - Use clear, descriptive tag values
  // - Consider additional context-specific tags
  tags = {
    // Stack-specific tags
    Stack   = "dni"
    Domain  = "myapp.com"
    Purpose = "DNI generation"
  }

  // üîß Stack Configuration Flags
  // Enable or disable specific stack-wide configurations
  stack_config = {
  }
}
</file>

<file path="infra/terragrunt/root.hcl">
locals {
  // üîß Configuration File Loading
  // Centralizes infrastructure configuration by reading global and environment-specific config files.
  // This mechanism allows for modular and flexible infrastructure management across different environments.
  // When adding new configuration sources, ensure they follow the established path and naming conventions.
  # cfg = read_terragrunt_config("${get_parent_terragrunt_dir()}/config.hcl")
  cfg       = read_terragrunt_config("${find_in_parent_folders("config.hcl")}")
  env_cfg   = read_terragrunt_config("${get_terragrunt_dir()}/../../env.hcl")
  stack_cfg = read_terragrunt_config("${get_terragrunt_dir()}/../stack.hcl")

  // üåç Deployment Context Extraction
  // Captures critical runtime environment details like deployment region and environment name.
  // These variables enable dynamic configuration and support multi-environment infrastructure strategies.
  // Modify environment-specific configurations in the respective env.hcl files.
  deployment_region        = local.cfg.locals.deployment_region
  deployment_environment   = local.env_cfg.locals.environment_name
  deployment_stack         = local.stack_cfg.locals.stack_name
  path_relative_to_include = path_relative_to_include()

  // üéØ Current Infrastructure Unit Identification
  // Dynamically determines the specific infrastructure unit being processed using Terragrunt's path resolution.
  // This approach allows for unit-specific configurations and operations without hardcoding unit names.
  // Ensure your directory structure maintains the expected hierarchy for accurate unit identification.
  current_unit = basename(local.path_relative_to_include)

  // üóÇÔ∏è Deployment Group and State Management
  // Generates a consistent, predictable state folder structure that supports complex infrastructure hierarchies.
  // Creates unique identifiers by combining product name and deployment group with a flat naming convention.
  // When restructuring infrastructure, maintain the logical relationship between product name and deployment paths.
  deployment_group = replace(local.path_relative_to_include, "/", "-")
  terraform_state_folder_flat = replace(
    format("%s-%s",
      local.cfg.locals.product_name,
      local.deployment_group
    ),
    "/",
    "-"
  )

  // üîë Remote State Key Path Generation
  // Constructs a standardized, hierarchical key for Terraform remote state that reflects the infrastructure's logical structure.
  // Format: <product_name>/<environment>/<unit-path>-<state_object_basename>
  // Example: myapp/global/dns-dns_zone-terraform.tfstate.json
  remote_state_key_path = join("/", [
    local.cfg.locals.product_name,
    local.deployment_environment,
    format("%s-%s",
      replace(trimprefix(local.path_relative_to_include, "${local.deployment_environment}/"), "/", "-"),
      local.cfg.locals.state_object_basename
    )
  ])

  // üîå Dynamic Provider Loading
  // Enables flexible, unit-specific provider configuration by dynamically loading provider settings.
  // Supports multi-provider setups and allows granular control over provider registration for different infrastructure units.
  // To add a new provider, update the shared provider configuration files and register the unit appropriately.
  dynamic_providers = try(
    local.cfg.locals.unit_cfg_providers.locals.providers != null
    ? local.cfg.locals.unit_cfg_providers.locals.providers
    : [],
    []
  )

  // üì¶ Dynamic Versions Configuration
  // Manages Terraform provider version constraints dynamically, ensuring compatibility across different infrastructure units.
  // Loads version configurations specific to registered units, preventing version conflicts.
  // When introducing new providers or updating versions, modify the corresponding provider configuration files.
  dynamic_versions = try(
    local.cfg.locals.unit_cfg_versions.locals.versions != null
    ? local.cfg.locals.unit_cfg_versions.locals.versions
    : [],
    []
  )

  // üõ°Ô∏è Fallback Provider Configuration
  // Ensures every infrastructure unit has a valid provider configuration by supplying a default 'null' provider.
  // Prevents configuration errors when no unit-specific providers are defined.
  // Serves as a safety mechanism to maintain infrastructure deployment capabilities.
  fallback_providers = [
    <<-EOF
# Default provider for units without specific provider configuration
provider "null" {}
    EOF
  ]

  // üè∑Ô∏è Fallback Versions Configuration
  // Provides a minimal Terraform versions configuration as a last resort for infrastructure units.
  // Prevents initialization errors by guaranteeing a valid versions block.
  // Can be extended to include more default provider version constraints if needed.
  fallback_versions = [
    <<-EOF
# Default versions block for units without specific version configuration
terraform {
  required_providers {
    null = {
      source = "hashicorp/null"
      version = "~> 3.2"
    }
  }
}
    EOF
  ]

  // üî¨ Provider and Versions Resolution
  // Implements an intelligent selection mechanism for provider and version configurations.
  // Prioritizes unit-specific dynamic configurations while maintaining a robust fallback strategy.
  // Ensures flexibility and reliability in infrastructure provisioning.
  final_providers = length(local.dynamic_providers) > 0 ? local.dynamic_providers : local.fallback_providers

  final_versions = length(local.dynamic_versions) > 0 ? local.dynamic_versions : local.fallback_versions

  // üì¢ Deployment Information Logging
  // Provides visibility into the current Terragrunt execution context by outputting critical deployment details.
  // Helps in debugging and tracking infrastructure changes during execution.
  // Can be extended to include additional diagnostic information if needed.
  echo_line_separator                      = run_cmd("sh", "-c", "echo '================================================================================'")
  echo_current_unit                        = run_cmd("sh", "-c", "echo 'üèóÔ∏è  Current Unit: ${local.current_unit}'")
  echo_product_name                        = run_cmd("sh", "-c", "echo 'üì¶  Product Name: ${local.cfg.locals.product_name}'")
  echo_product_version                     = run_cmd("sh", "-c", "echo 'üîß  Product Version: ${local.cfg.locals.product_version}'")
  echo_deployment_group                    = run_cmd("sh", "-c", "echo 'üì¶  Deployment Group: ${local.deployment_group}'")
  echo_deployment_region                   = run_cmd("sh", "-c", "echo 'üåç  Deployment Region: ${local.deployment_region}'")
  echo_deployment_stack                    = run_cmd("sh", "-c", "echo 'üèûÔ∏è  Deployment Stack: ${local.deployment_stack}'")
  echo_environment_name                    = run_cmd("sh", "-c", "echo 'üèûÔ∏è  Environment Name: ${local.deployment_environment}'")
  echo_remote_state_key_path               = run_cmd("sh", "-c", "echo 'üîë  Remote State Key Path: ${local.remote_state_key_path}'")
  echo_is_using_providers_then_true        = run_cmd("sh", "-c", "echo 'üîå  Is Using Providers: ${length(local.dynamic_providers) > 0 ? "true" : "false"}'")
  echo_is_overwriting_versionstf_then_true = run_cmd("sh", "-c", "echo 'üîß  Is Overwriting Versions: ${length(local.dynamic_versions) > 0 ? "true" : "false"}'")

  // üì¢ State Information
  echo_state_bucket_name = run_cmd("sh", "-c", "echo 'üóÑÔ∏è  State Bucket Name: ${local.cfg.locals.bucket_name}'")
  echo_state_lock_table  = run_cmd("sh", "-c", "echo 'üîí  State Lock Table: ${local.cfg.locals.lock_table}'")
}

terraform {
  // üîß Flexible Variable File Handling
  // Terragrunt makes it easy to manage different configuration files across environments.
  // We can seamlessly include global, environment-specific, and region-specific variable files.
  // This approach lets you customize your infrastructure settings without duplicating code.
  extra_arguments "optional_vars" {
    commands = [
      "apply",
      "destroy",
      "plan",
    ]

    optional_var_files = [
      // üìÅ Global Default Variables
      // Start with a base configuration that applies everywhere
      "default.tfvars",

      // üåç Environment-Specific Tweaks
      // Override or extend global settings for specific environments
      "${local.deployment_environment}/default.tfvars",

      // üó∫Ô∏è Region-Specific Customizations
      // Fine-tune configurations for different regions
      "${local.deployment_region}.tfvars",
    ]
  }
}

// üíæ Smart Remote State Management
// Keep your Terraform state secure, organized, and easily accessible.
// We use S3 as a centralized backend with built-in encryption and tagging.
remote_state {
  backend = "s3"
  generate = {
    path      = local.cfg.locals.backend_tf_filename
    if_exists = "overwrite"
  }

  config = {
    disable_bucket_update = true
    encrypt               = true

    region         = local.cfg.locals.region
    bucket         = local.cfg.locals.bucket_name
    dynamodb_table = local.cfg.locals.lock_table

    key = local.remote_state_key_path

    s3_bucket_tags      = local.cfg.locals.tags
    dynamodb_table_tags = local.cfg.locals.tags
  }
}

// üöÄ Dynamic Provider Configuration
// This block generates provider settings dynamically for each infrastructure unit.
// By doing so, it allows for a more flexible and manageable configuration of providers,
// adapting to the specific needs of each unit without hardcoding values.
// The 'path' specifies where the generated provider configuration will be saved.
// The 'if_exists' condition checks if the generation of the provider file is enabled;
// if so, it will overwrite any existing file, otherwise it will skip the generation.
// The 'contents' field constructs the provider configuration by joining the list of
// shared provider configurations, ensuring that only relevant settings are included.
generate "providers" {
  path      = "providers.tf"
  if_exists = local.cfg.locals.generate_providers_file ? "overwrite" : "skip"
  contents  = local.cfg.locals.generate_providers_file ? join("\n", local.cfg.locals.shared_config_providers.providers) : ""
}

// üèóÔ∏è Intelligent Version Management
// Dynamically handle provider versions across different infrastructure units.
// Ensures compatibility and makes version updates a breeze.
generate "versions" {
  path      = "versions.tf"
  if_exists = local.cfg.locals.generate_versions_file ? "overwrite" : "skip"
  contents  = local.cfg.locals.generate_versions_file ? join("\n", local.cfg.locals.unit_cfg_versions.locals.versions) : ""
}

// üîß Terraform Version Enforcement
// Automatically create a .terraform-version file to keep everyone on the same page.
// Control which Terraform version is used across your infrastructure.
generate "terraform_version" {
  path      = ".terraform-version"
  if_exists = "overwrite_terragrunt"
  contents  = local.cfg.locals.enable_terraform_version_file_override == "true" ? local.cfg.locals.tf_version_enforced : local.cfg.locals.terraform_version_disabled_comments
}
</file>

<file path="scripts/envrc-utils.sh">
#!/usr/bin/env bash
# =============================================================================
# Terragrunt Reference Architecture - Environment Utilities
# =============================================================================
# Common shell functions for .envrc files across the project
# This file contains reusable functions that can be sourced by any .envrc file

# =============================================================================
# CORE LOGGING FUNCTIONS
# =============================================================================

# Simple logging function with timestamp and log level
# Usage: _log "INFO" "Your message here"
_log() {
  local log_level="${1:-INFO}"
  local message="${2}"
  local timestamp
  timestamp="$(date +"%Y-%m-%d %H:%M:%S")"
  echo "[${log_level}] ${timestamp} - ${message}" >&2
}

# Layer-specific logging with prefix
# Usage: _layer_log "INFO" "Your message" "LAYER_NAME"
_layer_log() {
  local log_level="${1:-INFO}"
  local message="${2}"
  local prefix="${3:-}"

  if [[ -n "${prefix}" ]]; then
    _log "${log_level}" "[${prefix}] ${message}"
  else
    _log "${log_level}" "${message}"
  fi
}

# =============================================================================
# ENVIRONMENT VARIABLE MANAGEMENT
# =============================================================================

# Secure environment variable export with validation
# Usage: _safe_export "VARIABLE_NAME" "variable_value"
_safe_export() {
  local var_name="${1}"
  local var_value="${2}"
  local caller_info="${3:-unknown}"

  # Validate variable name
  if [[ ! "${var_name}" =~ ^[A-Z][A-Z0-9_]*$ ]]; then
    _log "ERROR" "Invalid environment variable name: ${var_name}. Must be uppercase and start with a letter."
    return 1
  fi

  # Check for empty values
  if [[ -z "${var_value// }" ]]; then
    _log "WARN" "Attempted to export empty or whitespace-only value for '${var_name}'"
    return 1
  fi

  # Sanitize and export
  var_value="$(echo "${var_value}" | xargs)"
  export "${var_name}"="${var_value}"
  _log "TRACK" "${var_name} = [REDACTED] (set from ${caller_info})"
}

# Layer-specific variable export with custom logging
# Usage: _layer_export "VARIABLE_NAME" "variable_value" "LAYER_NAME"
_layer_export() {
  local var_name="${1}"
  local var_value="${2}"
  local layer_name="${3:-}"

  # Use the existing _safe_export with caller information
  _safe_export "${var_name}" "${var_value}" "$(caller | awk '{print $2}')"

  if [[ -n "${layer_name}" ]]; then
    _layer_log "TRACK" "${layer_name} config: ${var_name} = [REDACTED]" "${layer_name}"
  fi
}

# =============================================================================
# PROJECT STRUCTURE DETECTION
# =============================================================================

# Project root detection
# Sets PROJECT_ROOT environment variable
_detect_project_root() {
  local current_dir="${PWD}"
  local root_markers=(".git" "justfile")

  while [ "${current_dir}" != "/" ]; do
    for marker in "${root_markers[@]}"; do
      if [ -e "${current_dir}/${marker}" ]; then
        _safe_export PROJECT_ROOT "${current_dir}"
        _log "INFO" "Project root detected: ${current_dir}"
        return 0
      fi
    done
    current_dir="$(dirname "${current_dir}")"
  done

  _safe_export PROJECT_ROOT "${PWD}"
  _log "WARN" "No specific project root marker found. Using current directory."
}

# Terragrunt root detection
# Sets TERRAGRUNT_ROOT environment variable
_detect_terragrunt_root() {
  local current_dir="${PWD}"
  local terragrunt_markers=(
    "terragrunt.hcl"
    "root.hcl"
    "config.hcl"
  )

  while [ "${current_dir}" != "/" ]; do
    for marker in "${terragrunt_markers[@]}"; do
      if [ -e "${current_dir}/${marker}" ]; then
        _layer_export TERRAGRUNT_ROOT "${current_dir}" "TERRAGRUNT"
        _layer_log "INFO" "Terragrunt root detected: ${current_dir}" "TERRAGRUNT"
        return 0
      fi
    done
    current_dir="$(dirname "${current_dir}")"
  done

  _layer_export TERRAGRUNT_ROOT "${PWD}" "TERRAGRUNT"
  _layer_log "WARN" "No specific Terragrunt root marker found. Using current directory." "TERRAGRUNT"
}

# =============================================================================
# LAYER CONFIGURATION FUNCTIONS
# =============================================================================

# Validate configuration for a specific layer
# Usage: _validate_layer_config "LAYER_NAME" "VAR1" "VAR2" ...
_validate_layer_config() {
  local layer_name="${1}"
  shift
  local required_vars=("$@")
  local missing_vars=()

  for var in "${required_vars[@]}"; do
    # Use indirect expansion safely
    if [[ -z "${!var:-}" ]]; then
      missing_vars+=("${var}")
    fi
  done

  if [[ ${#missing_vars[@]} -gt 0 ]]; then
    _layer_log "ERROR" "Missing critical configurations:" "${layer_name}"
    printf '%s\n' "${missing_vars[@]}"
    return 1
  fi

  _layer_log "INFO" "Configuration validated successfully" "${layer_name}"
}

# Display layer environment information
# Usage: _layer_env_info "LAYER_NAME" "VAR1:Description" "VAR2:Description" ...
_layer_env_info() {
  local layer_name="${1}"
  shift
  local var_descriptions=("$@")

  _layer_log "INFO" "üîπ ${layer_name} Layer Environment Initialized" "${layer_name}"

  for var_desc in "${var_descriptions[@]}"; do
    IFS=':' read -r var_name var_description <<< "${var_desc}"
    # Use indirect expansion safely
    if [[ -n "${!var_name:-}" ]]; then
      local display_value="${!var_name}"
      # Mask sensitive values if needed
      if [[ "${var_name}" == *"SECRET"* || "${var_name}" == *"PASSWORD"* || "${var_name}" == *"KEY"* ]]; then
        display_value="[REDACTED]"
      fi
      _layer_log "INFO" "${var_description:-${var_name}}: ${display_value}" "${layer_name}"
    fi
  done
}

# =============================================================================
# HELPER FUNCTIONS FOR ENVIRONMENT INITIALIZATION
# =============================================================================

# Initialize core environment
_core_init() {
  # Detect project root
  _detect_project_root

  # Set up basic environment variables
  _log "INFO" "Initializing core environment"
}

# Initialize Terragrunt environment
_terragrunt_init() {
  # Detect Terragrunt root
  _detect_terragrunt_root

  _layer_log "INFO" "Initializing Terragrunt environment" "TERRAGRUNT"
}

# =============================================================================
# USAGE EXAMPLES
# =============================================================================
#
# In root .envrc:
#   source "${PWD}/scripts/envrc-utils.sh"
#   _core_init
#   _safe_export VARIABLE_NAME "value"
#
# In any layer .envrc:
#   source_up || true
#   source "${PROJECT_ROOT}/scripts/envrc-utils.sh"
#
#   # Define layer name (can be any name that makes sense for your structure)
#   LAYER_NAME="MY_LAYER"
#
#   # Initialize layer variables
#   _layer_export VAR_NAME "value" "$LAYER_NAME"
#
#   # Display layer information
#   _layer_env_info "$LAYER_NAME" \
#     "VAR_NAME:Description of VAR_NAME" \
#     "ANOTHER_VAR:Description of ANOTHER_VAR"

# Display all exported variables with optional filtering
# Usage: _display_exported_vars [FILTER_PREFIX]
_display_exported_vars() {
  local filter_prefix="${1:-}"
  local exported_vars=()

  # Collect exported variables
  while IFS='=' read -r var value; do
    # Only include variables that match the optional prefix
    if [[ -z "${filter_prefix}" ]] || [[ "${var}" == "${filter_prefix}"* ]]; then
      # Mask sensitive values if needed
      if [[ "${var}" == *"SECRET"* || "${var}" == *"PASSWORD"* || "${var}" == *"KEY"* || "${var}" == *"TOKEN"* ]]; then
        value="[REDACTED]"
      fi
      exported_vars+=("${var}: ${value}")
    fi
  done < <(env | grep -E '^[A-Z_]+=' | sort)

  # Display header and variables
  if [[ ${#exported_vars[@]} -gt 0 ]]; then
    _log "INFO" "Exported Environment Variables:"
    printf '%s\n' "${exported_vars[@]}" >&2
  else
    _log "WARN" "No exported variables found$([ -n "${filter_prefix}" ] && echo " with prefix '${filter_prefix}'")"
  fi
}

# Shellcheck directives for sourcing and shell compatibility
# shellcheck shell=bash
# shellcheck disable=SC2155
true
</file>

<file path="scripts/justfile-utils.sh">
#!/usr/bin/env bash
# =============================================================================
# Terragrunt Reference Architecture - Justfile Utilities
# =============================================================================
# Utility functions for use with Justfile recipes
# This file contains functions that can be called from Justfile to keep
# recipes clean and maintainable

# =============================================================================
# CORE LOGGING FUNCTIONS
# =============================================================================

# Simple logging function with timestamp and level
# Usage: log_message "INFO" "Your message here"
log_message() {
  local log_level="${1:-INFO}"
  local message="$2"
  local timestamp
  timestamp=$(date +"%Y-%m-%d %H:%M:%S")
  echo "[${log_level}] ${timestamp} - ${message}" >&2
}

# =============================================================================
# TERRAGRUNT UTILITIES
# =============================================================================

# Format Terragrunt HCL files
# Usage: terragrunt_format "/path/to/terragrunt/dir" "check" "diff" "exclude_pattern"
terragrunt_format() {
  local terragrunt_dir="$1"
  local check_mode="$2"
  local diff_mode="$3"
  local exclude_pattern="$4"

  log_message "INFO" "üîç Advanced Terragrunt HCL Formatting"

  # Validate inputs
  if [[ ! -d "${terragrunt_dir}" ]]; then
    log_message "ERROR" "Terragrunt directory does not exist: ${terragrunt_dir}"
    return 1
  fi

  # Set up the command base
  local cmd="terragrunt hclfmt"

  # Add options based on parameters
  if [[ "${check_mode}" == "true" ]]; then
    cmd="${cmd} --check"
    log_message "INFO" "‚ÑπÔ∏è Running in check-only mode (no changes will be made)"
  fi

  if [[ "${diff_mode}" == "true" ]]; then
    cmd="${cmd} --diff"
    log_message "INFO" "‚ÑπÔ∏è Showing diffs between original and formatted files"
  fi

  # Change to the terragrunt directory
  cd "${terragrunt_dir}" || {
    log_message "ERROR" "Failed to change to directory: ${terragrunt_dir}"
    return 1
  }

  # Find all HCL files
  local all_hcl_files
  all_hcl_files=$(find . -name "*.hcl" || true)

  # Always exclude .terragrunt-cache
  if [[ -z "${exclude_pattern}" ]]; then
    exclude_pattern=".terragrunt-cache"
  elif [[ ! "${exclude_pattern}" =~ (^|,)\.terragrunt-cache(,|$) ]]; then
    exclude_pattern="${exclude_pattern},.terragrunt-cache"
  fi

  # Apply exclude patterns if specified
  local hcl_files="${all_hcl_files}"
  if [[ -n "${exclude_pattern}" ]]; then
    log_message "INFO" "‚ÑπÔ∏è Excluding patterns: ${exclude_pattern}"
    local temp_file
    temp_file=$(mktemp)
    echo "${all_hcl_files}" > "${temp_file}"

    IFS=',' read -ra exclude_patterns <<< "${exclude_pattern}"
    for pattern in "${exclude_patterns[@]}"; do
      log_message "DEBUG" "Excluding pattern: ${pattern}"
      # Use grep to filter out lines containing the pattern
      grep -v "${pattern}" "${temp_file}" > "${temp_file}.new"
      mv "${temp_file}.new" "${temp_file}"
    done

    hcl_files=$(cat "${temp_file}")
    rm "${temp_file}"

    # Log excluded files count
    local excluded_count
    excluded_count=$(($(echo "${all_hcl_files}" | wc -l) - $(echo "${hcl_files}" | wc -l)))
    log_message "INFO" "Excluded ${excluded_count} files matching patterns: ${exclude_pattern}"
  fi

  # Count total HCL files for reporting
  local total_files
  total_files=$(echo "${hcl_files}" | grep -c -v '^$')
  log_message "INFO" "üìä Found ${total_files} HCL files in ${terragrunt_dir}"

  # Exit early if no files found
  if [[ "${total_files}" -eq 0 ]]; then
    log_message "WARN" "No HCL files found to process"
    return 0
  fi

  # Process each file individually
  local formatted_count=0
  local failed_count=0
  local unchanged_count=0

  log_message "INFO" "üîÑ Formatting HCL files..."
  while IFS= read -r file; do
    if [[ -z "${file}" ]]; then
      continue
    fi

    echo "  Processing: ${file}"
    if ${cmd} --file "${file}" 2>/dev/null; then
      if grep -q "was updated" <<< "$(terragrunt hclfmt --check --file "${file}" 2>&1)"; then
        formatted_count=$((formatted_count+1))
        echo "    ‚úÖ File updated: ${file}"
      else
        unchanged_count=$((unchanged_count+1))
        echo "    ‚ÑπÔ∏è Already formatted: ${file}"
      fi
    else
      failed_count=$((failed_count+1))
      echo "    ‚ùå Failed to format: ${file}"
    fi
  done <<< "${hcl_files}"

  # Show success message with stats
  echo ""
  log_message "INFO" "üìä Formatting Statistics:"
  echo "   - Total files processed: ${total_files}"
  if [[ "${check_mode}" != "true" ]]; then
    echo "   - Files updated: ${formatted_count}"
  fi
  echo "   - Files already formatted: ${unchanged_count}"
  echo "   - Files failed: ${failed_count}"

  if [[ "${check_mode}" == "true" ]]; then
    log_message "INFO" "‚úÖ HCL format check completed"
  else
    log_message "INFO" "‚úÖ HCL formatting completed"
  fi

  return 0
}

# =============================================================================
# TERRAGRUNT HCL VALIDATE
# =============================================================================

# Validate Terragrunt HCL files syntax
# Usage: terragrunt_hclvalidate "/path/to/terragrunt/dir"
terragrunt_hclvalidate() {
  local terragrunt_dir="$1"

  log_message "INFO" "üîç Running Terragrunt HCL validation"

  # Validate inputs
  if [[ ! -d "${terragrunt_dir}" ]]; then
    log_message "ERROR" "Terragrunt directory does not exist: ${terragrunt_dir}"
    return 1
  fi

  # Change to the terragrunt directory
  cd "${terragrunt_dir}" || {
    log_message "ERROR" "Failed to change to directory: ${terragrunt_dir}"
    return 1
  }

  log_message "INFO" "üîÑ Validating HCL files in ${terragrunt_dir}..."

  # Execute terragrunt hclvalidate
  if terragrunt hclvalidate --json; then
    log_message "INFO" "‚úÖ HCL validation successful. All files are valid."
    return 0
  else
    local exit_code=$?
    log_message "ERROR" "‚ùå HCL validation failed. Found invalid files. See JSON output above for details."
    return ${exit_code}
  fi
}

# =============================================================================
# Command-line interface when script is executed directly
# =============================================================================

# Execute requested function when script is called directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
  command_to_run="${1:-terragrunt_format}" # Default to format if no command specified
  shift # Remove the command name from arguments

  case "${command_to_run}" in
    terragrunt_format)
      terragrunt_dir="${1:-.}"
      check_mode="${2:-false}"
      diff_mode="${3:-false}"
      exclude_pattern="${4:-}"
      terragrunt_format "${terragrunt_dir}" "${check_mode}" "${diff_mode}" "${exclude_pattern}"
      ;;
    terragrunt_hclvalidate)
      terragrunt_dir="${1:-.}"
      terragrunt_hclvalidate "${terragrunt_dir}"
      ;;
    *)
      log_message "ERROR" "Unknown command: ${command_to_run}"
      echo "Usage: $0 {terragrunt_format|terragrunt_hclvalidate} [arguments...]" >&2
      exit 1
      ;;
  esac
fi

# Shellcheck directives for sourcing and shell compatibility
# shellcheck shell=bash
# shellcheck disable=SC2155
true
</file>

<file path=".editorconfig">
root = true

[*]
charset = utf-8
end_of_line = lf
insert_final_newline = true
trim_trailing_whitespace = true
max_line_length = 120

# Documentation
[*.{md,txt,xml}]
trim_trailing_whitespace = false
max_line_length = 100

# Infrastructure as Code
[*.{tf,hcl,tfvars,terragrunt.hcl}]
indent_style = space
indent_size = 2

# Shell and Script Files
[*.{sh,bash,zsh,shellcheckrc}]
indent_style = space
indent_size = 2
max_line_length = 100

# Configuration Files
[{.envrc,config/direnv/direnvrc,justfile,*.config}]
indent_style = space
indent_size = 2

# Go Files
[*.go]
indent_style = tab
indent_size = 4

# JSON and Data Files
[*.{json,template,tpl}]
indent_style = space
indent_size = 2

# YAML Files
[*.{yml,yaml}]
indent_style = space
indent_size = 2

# Version and Metadata Files
[*.{version,.terraform-version}]
indent_style = space
indent_size = 2

# Makefiles (use tabs)
[{Makefile,*.mk}]
indent_style = tab
indent_size = 4

# Markdown (special handling)
[*.md]
trim_trailing_whitespace = false
max_line_length = 100
</file>

<file path=".gitattributes">
# Auto detect text files and perform LF normalization
* text=auto eol=lf

# Documentation
*.md text diff=markdown
*.txt text
*.xml text

# Infrastructure as Code
*.tf text diff=terraform
*.tfvars text diff=terraform
*.hcl text diff=hcl
*.terragrunt.hcl text diff=hcl
*.terragrunt-module-manifest text
*.terragrunt-source-manifest text
*.terragrunt-source-version text

# Shell and Script Files
*.sh text diff=bash
*.bash text diff=bash
*.zsh text diff=bash
.shellcheckrc text
justfile text

# Configuration Files
*.config text
.editorconfig text
.envrc text
config/direnv/direnvrc text

# Go Files
*.go text diff=golang
go.mod text diff=golang
go.sum text diff=golang

# JSON and Data Files
*.json text
*.template text
*.tpl text

# Version and Metadata Files
*.version text
.terraform-version text

# Binary and Compiled Files
*.exe binary
*.dll binary
*.so binary
*.dylib binary
*.tfstate binary

# Ignore specific files in export
.github export-ignore
.gitignore export-ignore
.golangci.yml export-ignore
.pre-commit-config.yaml export-ignore
</file>

<file path=".gitignore">
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
node_modules
.pnp
.pnp.js

# Others
.AI*
.aider*
.cursorrules
.cursor*

# testing
coverage

# next.js
.next/
out/
next-env.d.ts

# expo
.expo/
dist/

# production
build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# local env files
.env
.env*.local

# vercel
.vercel

# typescript
*.tsbuildinfo

# turbo
.turbo

# react-email
.react-email
packages/email/public
!packages/email/public/.gitkeep
!packages/email/.react-email/.gitkeep

# Storybook
storybook-static
tailwind.css
.aider*
.AI*
.cursor*
.DS_Store

# Go
*.exe
*.exe~
*.dll
*.so
*.dylib
*.test
*.out
go.sum
vendor/
bin/
tmp/

# Rust
target/
Cargo.lock
*.rs.bk

# Terragrunt
.terragrunt-cache/
*.tfstate
*.tfstate.backup
.terraform/
.terraform.lock.hcl

# Terraform
*.tfplan
crash.log
crash.*.log

tools/infractl/target/*
**/*.exe
**/*.exe~
**/*.dll
**/*.so
**/*.dylib
**/*.out

.infractl-cache
.infractl-cache/*
infra/.infractl-cache
infra/.infractl-cache/*
tools/infractl/target/infractl
tools/infractl/infractl

# Nix and direnv
.direnv/
.direnv.d/
result
result-*
.nix-shell-inputs/
</file>

<file path=".pre-commit-config.yaml">
---
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0 # Updated rev for potentially newer hooks
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
        exclude: &readme_excludes >
          (?x)^(
            infra/terraform/modules/.*/README\.md|
            infra/terragrunt/.*/README\.md|
            docs/.*\.md|
            README\.md|
            CHANGELOG\.md
          )$
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict
      - id: check-executables-have-shebangs
      - id: check-shebang-scripts-are-executable
      - id: check-symlinks
      - id: detect-aws-credentials
        args:
          - '--allow-missing-credentials'
      - id: check-json
        exclude: &json_excludes >
          (?x)^(
            \.release-please-manifest\.json|
            release-please-config\.json|
            ci/ci-terragrunt/dagger\.json|
            \.vscode/.*\.json
          )$
      - id: pretty-format-json
        exclude: *json_excludes

  - repo: https://github.com/google/yamlfmt
    rev: v0.16.0 # Keep existing or update if needed
    hooks:
      - id: yamlfmt
        args: [--conf, .yamlfmt.yml] # Point to the root config file

  - repo: https://github.com/adrienverge/yamllint
    rev: v1.35.1 # Keep existing or update if needed
    hooks:
      - id: yamllint
        name: YAML Lint
        # Removed args: ['-c=.yamllint.yml'] as file doesn't exist
        exclude: *readme_excludes # Exclude markdown files often containing YAML examples

  - repo: https://github.com/gruntwork-io/pre-commit
    rev: v0.1.25 # Keep existing or update if needed
    hooks:
      - id: shellcheck
      - id: terragrunt-hclfmt
      - id: terraform-validate
</file>

<file path="infra/terragrunt/_shared/_units/dni_generator.hcl">
locals {
  # ---------------------------------------------------------------------------------------------------------------------
  # üèóÔ∏è ENVIRONMENT CONFIGURATION ORCHESTRATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Dynamically load and aggregate environment-specific configuration files
  # This mechanism enables a flexible, layered infrastructure configuration approach
  #
  # Configuration Layers:
  # - Environment-level settings (env.hcl)
  # - Global architecture configurations
  # - Hierarchical tag management
  #
  # Key Benefits:
  # - Modular configuration management
  # - Centralized environment settings
  # - Flexible tag inheritance
  # ---------------------------------------------------------------------------------------------------------------------
  env_cfg = read_terragrunt_config(find_in_parent_folders("env.hcl"))

  # üåê Stack Configuration
  # Loads the stack configuration file that serves as the single source of truth
  # for stack-level settings and metadata
  stack_cfg = read_terragrunt_config(find_in_parent_folders("stack.hcl"))

  # ---------------------------------------------------------------------------------------------------------------------
  # üåê GLOBAL ARCHITECTURE CONFIGURATION RESOLVER
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Centralize and standardize infrastructure-wide configuration
  # Loads the root configuration file that serves as the single source of truth
  # for infrastructure-level settings and metadata
  #
  # Key Responsibilities:
  # - Provide global configuration context
  # - Enable consistent infrastructure metadata
  # - Support cross-module configuration sharing
  # ---------------------------------------------------------------------------------------------------------------------
  cfg = read_terragrunt_config("${find_in_parent_folders("config.hcl")}")

  # ---------------------------------------------------------------------------------------------------------------------
  # üè∑Ô∏è INTELLIGENT TAG ORCHESTRATION SYSTEM
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Create a sophisticated, hierarchical tag management mechanism
  # Implements a multi-layered tagging strategy that allows for:
  # - Global tag inheritance
  # - Environment-specific tag augmentation
  # - Unit-level tag customization
  #
  # Tag Hierarchy (from broadest to most specific):
  # 1. Global Architecture Tags üåê
  # 2. Environment-Level Tags üåç
  # 3. Unit-Specific Tags üß©
  # 4. Stack-Level Tags üìö
  #
  # Benefits:
  # - Consistent resource identification
  # - Flexible tag management
  # - Enhanced resource tracking and compliance
  # ---------------------------------------------------------------------------------------------------------------------
  unit_tags = {
    Unit = "dni-generator"
    Type = "random-generator"
  }

  # üîó TAG SOURCE AGGREGATION
  # Collect tags from different configuration levels
  env_tags    = local.env_cfg.locals.tags
  global_tags = local.cfg.locals.tags
  stack_tags  = local.stack_cfg.locals.tags

  # üß© FINAL TAG COMPOSITION
  # Merge tags with a clear precedence strategy
  # Precedence: Unit Tags > Environment Tags > Global Tags
  all_tags = merge(
    local.env_tags,
    local.unit_tags,
    local.global_tags,
    local.stack_tags
  )

  # ---------------------------------------------------------------------------------------------------------------------
  # üîß GIT MODULE SOURCE CONFIGURATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Intelligent Terraform module source management with:
  # - Centralized default configuration
  # - Flexible repository and path selection
  # - Semantic version control
  #
  git_base_url              = local.cfg.locals.cfg_git.git_base_urls.github
  tf_module_repository      = "your-org/terraform-modules.git"
  tf_module_version_default = get_env("TG_STACK_TF_MODULE_DNI_GENERATOR_VERSION_DEFAULT", "v0.1.0")
  tf_module_path_default    = "modules/dni-generator"

  tf_module_source = format(
    "%s%s//%s",
    local.git_base_url,
    local.tf_module_repository,
    local.tf_module_path_default
  )

  echo_tf_module_source = run_cmd("sh", "-c", "echo 'üîß  TF Module Source (parent): ${local.tf_module_source} version: ${local.tf_module_version_default}'")

  # ---------------------------------------------------------------------------------------------------------------------
  # üåê SPECIFIC MODULE CONFIGURATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Here we define the specific configuration for the module.
  # This is useful when we need to override the default configuration for the module.
  #
  # TODO: Add specific locals, or configuration for your module to be computed here.
}

# üîó DEPENDENCIES
# These blocks define dependencies for the Terragrunt configuration.
# Dependencies allow for the management of resources that rely on
# other configurations, ensuring that they are created or updated
# in the correct order. This promotes modularity and reusability
# of infrastructure components.

dependency "age_generator" {
  config_path = "${find_in_parent_folders("dni/age_generator")}"
  mock_outputs = {
    generated_age = 30
  }
}

dependency "name_generator" {
  config_path = "${find_in_parent_folders("dni/name_generator")}"
  mock_outputs = {
    full_name = "john-abc123"
  }
}

dependency "lastname_generator" {
  config_path = "${find_in_parent_folders("dni/lastname_generator")}"
  mock_outputs = {
    full_lastname = "smith-xyz789"
  }
}

dependencies {
  paths = [
    "${find_in_parent_folders("dni/age_generator")}",
    "${find_in_parent_folders("dni/name_generator")}",
    "${find_in_parent_folders("dni/lastname_generator")}"
  ]
}

# üöÄ TERRAGRUNT INFRASTRUCTURE UNIT CONFIGURATION
# Defines the input parameters for the infrastructure unit
# Combines global configuration, metadata, and tag management
inputs = {
  # Ensure all variables from variables.tf are explicitly set
  prefix                  = ""   # Default empty string
  generate_control_letter = true # Default true
  name                    = dependency.name_generator.outputs.full_name
  lastname                = dependency.lastname_generator.outputs.full_lastname

  # CRITICAL: Explicitly set the age variable
  # Use the dependency output or provide a default/mock value
  age = dependency.age_generator.outputs.generated_age

  country = "Spain" # Default country
  tags    = local.all_tags
}
</file>

<file path="infra/terragrunt/_shared/_units/lastname_generator.hcl">
// üß© Shared Unit Configuration for Lastname Generator Module

locals {
  # ---------------------------------------------------------------------------------------------------------------------
  # üèóÔ∏è ENVIRONMENT CONFIGURATION ORCHESTRATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Dynamically load and aggregate environment-specific configuration files
  # This mechanism enables a flexible, layered infrastructure configuration approach
  #
  # Configuration Layers:
  # - Environment-level settings (env.hcl)
  # - Global architecture configurations
  # - Hierarchical tag management
  #
  # Key Benefits:
  # - Modular configuration management
  # - Centralized environment settings
  # - Flexible tag inheritance
  # ---------------------------------------------------------------------------------------------------------------------
  env_cfg = read_terragrunt_config(find_in_parent_folders("env.hcl"))

  # üåê Stack Configuration
  # Loads the stack configuration file that serves as the single source of truth
  # for stack-level settings and metadata
  stack_cfg = read_terragrunt_config(find_in_parent_folders("stack.hcl"))

  # ---------------------------------------------------------------------------------------------------------------------
  # üåê GLOBAL ARCHITECTURE CONFIGURATION RESOLVER
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Centralize and standardize infrastructure-wide configuration
  # Loads the root configuration file that serves as the single source of truth
  # for infrastructure-level settings and metadata
  #
  # Key Responsibilities:
  # - Provide global configuration context
  # - Enable consistent infrastructure metadata
  # - Support cross-module configuration sharing
  # ---------------------------------------------------------------------------------------------------------------------
  cfg = read_terragrunt_config("${find_in_parent_folders("config.hcl")}")

  # ---------------------------------------------------------------------------------------------------------------------
  # üè∑Ô∏è INTELLIGENT TAG ORCHESTRATION SYSTEM
  # ---------------------------------------------------------------------------------------------------------------------
  # Purpose: Create a sophisticated, hierarchical tag management mechanism
  # Implements a multi-layered tagging strategy that allows for:
  # - Global tag inheritance
  # - Environment-specific tag augmentation
  # - Unit-level tag customization
  #
  # Tag Hierarchy (from broadest to most specific):
  # 1. Global Architecture Tags üåê
  # 2. Environment-Level Tags üåç
  # 3. Unit-Specific Tags üß©
  # 4. Stack-Level Tags üìö
  #
  # Benefits:
  # - Consistent resource identification
  # - Flexible tag management
  # - Enhanced resource tracking and compliance
  # ---------------------------------------------------------------------------------------------------------------------
  unit_tags = {
    Unit = "lastname-generator"
    Type = "random-generator"
  }

  # üîó TAG SOURCE AGGREGATION
  # Collect tags from different configuration levels
  env_tags    = local.env_cfg.locals.tags
  global_tags = local.cfg.locals.tags
  stack_tags  = local.stack_cfg.locals.tags

  # üß© FINAL TAG COMPOSITION
  # Merge tags with a clear precedence strategy
  # Precedence: Unit Tags > Environment Tags > Global Tags
  all_tags = merge(
    local.env_tags,
    local.unit_tags,
    local.global_tags,
    local.stack_tags
  )

  # ---------------------------------------------------------------------------------------------------------------------
  # üîß GIT MODULE SOURCE CONFIGURATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Intelligent Terraform module source management with:
  # - Centralized default configuration
  # - Flexible repository and path selection
  # - Semantic version control
  #
  git_base_url              = local.cfg.locals.cfg_git.git_base_urls.github
  tf_module_repository      = "your-org/terraform-modules.git"
  tf_module_version_default = get_env("TG_STACK_TF_MODULE_LASTNAME_GENERATOR_VERSION_DEFAULT", "v0.1.0")
  tf_module_path_default    = "modules/lastname-generator"

  tf_module_source = format(
    "%s%s//%s",
    local.git_base_url,
    local.tf_module_repository,
    local.tf_module_path_default
  )

  echo_tf_module_source = run_cmd("sh", "-c", "echo 'üîß  TF Module Source (parent): ${local.tf_module_source} version: ${local.tf_module_version_default}'")

  # ---------------------------------------------------------------------------------------------------------------------
  # üåê SPECIFIC MODULE CONFIGURATION
  # ---------------------------------------------------------------------------------------------------------------------
  # Here we define the specific configuration for the module.
  # This is useful when we need to override the default configuration for the module.
  #
  # TODO: Add specific locals, or configuration for your module to be computed here.
}

# üîó DEPENDENCY
# This block defines a dependency for the Terragrunt configuration.
# Dependencies allow for the management of resources that rely on
# other configurations, ensuring that they are created or updated
# in the correct order. This promotes modularity and reusability
# of infrastructure components, making it easier to manage complex
# setups. Dependencies can also include mock outputs for testing
# purposes without needing to provision the actual resources.
# dependency "cloudflare_dns_zone" {
#   config_path = find_in_parent_folders("<stack>/<unit>")
#   mock_outputs = {
#     cloudflare_zone_ids = {
#       "fake-zone-id" = "fake-zone-id"
#     }
#   }
# }

# üöÄ TERRAGRUNT INFRASTRUCTURE UNIT CONFIGURATION
# Defines the input parameters for the infrastructure unit
# Combines global configuration, metadata, and tag management
inputs = {
  input_lastname = "smith" # Required input
  suffix_length  = 6       # Optional, using default
  gender         = "any"   # Optional, using default
  tags           = local.all_tags
}

# Add explicit dependencies block
dependencies {
  paths = [] # No dependencies required
}
</file>

<file path="infra/terragrunt/global/dni/name_generator/terragrunt.hcl">
# üåê Root Terragrunt Configuration Inclusion
# This block imports the common configuration from the parent directory's terragrunt.hcl file.
# It enables consistent configuration sharing across multiple Terragrunt modules, ensuring that
# all modules can access shared settings and parameters defined at the root level.
include "root" {
  path           = find_in_parent_folders("root.hcl")
  merge_strategy = "deep"
}

# üß© Shared Units Configuration
# This block imports standardized component configuration from the shared components directory.
# It is important to note that any modifications should be made in the shared component configuration file
# located at: `_shared/_components/quota-generator.hcl`. This ensures that changes are reflected
# across all modules that utilize this shared configuration.
include "shared" {
  path           = "${get_terragrunt_dir()}/../../../_shared/_units/name_generator.hcl"
  expose         = true
  merge_strategy = "deep"
}

locals {
  # üîß Terraform Module Source Resolution
  # ---------------------------------------------------------------------------------------------------------------------
  # Intelligent Terraform module source management with:
  # - Local path testing support
  # - Version override capability
  # - Fallback to default version
  # - Dynamic source path computation
  #
  # Resolution Strategy:
  # 1. If local_path is provided, use it for local testing
  # 2. If version_override is set, use that version
  # 3. Otherwise, fall back to the default version from shared configuration
  #
  # Enables flexible module sourcing for:
  # - Local development and testing
  # - Precise version control
  # - Consistent module referencing across infrastructure
  # ---------------------------------------------------------------------------------------------------------------------
  tf_module_local_path       = "${include.shared.locals.cfg_git.git_base_urls.local}/name-generator"
  tf_module_version_override = ""
  tf_module_version          = local.tf_module_version_override != "" ? local.tf_module_version_override : include.shared.locals.tf_module_version_default
  tf_module_source           = include.shared.locals.tf_module_source

  tg_source_computed    = local.tf_module_local_path != "" ? local.tf_module_local_path : format("%s?ref=%s", local.tf_module_source, local.tf_module_version)
  echo_tf_module_source = run_cmd("sh", "-c", "echo 'üîß  TF Module Source (child): ${local.tg_source_computed}'")
}

# üöÄ Terraform Source Configuration
# Dynamically resolves the Terraform module source based on:
# - Local testing path (if provided)
# - Shared module source
# - Specific version reference
terraform {
  source = local.tg_source_computed
}

# üì¶ Inputs Configuration
# Provides an empty inputs block to satisfy Terragrunt configuration requirements
# Actual inputs are managed in the shared configuration
inputs = {}
</file>

<file path="infra/terragrunt/config.hcl">
// üåê Terragrunt Global Configuration
// Centralized infrastructure configuration management system that orchestrates shared settings,
// provider management, and global variables across the entire infrastructure ecosystem.
// This configuration serves as the single source of truth for infrastructure-wide settings.

locals {
  // üìÇ Shared Configuration Loading
  // Standardizes and centralizes configuration across infrastructure units by reading
  // configuration files from a predefined shared directory. This approach enables modular
  // and reusable infrastructure settings, ensuring consistency and reducing duplication.
  // When adding new shared configurations, maintain the established directory structure.
  # shared_config_app = read_terragrunt_config("${get_parent_terragrunt_dir()}/_shared/_config/app.hcl")
  shared_config_app          = read_terragrunt_config("${get_terragrunt_dir()}/_shared/_config/app.hcl")
  shared_config_remote_state = read_terragrunt_config("${get_terragrunt_dir()}/_shared/_config/remote_state.hcl")
  shared_config_tags         = read_terragrunt_config("${get_terragrunt_dir()}/_shared/_config/tags.hcl")
  shared_config_git          = read_terragrunt_config("${get_terragrunt_dir()}/_shared/_config/git.hcl")

  // üîå Provider Configuration Management
  // Safely read provider configurations if they exist
  unit_cfg_providers = try(
    read_terragrunt_config("${get_original_terragrunt_dir()}/unit_cfg_providers.hcl"),
    {
      locals = {
        providers = []
      }
    }
  )

  unit_cfg_versions = try(
    read_terragrunt_config("${get_original_terragrunt_dir()}/unit_cfg_versions.hcl"),
    {
      locals = {
        versions = []
      }
    }
  )

  // üõ°Ô∏è Provider Configuration Override
  // Flag to control provider file generation
  // When set to "true", forces provider file generation even if no providers are defined
  // When set to "false", prevents provider file generation if no providers are found
  // Default behavior (null/unset) allows generation based on configuration presence
  enable_providers_override = get_env("TG_STACK_FLAG_ENABLE_PROVIDERS_OVERRIDE", "true")

  // üåê Dynamic Provider Configuration
  shared_config_providers = local.unit_cfg_providers.locals

  // üîå Provider Configuration Management
  // Flag to control version file generation
  // When set to "true", forces version file generation even if no versions are defined
  // When set to "false", prevents version file generation if no versions are found
  // Default behavior (null/unset) allows generation based on configuration presence
  enable_versions_override = get_env("TG_STACK_FLAG_ENABLE_VERSIONS_OVERRIDE", "true")
  generate_versions_file   = length(local.unit_cfg_versions.locals.versions) > 0 && (local.enable_versions_override == "true" || local.enable_versions_override == null) ? true : false
  generate_providers_file  = length(local.shared_config_providers.providers) > 0 && (local.enable_providers_override == "true" || local.enable_providers_override == null) ? true : false

  // üîç Configuration Extraction
  // Simplifies access to nested configuration structures by creating direct references
  // to local blocks in shared configuration files. This approach reduces complex nested
  // access and provides a clean, straightforward way to retrieve configuration values.
  // Helps maintain readability and reduces the chance of configuration access errors.
  cfg_app          = local.shared_config_app.locals
  cfg_remote_state = local.shared_config_remote_state.locals
  cfg_tags         = local.shared_config_tags.locals
  cfg_git          = local.shared_config_git.locals
  // üè∑Ô∏è Global Resource Tagging
  // Implements consistent resource identification and management through a centralized
  // tagging strategy. These tags enable cost tracking, compliance monitoring, and
  // comprehensive resource management across the entire infrastructure.
  // Ensure all tags are meaningful, descriptive, and follow organizational standards.
  tags = local.cfg_tags.tags

  // üîñ Project Metadata Management
  // Maintains consistent project identification across infrastructure by providing
  // a single source of truth for project-level metadata. This ensures that project
  // name and version are consistently applied throughout all infrastructure components.
  // Update these values in the shared app configuration file to propagate changes.
  product_name    = local.cfg_app.product_name
  product_version = local.cfg_app.product_version

  // üåç Runtime Environment Configuration
  // Provides flexible, environment-driven configuration with sensible defaults to
  // support multi-environment deployments. Allows runtime configuration flexibility
  // through environment variables, enabling easy environment-specific customizations.
  // Use environment variables to override default settings when needed.
  deployment_region_unnormalized = get_env("TG_STACK_DEPLOYMENT_REGION", "us-east-1")
  deployment_region              = lower(trimspace(local.deployment_region_unnormalized))

  // üíæ Remote State Management
  // Configures and standardizes Terraform state storage by centralizing remote state
  // configuration. Ensures consistent state management across all infrastructure units,
  // providing a reliable and predictable approach to tracking infrastructure changes.
  // Modify remote state settings in the remote_state.hcl configuration file as your
  // infrastructure requirements evolve.
  bucket_name           = local.cfg_remote_state.bucket_name
  lock_table            = local.cfg_remote_state.lock_table
  region                = local.cfg_remote_state.region
  state_object_basename = local.cfg_remote_state.state_object_basename
  backend_tf_filename   = local.cfg_remote_state.backend_tf_filename

  // üì¶ Terraform Version Management
  // Centralized configuration for Terraform version control and file generation.
  // These settings control both the enforced version and the generation of .terraform-version files.
  terraform_version_disabled_comments = <<-EOT
# Terraform Version File Generation Disabled
# To enable version file generation, set TG_STACK_FLAG_ENABLE_TERRAFORM_VERSION_FILE_OVERRIDE to "true"
# And, set TG_STACK_TF_VERSION to the desired Terraform version
# Current setting prevents automatic version file creation
    EOT

  // üîí Version Enforcement Configuration
  // To modify the enforced Terraform version:
  // 1. Set TG_STACK_TF_VERSION environment variable, or
  // 2. Update the default version here (currently "1.11.3")
  tf_version_enforced_unnormalised = get_env("TG_STACK_TF_VERSION", "1.11.3")
  tf_version_enforced              = lower(trimspace(local.tf_version_enforced_unnormalised))

  // üéõÔ∏è Version File Generation Control
  // To enable .terraform-version file generation:
  // 1. Set TG_STACK_FLAG_ENABLE_TERRAFORM_VERSION_FILE_OVERRIDE to "true"
  // 2. Default is "false" to prevent automatic generation
  // When enabled: Creates .terraform-version with tf_version_enforced
  // When disabled: Creates file with instructions (see terraform_version_disabled_comments)
  enable_terraform_version_file_override = get_env("TG_STACK_FLAG_ENABLE_TERRAFORM_VERSION_FILE_OVERRIDE", "false")
}
</file>

<file path="release-please-config.json">
{
  "$schema": "https://raw.githubusercontent.com/googleapis/release-please/main/schemas/config.json",
  "packages": {
    ".": {
      "bump-minor-pre-major": true,
      "bump-patch-for-minor-pre-major": true,
      "changelog-path": "CHANGELOG.md",
      "changelog-sections": [
        {
          "section": "Features",
          "type": "feat"
        },
        {
          "section": "Bug Fixes",
          "type": "fix"
        },
        {
          "section": "Other",
          "type": "chore"
        },
        {
          "section": "Docs",
          "type": "docs"
        },
        {
          "section": "Performance",
          "type": "perf"
        },
        {
          "hidden": true,
          "section": "Build",
          "type": "build"
        },
        {
          "section": "Dependency Updates",
          "type": "deps"
        },
        {
          "hidden": true,
          "section": "CI",
          "type": "ci"
        },
        {
          "section": "Refactoring",
          "type": "refactor"
        },
        {
          "hidden": true,
          "section": "Reverts",
          "type": "revert"
        },
        {
          "hidden": true,
          "section": "Styling",
          "type": "style"
        },
        {
          "hidden": true,
          "section": "Tests",
          "type": "test"
        }
      ],
      "draft": false,
      "extra-files": [
        "README.md"
      ],
      "prerelease": false,
      "release-type": "simple"
    }
  }
}
</file>

<file path=".github/labeler.yml">
---
# Documentation changes
docs:
  - changed-files:
      any-glob-to-any-file: ['**/*.md', 'docs/**/*']

# Terragrunt infrastructure changes
terragrunt:
  - changed-files:
      any-glob-to-any-file: ['infra/terragrunt/**/*.hcl']

# Terraform infrastructure changes
terraform:
  - changed-files:
      any-glob-to-any-file: ['infra/terraform/**/*.tf']

# CI/CD workflow changes
ci:
  - changed-files:
      any-glob-to-any-file: ['.github/**/*']

# Test changes
tests:
  - changed-files:
      any-glob-to-any-file: ['**/*_test.*']

dx:
  - changed-files:
      any-glob-to-any-file: ['justfile']

github:
  - changed-files:
      any-glob-to-any-file: ['.github/**/*']

cd:
  - changed-files:
      any-glob-to-any-file: ['@release-please-config.json', '@.release-please-manifest.json']
</file>

<file path=".envrc">
#!/usr/bin/env bash
# Terragrunt Reference Architecture - Environment Configuration
# Simplified and modular environment setup

# Exit immediately if any command fails
set -e

# Ensure PROJECT_ROOT is set reliably
PROJECT_ROOT="${PROJECT_ROOT:-$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)}"
export PROJECT_ROOT

# Source utility functions
# shellcheck source=./scripts/envrc-utils.sh
source "${PROJECT_ROOT}/scripts/envrc-utils.sh"

# Core initialization
_core_init

# =====================================================================
# üîß CUSTOMIZATION SECTION
# =====================================================================
# Configuration variables are grouped by functional categories for easier
# customization and maintenance.

# ---------------------------------------------------------------------
# 1Ô∏è‚É£ PROJECT METADATA
# ---------------------------------------------------------------------
# Define core project information and authorship
# ---------------------------------------------------------------------
TG_STACK_APP_AUTHOR="${TG_STACK_APP_AUTHOR:-Your Name}"
_safe_export TG_STACK_APP_AUTHOR "${TG_STACK_APP_AUTHOR}"

TG_STACK_APP_PRODUCT_NAME="${TG_STACK_APP_PRODUCT_NAME:-your-app-name}"
_safe_export TG_STACK_APP_PRODUCT_NAME "${TG_STACK_APP_PRODUCT_NAME}"

# ---------------------------------------------------------------------
# 2Ô∏è‚É£ CLOUD PROVIDER & REGION SETTINGS
# ---------------------------------------------------------------------
# Configure cloud provider-specific settings
# ---------------------------------------------------------------------
DEFAULT_REGION="${DEFAULT_REGION:-us-east-1}"
_safe_export DEFAULT_REGION "${DEFAULT_REGION}"

# ---------------------------------------------------------------------
# 3Ô∏è‚É£ TERRAFORM & TERRAGRUNT CONFIGURATION
# ---------------------------------------------------------------------
# Control Terraform behavior and version requirements
# ---------------------------------------------------------------------
# Core Terraform Settings
# The following uses the new recommended environment variables
# instead of the deprecated ones

# Replace deprecated TF_INPUT with TG_NON_INTERACTIVE
TG_NON_INTERACTIVE="${TG_NON_INTERACTIVE:-true}"
_safe_export TG_NON_INTERACTIVE "${TG_NON_INTERACTIVE}"

TG_STACK_TF_VERSION="${TG_STACK_TF_VERSION:-1.9.0}"
_safe_export TG_STACK_TF_VERSION "${TG_STACK_TF_VERSION}"

# Terragrunt Performance Settings
TERRAGRUNT_DOWNLOAD_DIR="${TERRAGRUNT_DOWNLOAD_DIR:-${HOME}/.terragrunt-cache/$(basename "${PROJECT_ROOT}")}"
_safe_export TERRAGRUNT_DOWNLOAD_DIR "${TERRAGRUNT_DOWNLOAD_DIR}"

TERRAGRUNT_CACHE_MAX_AGE="${TERRAGRUNT_CACHE_MAX_AGE:-168h}"
_safe_export TERRAGRUNT_CACHE_MAX_AGE "${TERRAGRUNT_CACHE_MAX_AGE}"

# Terragrunt Behavior Settings
# Replace deprecated TERRAGRUNT_LOG_LEVEL with TG_LOG_LEVEL
TG_LOG_LEVEL="${TG_LOG_LEVEL:-info}"
_safe_export TG_LOG_LEVEL "${TG_LOG_LEVEL}"

TERRAGRUNT_DISABLE_CONSOLE_OUTPUT="${TERRAGRUNT_DISABLE_CONSOLE_OUTPUT:-false}"
_safe_export TERRAGRUNT_DISABLE_CONSOLE_OUTPUT "${TERRAGRUNT_DISABLE_CONSOLE_OUTPUT}"

# Replace deprecated TERRAGRUNT_AUTO_INIT with TG_NO_AUTO_INIT
# Note: The logic is inverted, so we use false when auto-init is desired
TG_NO_AUTO_INIT="${TG_NO_AUTO_INIT:-false}"
_safe_export TG_NO_AUTO_INIT "${TG_NO_AUTO_INIT}"

# Replace deprecated TERRAGRUNT_AUTO_RETRY with TG_NO_AUTO_RETRY
# Note: The logic is inverted, so we use false when auto-retry is desired
TG_NO_AUTO_RETRY="${TG_NO_AUTO_RETRY:-false}"
_safe_export TG_NO_AUTO_RETRY "${TG_NO_AUTO_RETRY}"

# ---------------------------------------------------------------------
# 4Ô∏è‚É£ LOGGING & DIAGNOSTICS
# ---------------------------------------------------------------------
# Configure output verbosity and log storage
# ---------------------------------------------------------------------
LOG_LEVEL="${LOG_LEVEL:-info}"
_safe_export LOG_LEVEL "${LOG_LEVEL}"

LOG_DIR_PATH="${HOME}/.logs/${TG_STACK_APP_PRODUCT_NAME}"
# Ensure log directory exists
mkdir -p "${LOG_DIR_PATH}" 2>/dev/null || true
_safe_export LOG_DIR "${LOG_DIR_PATH}"

# ---------------------------------------------------------------------
# 5Ô∏è‚É£ REMOTE STATE CONFIGURATION
# ---------------------------------------------------------------------
# Define backend storage for Terraform state
# ---------------------------------------------------------------------
# Placeholder values - MUST be replaced in actual configuration
_safe_export TG_STACK_REMOTE_STATE_BUCKET_NAME "terraform-state-makemyinfra"
_safe_export TG_STACK_REMOTE_STATE_LOCK_TABLE "terraform-state-makemyinfra"

# ---------------------------------------------------------------------
# 6Ô∏è‚É£ CUSTOM USE-CASE VARIABLES
# ---------------------------------------------------------------------
# Add your custom environment variables below
# Examples:
# _safe_export TG_CUSTOM_VAR_1 "value1"
# _safe_export TG_CUSTOM_VAR_2 "value2"
# ---------------------------------------------------------------------

# Tool Availability Check
_validate_layer_config "CORE" \
  "DEFAULT_REGION" \
  "TG_STACK_APP_PRODUCT_NAME"

# Final initialization log
_log "INFO" "Environment for ${TG_STACK_APP_PRODUCT_NAME} initialized successfully"

# Display layer information for core Terragrunt settings
_layer_env_info "TERRAGRUNT" \
  "TERRAGRUNT_DOWNLOAD_DIR:Terragrunt Download Directory" \
  "TG_LOG_LEVEL:Terragrunt Log Level" \
  "TG_NO_AUTO_INIT:Disable Auto Initialize" \
  "TG_NO_AUTO_RETRY:Disable Auto Retry"

# Display all exported environment variables
_display_exported_vars ""
</file>

<file path="README.md">
# Terragrunt Reference Architecture V3 üèóÔ∏è

## üåê Overview

A cutting-edge, production-grade infrastructure management framework (or, just a reference architecture for infrrastructure-at-scale, with [Terragrunt](https://terragrunt.gruntwork.io/)) designed to be modular, flexible, and scalable. It's very opinionated, and it came from years of experience building infrastructure at scale. Use it, adapt it to your needs, and make it your own.

## ‚ú® Key Features

| Feature                                   | Description                                                                                                                                                                                                                                                                                                        |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| üß© Modular Architecture                   | Discrete, composable infrastructure units, that can inherit from shared unit's configurations, and from their parents in the hierarchy (stacks, or environments)                                                                                                                                                   |
| üåà Highly Hierarchical Flexible Overrides | Designed to support multiple environments (the most common abstraction), where each environment can hold many stacks, and each stack can hold many units                                                                                                                                                           |
| üöÄ Multi-Provider Compatibility           | Support for diverse cloud and infrastructure providers. Dynamically set providers, versions and overrides. It passes the control of the providers, and versions (if applicable) to the units, which are the smallest components of the architecture that deals directly with the terraform abstractions (modules). |
| üîß Dynamic Environment Configuration      | Hierarchical, secure, and extensible environment variable management with recursive `.envrc` file inheritance, secure variable export with validation, automatic inheritance and override mechanisms, and comprehensive logging and tracing of environment setup                                                  |
| üßº Clean Code Configuration               | Strict separation of configuration logic, with clear distinctions between global settings, provider management, and Terragrunt generation rules in `config.hcl` and `root.hcl`. Implements comprehensive commenting and modular configuration design.                                                              |

## üìê Architecture Overview

### Hierarchical Infrastructure Organization

```mermaid
graph TD
    subgraph "Root Configuration"
        A[Root Configuration]
    end

    subgraph "Shared Layer"
        SharedConfig[Shared Configuration /_shared/_config]
        SharedUnits[Shared Units /_shared/_units]
    end

    subgraph "Infrastructure Hierarchy"
        Environment[Environment]
        Stack[Stack]
        Unit[Unit]
    end

    A -->|Inherit common configuration, metadata, etc. | SharedConfig
    A -->|Provides Base Templates, and shared units configuration| SharedUnits
    A -->|References an| Environment

    SharedConfig -->|Provides Configuration specific to the stack| Stack
    SharedConfig -->|Configures and resolve automatically common settings| Unit

    SharedUnits -->|Automatically includes, and injects environment variables, or configuration| Environment
    SharedUnits -->|Includes stack-specific configuration| Stack
    SharedUnits -->|Minimal configuration at the unit level, most of it is defined in their parents units| Unit

    Environment -->|Organizes| Stack
    Stack -->|Contains| Unit

    classDef rootConfig fill:#f96,stroke:#333,stroke-width:2px;
    classDef sharedComponent fill:#6f9,stroke:#333,stroke-width:2px;
    classDef infrastructureHierarchy fill:#69f,stroke:#333,stroke-width:2px;

    class A rootConfig;
    class SharedConfig,SharedUnits sharedComponent;
    class Environment,Stack,Unit infrastructureHierarchy;
```

### Project Structure

The project structure follows the pattern:

- **Environment**: A collection of stacks, and units. The most logical approach to organise the infrastructure is to group the infrastructure by environment.
  - **Stack**: A collection of units.
    - **Unit**: A collection of terraform modules.

> **NOTE**: Each layer is fully flexible, and can be changed to fit the needs of your project, or particular domain. E.g.: instead of environment, you could use a different layer to group the infrastructure by region, or by application.

The project structure is as follows:

```
infra/terragrunt
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ _shared
‚îÇ   ‚îú‚îÄ‚îÄ _config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.hcl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ remote_state.hcl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tags.hcl
‚îÇ   ‚îî‚îÄ‚îÄ _units
‚îÇ       ‚îú‚îÄ‚îÄ README.md
‚îÇ       ‚îú‚îÄ‚îÄ <unit>.hcl
‚îÇ       ‚îú‚îÄ‚îÄ <unit-2>.hcl
‚îú‚îÄ‚îÄ _templates
‚îú‚îÄ‚îÄ config.hcl
‚îú‚îÄ‚îÄ default.tfvars
‚îú‚îÄ‚îÄ <environment>
‚îÇ   ‚îú‚îÄ‚îÄ default.tfvars
‚îÇ   ‚îú‚îÄ‚îÄ <stack>
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ <unit>
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ terragrunt.hcl
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unit_cfg_providers.hcl
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ unit_cfg_versions.hcl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ <unit-2>
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ terragrunt.hcl
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unit_cfg_providers.hcl
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ unit_cfg_versions.hcl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stack.hcl
‚îÇ   ‚îî‚îÄ‚îÄ env.hcl
‚îî‚îÄ‚îÄ root.hcl
```

## Cool things inside? üåü

### üîê Environment Variable Management with `.envrc`

A sophisticated, secure environment variable management system powered by [direnv](https://direnv.net/). It works by recursively loading environment variables through a hierarchical structure of `.envrc` files:

- `.envrc` in the root directory: Sets global defaults and core environment variables
- `infra/terragrunt/.envrc`: Terragrunt-specific configuration that inherits from the root
- `infra/terragrunt/<environment>/.envrc`: Environment-specific variables (dev, staging, prod)
- `infra/terragrunt/<environment>/<stack>/.envrc`: Stack-specific variables (optional)

Each `.envrc` file inherits from its parent using `source_up` and can override or extend variables as needed. This creates a clean, hierarchical configuration that's easy to manage and customize.

> **NOTE**: Environment variables defined in child directories will override those from parent directories, allowing for precise customization at each level.

### üîß Environment Setup

To set up your environment:

1. Install [direnv](https://direnv.net/) if you haven't already
2. Run `just setup-env` to create a basic `.envrc` file if needed
3. Edit the `.envrc` files at different levels to customize your environment
4. Run `direnv allow` in each directory to apply changes

Each `.envrc` file is organized into clear sections:

- **HELPER FUNCTIONS**: Utility functions for logging, validation, and secure variable export
- **ENVIRONMENT VARIABLES**: Customizable variables organized by category
- **CUSTOM ENVIRONMENT VARIABLES**: Section for adding your own project-specific variables

### üîß Dynamic Environment Variable Management

A sophisticated environment variable management system powered by [direnv](https://direnv.net/).

#### Key Features
- Hierarchical inheritance of variables across project layers
- Secure validation and export of environment variables
- Flexible configuration across different environments

#### Supported Environment Variables

For a comprehensive list of supported environment variables, their descriptions, and customization levels, please refer to the [Environment Variables Documentation](docs/environment-variables.md).

| Category | Variable Name | Description | Default Value | Customization Level |
| -------- | ------------- | ----------- | ------------- | ------------------- |
| **Terragrunt Flags** | `TG_STACK_FLAG_ENABLE_PROVIDERS_OVERRIDE` | Controls provider file generation | `"true"` | Global/Unit |
| | `TG_STACK_FLAG_ENABLE_VERSIONS_OVERRIDE` | Controls version file generation | `"true"` | Global/Unit |
| | `TG_STACK_FLAG_ENABLE_TERRAFORM_VERSION_FILE_OVERRIDE` | Controls .terraform-version file generation | `"false"` | Global |
| **Deployment Configuration** | `TG_STACK_DEPLOYMENT_REGION` | Deployment AWS region | `"us-east-1"` | Global/Environment |
| **Terraform Version** | `TG_STACK_TF_VERSION` | Enforced Terraform version | `"1.9.0"` | Global |
| **Provider Credentials** | `TG_STACK_PROVIDER_CREDENTIAL` | Provider authentication credentials | `""` (empty) | Unit-specific |
| **Application Metadata** | `TG_STACK_APP_PRODUCT_NAME` | Project/application name | `"my-app"` | Global |
| | `TG_STACK_APP_PRODUCT_VERSION` | Project/application version | `"0.0.0"` | Global |
| | `TG_STACK_APP_AUTHOR` | Configuration author | `""` (empty) | Global |
| **Environment** | `TG_ENVIRONMENT` | Current environment | `"development"` | Environment-specific |
| **Remote State** | `TG_STACK_REMOTE_STATE_BUCKET_NAME` | S3 bucket for remote state | `""` (empty) | Global |
| | `TG_STACK_REMOTE_STATE_LOCK_TABLE` | DynamoDB lock table | `""` (empty) | Global |
| | `TG_STACK_REMOTE_STATE_REGION` | Remote state storage region | `"us-east-1"` | Global |
| | `TG_STACK_REMOTE_STATE_OBJECT_BASENAME` | Remote state file basename | `"terraform.tfstate.json"` | Global |
| | `TG_STACK_REMOTE_STATE_BACKEND_TF_FILENAME` | Backend configuration filename | `"backend.tf"` | Global |
| **Terragrunt Configuration** | `TERRAGRUNT_DOWNLOAD_DIR` | Terragrunt cache directory | `"${HOME}/.terragrunt-cache/$(basename "$PROJECT_ROOT")"` | Global |
| | `TERRAGRUNT_CACHE_MAX_AGE` | Terragrunt cache expiration | `"168h"` (7 days) | Global |
| | `TERRAGRUNT_LOG_LEVEL` | Terragrunt logging verbosity | `"info"` | Global |
| | `TERRAGRUNT_DISABLE_CONSOLE_OUTPUT` | Console output control | `"false"` | Global |
| | `TERRAGRUNT_AUTO_INIT` | Automatic Terragrunt initialization | `"true"` | Global |
| | `TERRAGRUNT_AUTO_RETRY` | Automatic retry on failure | `"true"` | Global |
| **Terraform Configuration** | `TF_INPUT` | Disable interactive Terraform input | `"0"` | Global |
| **Project-wide Variables** | `PROJECT_ROOT` | Project root directory | Current directory | Global |
| **Locale Settings** | `LANG` | Language setting | `"en_US.UTF-8"` | Global |
| | `LC_ALL` | Locale setting | `"en_US.UTF-8"` | Global |
| **Debugging & Logging** | `DIRENV_LOG_FORMAT` | Direnv log format | `"[direnv] %s"` | Global |

#### Quick Environment Setup

1. Install [direnv](https://direnv.net/)
2. Run `just setup-env` to create initial configuration
3. Edit `.envrc` files to customize your environment
4. Run `direnv allow` to load variables

> **Pro Tip**: Each `.envrc` file includes a section for custom environment variables where you can add project-specific settings.

For more detailed information, consult the [Environment Variables Documentation](docs/environment-variables.md).

### üîß Dynamic Provider and Version Management

Normally, using Terragrunt and depending on what type of Terraform modules your units are using, you might want to skip the generation, override if it was generated by terragrunt or just override these files from the terraform modules being used:

- `providers.tf`
- `versions.tf`

For that, this reference architecture support a very flexible approach to manage these scenarios, based on the following principle: the **unit** which interacts directly with the terraform (modules) interface is the one that defines the providers and versions (if applicable). At the unit level, a certain terraform module might require several providers, and versions, and at the same time, another terraform module might require a different set of providers and versions. This flexibility is achieved by using the `unit_cfg_providers.hcl` and `unit_cfg_versions.hcl` files, which are located in the unit directory.

At minimum, an unit in this architecture will have 4 files. Let's take a look at the following example:

```text
infra/terragrunt/global/dni/dni_generator
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ terragrunt.hcl
‚îú‚îÄ‚îÄ unit_cfg_providers.hcl
‚îî‚îÄ‚îÄ unit_cfg_versions.hcl
```

If a given unit requires a specific provider, and version, it will be defined in the `unit_cfg_providers.hcl` and `unit_cfg_versions.hcl` files. From there, the credentials (if applicable) and the providers, and versions shape can be defined in a reliable, and secure way. See the [unit_cfg_providers.hcl](infra/terragrunt/global/dni/dni_generator/unit_cfg_providers.hcl) and [unit_cfg_versions.hcl](infra/terragrunt/global/dni/dni_generator/unit_cfg_versions.hcl) files for a more real-world example.
When the providers, and versions are defined, they are reliably read automatically by terragrunt (the [config.hcl](infra/terragrunt/config.hcl) file) and used to generate the `providers.tf` and `versions.tf` files if the following conditions are met:

#### Providers Dynamic Generation

The `providers.tf` file is generated dynamically by terragrunt, and it's generated based on the following conditions:

- The `unit_cfg_providers.hcl` file is present in the unit directory (e.g.: `infra/terragrunt/<environment></environment>/<stack>/<unit>/unit_cfg_providers.hcl`)
- The `unit_cfg_providers.hcl` file is not empty.
- The `TG_STACK_FLAG_ENABLE_PROVIDERS_OVERRIDE` feature flag is set to `true` (default behavior)

#### Versions Dynamic Generation

The `versions.tf` file is generated dynamically by terragrunt, and it's generated based on the following conditions:

- The `unit_cfg_versions.hcl` file is present in the unit directory (e.g.: `infra/terragrunt/<environment></environment>/<stack>/<unit>/unit_cfg_versions.hcl`)
- The `unit_cfg_versions.hcl` file is not empty.
- The `TG_STACK_FLAG_ENABLE_VERSIONS_OVERRIDE` feature flag is set to `true` (default behavior)

### üîÑ AWS Remote State Backend

This reference architecture uses AWS S3 and DynamoDB for secure, scalable remote state management. A properly configured remote backend provides state locking, versioning, encryption, and access control.

#### Setting Up Remote Backend

The architecture requires an S3 bucket and DynamoDB table for storing and locking Terraform state. See our [AWS Remote Backend Setup Guide](docs/aws-remote-backend-setup.md) for detailed instructions on:

- Creating a secure S3 bucket with proper versioning and encryption
- Configuring a DynamoDB table for state locking
- Setting up appropriate security measures
- Configuring your environment to use the remote backend

Once configured, update your environment variables to reference your backend:

```bash
TG_STACK_REMOTE_STATE_BUCKET_NAME="your-state-bucket"
TG_STACK_REMOTE_STATE_LOCK_TABLE="your-lock-table"
```

## üìö Documentation

Dive deep into our architecture with our detailed documentation:

1. [Infrastructure Configuration Management](infra/terragrunt/README.md)

   - Configuration strategies
   - Environment variable management
   - Best practices

2. [Shared Components Guide](infra/terragrunt/_shared/README.md)

   - Reusable configuration modules
   - Centralized resource management
   - Standardization techniques

3. [Stack Architecture Principles](infra/terragrunt/global/dni/README.md)

   - Modular stack design
   - Component interaction patterns
   - Scalability considerations

4. [AWS Remote Backend Setup](docs/aws-remote-backend-setup.md)

   - S3 state storage configuration
   - DynamoDB state locking
   - Security best practices
   - Troubleshooting guidance

5. [Dagger CI Module Guide](ci/README.md)
   - Explains the included Dagger module for CI automation.

## üöÄ Getting Started

### Prerequisites

- [Terragrunt](https://terragrunt.gruntwork.io/)
- [Terraform](https://www.terraform.io/)
- [direnv](https://direnv.net/) (required for environment management)
- (Optional) [JustFile](https://github.com/casey/just)

### Environment Setup

1. Install direnv: Follow the [installation instructions](https://direnv.net/docs/installation.html) for your platform
2. Run `just setup-env` to create a basic `.envrc` file if needed
3. Edit the `.envrc` files at different levels to customize your environment
4. Run `direnv allow` in each directory to apply changes

### Running Terragrunt commands

This reference architecture already includes a stack implementation that simulates a DNI generation (showing the most common case, where units are orchestrated in an specific order, with dependencies between them, etc.), a `dni-generator` module that requires a `age-generator` module, `name-generator` module, and `lastname-generator` module. See the [terraform/modules](infra/terraform/modules/README.md) directory for more details.

Run a Terragrunt command on a specific unit:

```bash
# Run a Terragrunt command on a specific unit:
just tg-run global dni dni_generator plan

# or just tg-run since these are the default values.
just tg-run
```

Or, in this particular scenario, you can run the entire stack, and leave [Terragrunt](https://terragrunt.gruntwork.io/) to handle the dependencies between the units, ordering the execution of the units in the correct order, etc.:

```bash
# Running terragrunt run-all plan, in the environment 'global', and stack 'dni'
just tg-run-all-plan global dni

```

More recipes are available in the [justfile](justfile) file.

### Quick Setup

1. Clone the repository
2. Install prerequisites (Terraform, Terragrunt, direnv)
3. Run `just setup-env` to create your environment configuration
4. Run `direnv allow` to load environment variables
5. Review documentation and customize configurations

## ü§ñ CI Automation with Dagger

This reference architecture includes a [Dagger](https://dagger.io/) module located in [`ci/ci-terragrunt`](ci/ci-terragrunt) to automate Terragrunt workflows in CI pipelines.

- **GitHub Actions:** A working example workflow demonstrating how to use the Dagger module for Terragrunt static checks and plans can be found in [`.github/workflows/dagger-ci.yml`](.github/workflows/dagger-ci.yml).
- **Local Execution:** The [`justfile`](justfile) provides convenient recipes (e.g., `just ci-job-units-static-check`) for running these Dagger CI jobs locally.
- **Module Details:** For a detailed explanation of the Dagger module's functions and how to extend it, see the [Dagger CI Module Guide](ci/README.md).

## ü§ù Contributing

Contributions are welcome! Please follow our guidelines:

## üìÑ License

[MIT License](LICENSE)

## üìû Contact

For questions, support, or collaboration:

- Open an [Issue](https://github.com/your-org/terragrunt-ref-arch-v3/issues)
- Just reach out to me on [Linkedin](https://www.linkedin.com/in/alextorresruiz/)
</file>

<file path="CHANGELOG.md">
# Changelog

## [1.5.0](https://github.com/Excoriate/terragrunt-ref-arch-v3/compare/v1.4.0...v1.5.0) (2025-01-29)


### Features

* Enhance PR labeling with new configuration options ([#2](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/2)) ([2abaeda](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/2abaeda3c11d9efedea5cfa3b05311987b8c689b))
* first commit ([0633503](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/0633503b4e96f14016630f383c501e1d07bf8392))
* Update release-please configuration ([#7](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/7)) ([d5366c5](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/d5366c56a9446261c81a51ca7992b36f09ffc527))


### Bug Fixes

* Update release-please configuration for Terraform and Terragrunt ([#3](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/3)) ([a044369](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/a0443693fabdd4c987a1cbe45b1d0e736b9e554f))

## [1.4.0](https://github.com/Excoriate/terragrunt-ref-arch-v3/compare/v1.3.0...v1.4.0) (2025-01-23)


### Features

* Enhance PR labeling with new configuration options ([#2](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/2)) ([2abaeda](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/2abaeda3c11d9efedea5cfa3b05311987b8c689b))
* first commit ([0633503](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/0633503b4e96f14016630f383c501e1d07bf8392))
* Update release-please configuration ([#7](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/7)) ([d5366c5](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/d5366c56a9446261c81a51ca7992b36f09ffc527))


### Bug Fixes

* Update release-please configuration for Terraform and Terragrunt ([#3](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/3)) ([a044369](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/a0443693fabdd4c987a1cbe45b1d0e736b9e554f))

## [1.3.0](https://github.com/Excoriate/terragrunt-ref-arch-v3/compare/v1.2.0...v1.3.0) (2025-01-23)


### Features

* Enhance PR labeling with new configuration options ([#2](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/2)) ([2abaeda](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/2abaeda3c11d9efedea5cfa3b05311987b8c689b))
* first commit ([0633503](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/0633503b4e96f14016630f383c501e1d07bf8392))
* Update release-please configuration ([#7](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/7)) ([d5366c5](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/d5366c56a9446261c81a51ca7992b36f09ffc527))


### Bug Fixes

* Update release-please configuration for Terraform and Terragrunt ([#3](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/3)) ([a044369](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/a0443693fabdd4c987a1cbe45b1d0e736b9e554f))

## [1.2.0](https://github.com/Excoriate/terragrunt-ref-arch-v3/compare/v1.1.0...v1.2.0) (2025-01-23)


### Features

* Enhance PR labeling with new configuration options ([#2](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/2)) ([2abaeda](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/2abaeda3c11d9efedea5cfa3b05311987b8c689b))
* first commit ([0633503](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/0633503b4e96f14016630f383c501e1d07bf8392))


### Bug Fixes

* Update release-please configuration for Terraform and Terragrunt ([#3](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/3)) ([a044369](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/a0443693fabdd4c987a1cbe45b1d0e736b9e554f))

## [1.1.0](https://github.com/Excoriate/terragrunt-ref-arch-v3/compare/v1.0.0...v1.1.0) (2025-01-23)


### Features

* Enhance PR labeling with new configuration options ([#2](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/2)) ([2abaeda](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/2abaeda3c11d9efedea5cfa3b05311987b8c689b))
* first commit ([0633503](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/0633503b4e96f14016630f383c501e1d07bf8392))


### Bug Fixes

* Update release-please configuration for Terraform and Terragrunt ([#3](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/3)) ([a044369](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/a0443693fabdd4c987a1cbe45b1d0e736b9e554f))

## 1.0.0 (2025-01-23)


### Features

* Enhance PR labeling with new configuration options ([#2](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/2)) ([2abaeda](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/2abaeda3c11d9efedea5cfa3b05311987b8c689b))
* first commit ([0633503](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/0633503b4e96f14016630f383c501e1d07bf8392))


### Bug Fixes

* Update release-please configuration for Terraform and Terragrunt ([#3](https://github.com/Excoriate/terragrunt-ref-arch-v3/issues/3)) ([a044369](https://github.com/Excoriate/terragrunt-ref-arch-v3/commit/a0443693fabdd4c987a1cbe45b1d0e736b9e554f))
</file>

<file path="justfile">
# üèóÔ∏è Terragrunt Reference Architecture - Justfile
# This Justfile provides a streamlined interface for managing Terragrunt-based infrastructure
# Designed to simplify complex infrastructure workflows and provide consistent, reproducible deployments

# üìç Path configurations
# Centralize path management to ensure consistent directory references across recipes
TERRAGRUNT_DIR := "./infra/terragrunt"

# üêö Shell configuration
# Use bash with strict error handling to prevent silent failures
# -u: Treat unset variables as an error
# -e: Exit immediately if a command exits with a non-zero status
set shell := ["bash", "-uce"]
set dotenv-load

# üìã Default recipe: List all available commands
# Provides a quick overview of available infrastructure management commands
default:
    @just --list

# üóëÔ∏è Clean macOS system files
# Removes .DS_Store files that can cause unnecessary version control noise
# Helps maintain a clean repository across different operating systems
clean-ds:
    @echo "üßπ Cleaning .DS_Store files"
    @find . -name '.DS_Store' -type f -delete

# üîß Install pre-commit hooks in local environment for code consistency
hooks-install:
    @echo "üß∞ Installing pre-commit hooks locally..."
    @./scripts/hooks/pre-commit-init.sh init

# üïµÔ∏è Run pre-commit hooks across all files in local environment
hooks-run:
    @echo "üîç Running pre-commit hooks from .pre-commit-config.yaml..."
    @./scripts/hooks/pre-commit-init.sh run

# üßπ Terragrunt and Terraform cache cleanup
[working-directory:'infra/terragrunt']
tg-clean-all:
    @echo "üßπ Cleaning Terragrunt cache for all environments and .terraform directories"
    @find . -maxdepth 4 -type d \( -name ".terragrunt-cache" -o -name ".terraform" \) -exec rm -rf {} +
    @find . -maxdepth 4 -type f -name ".terraform.lock.hcl" -exec rm -rf {} +
    @find . -maxdepth 4 -type f -name ".terraform.lock.hcl" -exec rm -rf {} +

# üßπ Terragrunt and Terraform cache cleanup for a specific path
[working-directory:'infra/terragrunt']
tg-clean tgpath:
    @echo "üßπ Cleaning Terragrunt cache for specific path: {{tgpath}}"
    @if [ -d {{tgpath}} ]; then \
        cd {{tgpath}} && \
        find . -maxdepth 4 -type d \( -name ".terragrunt-cache" -o -name ".terraform" \) -exec rm -rf {} + && \
        find . -maxdepth 4 -type f -name ".terraform.lock.hcl" -exec rm -rf {} +; \
    else \
        echo "‚ùå Directory {{tgpath}} does not exist."; \
    fi

# üßπ Terragrunt format, run hclfmt on all Terragrunt files
# Example: `just tg-format check=true diff=true exclude=".terragrunt-cache,modules"`
tg-format check="false" diff="false" exclude="":
    @echo "üîç Running Terragrunt HCL formatting via utility script"
    @./scripts/justfile-utils.sh "{{TERRAGRUNT_DIR}}" "{{check}}" "{{diff}}" "{{exclude}}"

# ‚úÖ Terragrunt validate, run hclvalidate on all Terragrunt files
# Example: `just tg-hclvalidate`
tg-hclvalidate:
    @echo "‚úÖ Running Terragrunt HCL validation via utility script"
    @./scripts/justfile-utils.sh terragrunt_hclvalidate "{{TERRAGRUNT_DIR}}"

tg_env := "global"
tg_stack := "dni"
tg_unit := "dni_generator"

# üöÄ Run Terragrunt on a specific infrastructure unit
# Flexible recipe for running Terragrunt commands on individual units
# Example: `just tg-run cmd=init`
tg-run cmd="init":
    @cd infra/terragrunt/{{tg_env}}/{{tg_stack}}/{{tg_unit}} && terragrunt {{cmd}}

# üåê Run Terragrunt plan across all units in a stack
# Provides a comprehensive view of potential infrastructure changes
# Useful for pre-deployment validation and impact assessment
tg-run-all-plan :
    @cd infra/terragrunt/{{tg_env}}/{{tg_stack}} && terragrunt run-all plan

# üöÄ Apply infrastructure changes across all units in a stack
# Automated, non-interactive deployment of infrastructure
# Includes auto-approval to streamline deployment processes
tg-run-all-apply :
    @cd infra/terragrunt/{{tg_env}}/{{tg_stack}} && terragrunt run-all apply --auto-approve --terragrunt-non-interactive

# üí• Destroy infrastructure across all units in a stack
# Provides a safe, controlled method for infrastructure teardown
# Non-interactive with auto-approval for scripting and automation
tg-run-all-destroy:
    @cd infra/terragrunt/{{tg_env}}/{{tg_stack}} && terragrunt run-all destroy --terragrunt-non-interactive --auto-approve

# üõ†Ô∏è Allow direnv to run
# Ensures that direnv is allowed to run in the current directory
# Useful for managing environment variables and configurations
allow-direnv:
    @echo "üîí Allow direnv to run..."
    @direnv allow

# üîÑ Reload direnv environment
# Manually reload the direnv environment when needed
reload-env:
    @echo "üîÑ Manually reloading direnv environment..."
    @direnv reload

# üßπ Clean direnv cache
# Removes the direnv cache to force a fresh environment build
# Useful when experiencing issues with the development environment
clean-direnv:
    @echo "üßπ Cleaning direnv cache..."
    @rm -rf .direnv
    @direnv allow
    @echo "‚úÖ direnv cache cleaned. Environment will rebuild on next shell activation."

# üîç Open Dagger CI terminal. E.g.: just ci-terminal --help
[working-directory:'ci/ci-terragrunt']
ci-terminal args="":
    @echo "üîç Open Dagger CI terminal"
    @echo "üîç Building the dagger module"
    @dagger develop
    @echo "üîç Inspecting the available functions"
    @dagger functions
    @echo "üîç Running the function"
    @dagger call open-terminal {{args}}

# üîç Run Dagger CI function
[working-directory:'ci/ci-terragrunt']
ci-shell:
    @echo "üîç Running Dagger CI for terragrunt"
    @echo "üîç Building the dagger module"
    @dagger develop
    @echo "üîç Inspecting the available functions"
    @dagger functions
    @echo "üîç Running the function"
    @dagger

# aws_access_key_id := env("AWS_ACCESS_KEY_ID")
# aws_secret_access_key := env("AWS_SECRET_ACCESS_KEY")

# üîç Run Dagger CI function
[working-directory:'ci/ci-terragrunt']
ci-job-units-static-check env="global" layer="dni" unit="dni_generator":
    @echo "üîç Building the dagger module"
    @dagger develop
    @echo "üîç Inspecting the available functions"
    @dagger functions
    @echo "üîç Running the function"
    @dagger call job-terragrunt-units-static-check \
      --load-dot-env-file \
      --no-cache \
      --aws-access-key-id env://AWS_ACCESS_KEY_ID \
      --aws-secret-access-key env://AWS_SECRET_ACCESS_KEY

# üîç Run Dagger CI function
[working-directory:'ci/ci-terragrunt']
ci-job-units-plan env="global" layer="dni" unit="dni_generator":
    @echo "üîç Building the dagger module"
    @dagger develop
    @echo "üîç Inspecting the available functions"
    @dagger functions
    @echo "üîç Running the function"
    @dagger call job-terragrunt-units-plan \
      --load-dot-env-file \
      --no-cache \
      --aws-access-key-id env://AWS_ACCESS_KEY_ID \
      --aws-secret-access-key env://AWS_SECRET_ACCESS_KEY

[working-directory:'ci/ci-terragrunt']
ci-job-tfmodules-static-check:
    @echo "üîç Building the dagger module"
    @dagger develop
    @echo "üîç Inspecting the available functions"
    @dagger functions
    @echo "üîç Running the function"
    @dagger call job-terraform-modules-static-check

dev:
    @echo "üåø Starting Nix Development Shell for Terraform Registry Module Template üè∑Ô∏è"
    @nix develop . --impure --extra-experimental-features nix-command --extra-experimental-features flakes
</file>

<file path=".release-please-manifest.json">
{
  ".": "1.5.0"
}
</file>

</files>
